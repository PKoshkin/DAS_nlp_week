{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/probabll/dgm4nlp/blob/master/notebooks/sst/SST.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Udt3kHMdWvYe"
   },
   "source": [
    "We will need to import some helper code, so we need to run this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "U8eXUCRiWvYi"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "nb_dir = os.path.split(os.getcwd())[0]\n",
    "if nb_dir not in sys.path:\n",
    "    sys.path.append(nb_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rYhItDYMZi6a"
   },
   "source": [
    "# Colab\n",
    "\n",
    "We will need to download some data for this notebook, so if you are using [colab](https://colab.research.google.com), set the `using_colab` flag below to `True` in order to clone our [github repo](https://github.com/probabll/dgm4nlp)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_shCMftIx1rW"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "README.md                     kolka_4_SST.ipynb\r\n",
      "SST.ipynb                     kolka_4_SST_modify.ipynb\r\n",
      "__init__.py                   kolka_working_SST-Copy1.ipynb\r\n",
      "\u001b[1m\u001b[36m__pycache__\u001b[m\u001b[m                   kolka_working_SST.ipynb\r\n",
      "\u001b[1m\u001b[36mdata\u001b[m\u001b[m                          my_first_SST.ipynb\r\n",
      "evaluate.py                   \u001b[1m\u001b[36mnn\u001b[m\u001b[m\r\n",
      "\u001b[1m\u001b[36mimg\u001b[m\u001b[m                           \u001b[31mplotting.py\u001b[m\u001b[m\r\n",
      "kolka_2_SST.ipynb             sstutil.py\r\n",
      "kolka_3_SST.ipynb             util.py\r\n"
     ]
    }
   ],
   "source": [
    "using_colab = not True\n",
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "N-fFME2OW22i"
   },
   "outputs": [],
   "source": [
    "if using_colab:\n",
    "  !rm -fr dgm4nlp sst\n",
    "  !git clone https://github.com/probabll/dgm4nlp.git\n",
    "  !cp -R dgm4nlp/notebooks/sst ./  \n",
    "  !ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "l_7NCZlZacNu"
   },
   "source": [
    "Now we can start our lab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "s9mH-rUhWvYq"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "# CPU should be fine for this lab\n",
    "device = torch.device('cpu')  \n",
    "#device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')  \n",
    "from collections import OrderedDict\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "okMoxTJ9bWjc"
   },
   "source": [
    "# Sentiment Classification \n",
    "\n",
    "\n",
    "We are going to augment a sentiment classifier with a layer of discrete latent variables which will help us improve the model's interpretability. But first, let's quickly review the baseline task.\n",
    "\n",
    "\n",
    "In sentiment classification, we have some text input $x = \\langle x_1, \\ldots, x_n \\rangle$, e.g. a sentence or short paragraph, which expresses a certain sentiment $y$, i.e. one of $K$ classes, towards a subject (e.g. a film or a product). \n",
    "\n",
    "\n",
    "\n",
    "We can learn a sentiment classifier by learning a categorical distribution over classes for a given input:\n",
    "\n",
    "\\begin{align}\n",
    "Y|x &\\sim \\text{Cat}(f(x; \\theta))\n",
    "\\end{align}\n",
    "\n",
    "where the Categorical pmf is $\\text{Cat}(y|\\pi) = \\pi_y$.\n",
    "\n",
    "A categorical distribution over $K$ classes is parameterised by a $K$-dimensional probability vector, here we use a neural network $f$ to map from the input to this probability vector. Technically we say *a neural network parameterise our model*, that is, it computes the parameters of our categorical observation model. The figure below is a graphical depiction of the model: circled nodes are random variables (a shaded node is an observed variable), uncircled nodes are deterministic, a plate indicates multiple draws.\n",
    "\n",
    "<img src=\"https://github.com/probabll/dgm4nlp/raw/master/notebooks/sst/img/classifier.png\"  height=\"100\">\n",
    "\n",
    "The neural network (NN) $f(\\cdot; \\theta)$ has parameters of its own, i.e. the weights of the various architecture blocks used, which we denoted generically by $\\theta$.\n",
    "\n",
    "Suppose we have a dataset $\\mathcal D = \\{(x^{(1)}, y^{(1)}), \\ldots, (x^{(N)}, y^{(N)})\\}$ containing $N$ i.i.d. observations. Then we can use the log-likelihood function \n",
    "\\begin{align}\n",
    "\\mathcal L(\\theta|\\mathcal D) &= \\sum_{k=1}^{N} \\log P(y^{(k)}|x^{(k)}, \\theta) \\\\\n",
    "&= \\sum_{k=1}^{N} \\log \\text{Cat}(y^{(k)}|f(x^{(k)}; \\theta))\n",
    "\\end{align}\n",
    " to estimate $\\theta$ by maximisation:\n",
    " \\begin{align}\n",
    " \\theta^\\star = \\arg\\max_{\\theta \\in \\Theta} \\mathcal L(\\theta|\\mathcal D) ~ .\n",
    " \\end{align}\n",
    " \n",
    "\n",
    "We can use stochastic gradient-ascent to find a local optimum of $\\mathcal L(\\theta|\\mathcal D)$, which only requires a gradient estimate:\n",
    "\n",
    "\\begin{align}\n",
    "\\nabla_\\theta \\mathcal L(\\theta|\\mathcal D) &= \\sum_{k=1}^{|\\mathcal D|} \\nabla_\\theta  \\log P(y^{(k)}|x^{(k)}, \\theta) \\\\ \n",
    "&= \\sum_{k=1}^{|\\mathcal D|} \\frac{1}{N} N \\nabla_\\theta  \\log P(y^{(k)}|x^{(k)}, \\theta)  \\\\\n",
    "&= \\mathbb E_{\\mathcal U(1/N)} \\left[ N \\nabla_\\theta  \\log P(y^{(K)}|x^{(K)}, \\theta) \\right]  \\\\\n",
    "&\\overset{\\text{MC}}{\\approx} \\frac{N}{M} \\sum_{m=1}^M \\nabla_\\theta  \\log P(y^{(k_m)}|x^{(k_m)}, \\theta) \\\\\n",
    "&\\text{where }K_m \\sim \\mathcal U(1/N)\n",
    "\\end{align}\n",
    "\n",
    "This is a Monte Carlo (MC) estimate of the gradient computed on $M$ data points selected uniformly at random from $\\mathcal D$.\n",
    "\n",
    "For as long as $f$ remains differentiable wrt to its inputs and parameters, we can rely on automatic differentiation to obtain gradient estimates.\n",
    "\n",
    "In what follows we show how to design $f$ and how to extend this basic model to a latent-variable model.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4LUjyO-39zan"
   },
   "source": [
    "## Data\n",
    "\n",
    "We provide you some code to load the data (see `sst.sstutil.examplereader`). Play with the snippet below and inspect a few training instances:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4z8Bt5no9z6w"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data\n",
      "train 8544\n",
      "dev 1101\n",
      "test 2210\n",
      "\n",
      "# Examples\n",
      "First dev example: Example(tokens=['It', \"'s\", 'a', 'lovely', 'film', 'with', 'lovely', 'performances', 'by', 'Buy', 'and', 'Accorsi', '.'], label=3, transitions=[0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1], token_labels=[2, 2, 2, 3, 2, 2, 3, 2, 2, 2, 2, 2, 2])\n",
      "First dev example tokens: ['It', \"'s\", 'a', 'lovely', 'film', 'with', 'lovely', 'performances', 'by', 'Buy', 'and', 'Accorsi', '.']\n",
      "First dev example label: 3\n"
     ]
    }
   ],
   "source": [
    "from sst.sstutil import examplereader, Vocabulary, load_glove    \n",
    "\n",
    "\n",
    "# Let's load the data into memory.\n",
    "print(\"Loading data\")\n",
    "train_data = list(examplereader('../sst/data/sst/train.txt'))\n",
    "dev_data = list(examplereader('../sst/data/sst/dev.txt'))\n",
    "test_data = list(examplereader('../sst/data/sst/test.txt'))\n",
    "\n",
    "print(\"train\", len(train_data))\n",
    "print(\"dev\", len(dev_data))\n",
    "print(\"test\", len(test_data))\n",
    "\n",
    "print('\\n# Examples')\n",
    "example = dev_data[0]\n",
    "print(\"First dev example:\", example)\n",
    "print(\"First dev example tokens:\", example.tokens)\n",
    "print(\"First dev example label:\", example.label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lB2lEsNuWvYx"
   },
   "source": [
    "## Architecture\n",
    "\n",
    "\n",
    "The function $f$ conditions on a high-dimensional input (i.e. text), so we need to convert it to continuous real vectors. This is the job an *encoder*. \n",
    "\n",
    "**Embedding Layer**\n",
    "\n",
    "The first step is to convert the words in $x$ to vectors, which in this lab we will do with a pre-trained embedding layer (we will use GloVe).\n",
    "\n",
    "We will denote the embedding of the $i$th word of the input by:\n",
    "\n",
    "\\begin{equation}\n",
    "\\mathbf x_i = \\text{glove}(x_i)\n",
    "\\end{equation}\n",
    "\n",
    "**Encoder Layer**\n",
    "\n",
    "In this lab, an encoder takes a sequence of input vectors $\\mathbf x_1^n$, each $I$-dimensional, and produces a sequence of output vectors $\\mathbf t_1^n$, each $O$-dimensional and a summary vector $\\mathbf h \\in \\mathbb R^O$:\n",
    "\n",
    "\\begin{equation}\n",
    "    \\mathbf t_1^n, \\mathbf h = \\text{encoder}(\\mathbf x_1^n; \\theta_{\\text{enc}})\n",
    "\\end{equation}\n",
    "\n",
    "where we use $\\theta_{\\text{enc}}$ to denote the subset of parameters in $\\theta$ that are specific to this encoder block. \n",
    "\n",
    "*Remark:* in practice for a correct batched implementation, our encoders also take a mask matrix and a vector of lengths.\n",
    "\n",
    "Examples of encoding functions can be a feed-forward NN (with an aggregator based on sum or average/max pooling) or a recurrent NN (e.g. an LSTM/GRU). Other architectures are also possible.\n",
    "\n",
    "**Output Layer**\n",
    "\n",
    "From our summary vector $\\mathbf h$, we need to parameterise a categorical distribution over $K$ classes, thus we use\n",
    "\n",
    "\\begin{align}\n",
    "f(x; \\theta) &= \\text{softmax}(\\text{dense}_K(\\mathbf h; \\theta_{\\text{output}}))\n",
    "\\end{align}\n",
    "\n",
    "where $\\text{dense}_K$ is a dense layer with $K=5$ outputs and $\\theta_{\\text{output}}$ corresponds to its parameters (weight matrix and bias vector). Note that we need to use the softmax activation function in order to guarantee that the output of $f$ is a normalised probability vector.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Kc15Nv2i41cq"
   },
   "source": [
    "## Implementation\n",
    "\n",
    "To leave an indication of the shape of tensors in the code, we use the following convention\n",
    "\n",
    "```python\n",
    "[B, T, D]\n",
    "```\n",
    "\n",
    "where `B` stands for `batch_size`, `T` stands for `time` (or rather *maximum sequence length*), and `D` is the size of the representation.\n",
    "\n",
    "\n",
    "Consider the following abstract Encoder class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xwEPXT2MWvYz",
    "tags": [
     "encoders"
    ]
   },
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    \"\"\"\n",
    "    An Encoder for us is a function that\n",
    "      1. transforms a sequence of I-dimensional vectors into a sequence of O-dimensional vectors\n",
    "      2. summarises a sequence of I-dimensional vectors into one O-dimensional vector\n",
    "      \n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    def __init__(self):\n",
    "        super(Encoder, self).__init__()\n",
    "        \n",
    "    def forward(self, inputs, mask, lengths):\n",
    "        \"\"\"\n",
    "        The input is a batch-first tensor of token ids. Here is an example:\n",
    "        \n",
    "        Example of inputs (though rather than words, we have word ids):\n",
    "            INPUTS                     MASK       LENGTHS\n",
    "            [the nice cat -PAD-]    -> [1 1 1 0]  [3]\n",
    "            [the nice dog running]  -> [1 1 1 1]  [4]\n",
    "            \n",
    "        Note that:\n",
    "              mask =  inputs == 1\n",
    "              lengths = mask.sum(dim=-1)\n",
    "        \n",
    "        :param inputs: [B, T, I]\n",
    "        :param mask: [B, T]\n",
    "        :param lengths: [B]\n",
    "        :returns: [B, T, O], [B, O]\n",
    "            where the first tensor is the transformed input\n",
    "            and the second tensor is a summary of all inputs\n",
    "        \"\"\"\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WA5wmkcRg9Am"
   },
   "source": [
    "Let's start easy, implement a *bag of words* encoder:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "U-9hLQ0lF5SG"
   },
   "outputs": [],
   "source": [
    "class BagOfWordsEncoder(Encoder):\n",
    "    \"\"\"\n",
    "    This encoder does not transform the input sequence, \n",
    "     and its summary output is just a sum.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(BagOfWordsEncoder, self).__init__()\n",
    "        \n",
    "    def forward(self, inputs, mask, lengths=None, **kwargs):\n",
    "        reshaped_mask = mask.float().unsqueeze(-1)  # shape: [B, T, 1]\n",
    "        return inputs, (inputs * reshaped_mask).sum(1) / (reshaped_mask.sum(1) + 1e-8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IS7x0hLrUXfN"
   },
   "source": [
    "You can also consider implementing\n",
    "\n",
    "* a feed-forward encoder with average pooling\n",
    "* and a biLSTM encoder\n",
    "\n",
    "but these are certainly optional."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BpOGFpK_Uo0-"
   },
   "outputs": [],
   "source": [
    "class FFEncoder(Encoder):\n",
    "    \"\"\"\n",
    "    A typical feed-forward NN with tanh hidden activations.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self,\n",
    "                 input_size,\n",
    "                 output_size, \n",
    "                 activation=None, \n",
    "                 hidden_sizes=[], \n",
    "                 aggregator='sum',\n",
    "                 dropout=0.5):\n",
    "        \"\"\"\n",
    "        :param input_size: int\n",
    "        :param output_size: int\n",
    "        :param hidden_sizes: list of integers (dimensionality of hidden layers)\n",
    "        :param aggregator: 'sum' or 'avg'\n",
    "        :param dropout: dropout rate\n",
    "        \"\"\"\n",
    "        super(FFEncoder, self).__init__()\n",
    "        layers = []\n",
    "        \n",
    "        def add_droput(name):\n",
    "            if dropout > 0:\n",
    "                layers.append((name, nn.Dropout(p=dropout)))\n",
    "        \n",
    "        if len(hidden_sizes) > 0:                    \n",
    "            for i, size in enumerate(hidden_sizes):\n",
    "                add_droput('dropout_{}'.format(i))\n",
    "                layers.append(('linear_{}'.format(i), nn.Linear(input_size, size)))\n",
    "                layers.append(('tanh_{}'.format(i), nn.Tanh()))\n",
    "                input_size = size\n",
    "\n",
    "        last_output_size = hidden_sizes[-1] if len(hidden_sizes) > 0 else input_size\n",
    "        add_droput('final_dropout')\n",
    "        layers.append(('final_linear', nn.Linear(last_output_size, output_size)))       \n",
    "        self.layer = nn.Sequential(OrderedDict(layers))\n",
    "\n",
    "        self.activation = activation\n",
    "\n",
    "        if not aggregator in ['sum', 'avg']:\n",
    "            raise ValueError(\"I can only aggregate outputs using 'sum' or 'avg'\")\n",
    "        self.aggregator = aggregator\n",
    "        \n",
    "    def forward(self, inputs, mask, lengths):\n",
    "        outputs = self.layer(inputs)  # shape: [B, T, O]\n",
    "        if not self.activation is None:\n",
    "            outputs = self.activation(outputs)  # shape: [B, T, O]\n",
    "        reshaped_mask = mask.float().unsqueeze(-1)  # shape: [B, T, 1]\n",
    "        summary = (outputs * reshaped_mask).sum(dim=1)  # shape: [B, O]\n",
    "        if self.aggregator == 'avg':\n",
    "            reshaped_lens = lengths.float().unsqueeze(-1)  # shape: [B, 1]\n",
    "            summary /= reshaped_lens  # shape: [B, O]\n",
    "        return outputs, summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IxQ5djZ_VAvK"
   },
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "\n",
    "\n",
    "class LSTMEncoder(Encoder):\n",
    "    \"\"\"\n",
    "    This module encodes a sequence into a single vector using an LSTM,\n",
    "     it also returns the hidden states at each time step.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 in_features,\n",
    "                 hidden_size: int=200,\n",
    "                 batch_first: bool=True,\n",
    "                 bidirectional: bool=True):\n",
    "        \"\"\"\n",
    "        :param in_features:\n",
    "        :param hidden_size:\n",
    "        :param batch_first:\n",
    "        :param bidirectional:\n",
    "        \"\"\"\n",
    "        super(LSTMEncoder, self).__init__()\n",
    "        self.lstm = nn.LSTM(\n",
    "            in_features,\n",
    "            hidden_size,\n",
    "            batch_first=batch_first,\n",
    "            bidirectional=bidirectional\n",
    "        )\n",
    "        self.batch_first = batch_first\n",
    "\n",
    "    def forward(self, x, mask, lengths):\n",
    "        \"\"\"\n",
    "        Encode sentence x\n",
    "        :param x: sequence of word embeddings, shape [B, T, E]\n",
    "        :param mask: byte mask that is 0 for invalid positions, shape [B, T]\n",
    "        :param lengths: the lengths of each input sequence [B]\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        mask = mask.unsqueeze(2).float()\n",
    "        res = self.lstm(x * mask)[0]\n",
    "        return res, res[:, -1, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "s_zz5zIyVkSh"
   },
   "source": [
    "Here is some helper code to select and return an encoder:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "59ZU6JddVjMV"
   },
   "outputs": [],
   "source": [
    "def get_encoder(layer, in_features, hidden_size, bidirectional=True):\n",
    "    \"\"\"Returns the requested layer.\"\"\"\n",
    "\n",
    "    # TODO: make pass and average layers\n",
    "    if layer == \"bow\":\n",
    "        return BagOfWordsEncoder()\n",
    "    elif layer == 'ff':\n",
    "        return FFEncoder(\n",
    "            in_features, \n",
    "            2 * hidden_size,   # for convenience\n",
    "            hidden_sizes=[hidden_size], \n",
    "            aggregator='avg'\n",
    "        )\n",
    "    elif layer == \"lstm\":\n",
    "        return LSTMEncoder(\n",
    "            in_features, \n",
    "            hidden_size,\n",
    "            bidirectional=bidirectional\n",
    "        )\n",
    "    else:\n",
    "        raise ValueError(\"Unknown layer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kY8LZiMN5CHW"
   },
   "source": [
    "# Sentiment Classification with Latent Rationale\n",
    "\n",
    "A latent rationale is a compact and informative fragment of the input based on which a NN classifier makes its decisions. [Lei et al (2016)](http://aclweb.org/anthology/D16-1011) proposed to induce such rationales along with a regression model for multi-aspect sentiment analsysis, their model is trained via REINFORCE on a dataset of beer reviews.\n",
    "\n",
    "*Remark:* the model we will develop here can be seen as a probabilistic version of their model. The rest of this notebook focus on our own probabilitisc view of the model.\n",
    "\n",
    "The picture below depicts our latent-variable model for rationale extraction:\n",
    "\n",
    "<img src=\"https://github.com/probabll/dgm4nlp/raw/master/notebooks/sst/img/rationale.png\"  height=\"200\">\n",
    "\n",
    "where we augment the model with a collection of latent variables $z = \\langle z_1, \\ldots, z_n\\rangle$ where $z_i$ is a binary latent variable. Each latent variable $z_i$ regulates whether or not the input $x_i$ is available to the classifier.  We use $x \\odot z$ to denote the selected words, which, in the terminology of Lei et al, is a latent rationale.\n",
    "\n",
    "Again the classifier parameterises a Categorical distribution over $K=5$ outcomes, though this time it can encode only a selection of the input:\n",
    "\n",
    "\\begin{align}\n",
    "    Z_i & \\sim \\text{Bern}(p_1) \\\\\n",
    "    Y|z,x &\\sim \\text{Cat}(f(x \\odot z; \\theta))\n",
    "\\end{align}\n",
    "\n",
    "where we have a shared and fixed Bernoulli prior (with parameter $p_1$) for all $n$ latent variables.\n",
    "\n",
    "\n",
    "Here is an example design for $f$:\n",
    "\n",
    "\\begin{align}\n",
    "\\mathbf x_i &= z_i \\, \\text{glove}(x_i) \\\\\n",
    "\\mathbf t_1^n, \\mathbf h &= \\text{encoder}(\\mathbf x_1^n; \\theta_{\\text{enc}}) \\\\\n",
    "f(x \\odot z; \\theta) &= \\text{softmax}(\\text{dense}_K(\\mathbf h; \\theta_{\\text{output}}))\n",
    "\\end{align}\n",
    "\n",
    "where:\n",
    "* $z_i$ either leaves $\\mathbf x_i$ unchanged or turns it into a vector of zeros;\n",
    "* the encoder only sees features from selected inputs, i.e. $x_i$ for which $z_i = 1$;\n",
    "* $\\text{dense}_K$ is a linear layer with $K=5$ outputs.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hDHNxLHMWvY-"
   },
   "source": [
    "## Prior\n",
    "\n",
    "\n",
    "Our prior is a Bernoulli with fixed parameter $0 < p_1 < 1$:\n",
    "\n",
    "\\begin{align}\n",
    "Z_i & \\sim \\text{Bern}(p_1)\n",
    "\\end{align}\n",
    "\n",
    "whose pmf is $\\text{Bern}(z_i|p_1) = p_1^{z_i}\\times (1-p_1)^{1-z_i}$.\n",
    "\n",
    "As we will be using Bernoulli priors and posteriors, it is a good idea to implement a Bernoulli class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iCBcHnTsOuDr"
   },
   "outputs": [],
   "source": [
    "class Bernoulli:\n",
    "    \"\"\"\n",
    "    This class encapsulates a collection of Bernoulli distributions. \n",
    "    Each Bernoulli is uniquely specified by p_1, where\n",
    "        Bernoulli(X=x|p_1) = pow(p_1, x) + pow(1 - p_1, 1 - x)\n",
    "    is the Bernoulli probability mass function (pmf).    \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, logits=None, probs=None):\n",
    "        \"\"\"\n",
    "        We can specify a Bernoulli distribution via a logit or a probability. \n",
    "         You need to specify at least one, and if you specify both, beware that\n",
    "         in this implementation logits will be used.\n",
    "         \n",
    "        Recall that: probs = sigmoid(logits).\n",
    "         \n",
    "        :param logits: a tensor of logits (a logit is defined as log (p_1 / p_0))\n",
    "            where p_0 = 1 - p_1\n",
    "        :param probs: a tensor of probabilities, each in (0, 1)\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        #print(\"probs:\", type(probs))\n",
    "        #print(\"logits:\", type(logits))\n",
    "                \n",
    "        if probs is None and logits is None:\n",
    "            raise ValueError('I need probabilities or logits')   \n",
    "        \n",
    "        self.probs = probs if logits is None else torch.sigmoid(logits)\n",
    "    \n",
    "    def sample(self):\n",
    "        \"\"\"Returns a sample with the same shape as the parameters\"\"\"\n",
    "        return torch.rand(self.probs.shape) < self.probs\n",
    "    \n",
    "    def log_pmf(self, x):\n",
    "        \"\"\"\n",
    "        Assess the log probability of a sample. \n",
    "        :param x: either a single sample (0 or 1) or a tensor of samples with the same shape as the parameters.\n",
    "        :returns: tensor with log probabilities with the same shape as parameters\n",
    "            (if the input is a single sample we broadcast it to the shape of the parameters)\n",
    "        \"\"\"\n",
    "        return x * torch.log(self.probs) + (1.0 - x) * torch.log(1.0 - self.probs)\n",
    "    \n",
    "    def kl(self, other: 'Bernoulli'):\n",
    "        \"\"\"\n",
    "        Compute the KL divergence between two Bernoulli distributions (from self to other).\n",
    "        \n",
    "        :return: KL[self||other] with same shape parameters\n",
    "        \"\"\"\n",
    "        return self.probs * (torch.log(self.probs) - torch.log(other.probs)) \\\n",
    "            + (1 - self.probs) * (torch.log(1 - self.probs) - torch.log(1 - other.probs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "u0yfkCZlWvZP"
   },
   "source": [
    "## Classifier\n",
    "\n",
    "The classifier encodes only a selection of the input, which we denote $x \\odot z$, and parameterises a Categorical distribution over $5$ outcomes (sentiment levels).\n",
    "\n",
    "Thus let's implement a Categorical distribution (we will only need to be able to assess its lgo pmf):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "F-6JLDnBQcdg"
   },
   "outputs": [],
   "source": [
    "class Categorical:\n",
    "    \n",
    "    def __init__(self, log_probs):\n",
    "        # [B, K]: class probs\n",
    "        self.log_probs = log_probs\n",
    "        \n",
    "    def log_pmf(self, x):\n",
    "        \"\"\"\n",
    "        :param x: [B] integers (targets)\n",
    "        :returns: [B] scalars (log probabilities)\n",
    "        \"\"\"\n",
    "        return self.log_probs[np.arange(len(x)), x]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CdrM_YRI8xBF"
   },
   "source": [
    "and a classifier architecture:\n",
    "\n",
    "* implement the forward method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sz7GaKbgRCd8"
   },
   "outputs": [],
   "source": [
    "class Classifier(nn.Module):\n",
    "    \"\"\"\n",
    "    The Encoder takes an input text (and rationale z) and computes p(y|x,z)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 embed: nn.Embedding=None,\n",
    "                 hidden_size: int=200,\n",
    "                 output_size: int=1,\n",
    "                 dropout: float=0.1,\n",
    "                 layer: str=\"pass\"):\n",
    "\n",
    "        super(Classifier, self).__init__()\n",
    "\n",
    "        emb_size = embed.weight.shape[1]\n",
    "        enc_size = hidden_size * 2\n",
    "        # Here we embed the words\n",
    "        self.embed_layer = nn.Sequential(embed)\n",
    "\n",
    "        self.enc_layer = get_encoder(layer, emb_size, hidden_size)\n",
    "\n",
    "        # and here we predict categorical parameters\n",
    "        self.output_layer = nn.Sequential(\n",
    "            nn.Dropout(p=dropout),\n",
    "            nn.Linear(enc_size, output_size),\n",
    "            nn.LogSoftmax(dim=-1)\n",
    "        )\n",
    "\n",
    "        self.report_params()\n",
    "\n",
    "    def report_params(self):\n",
    "        count = 0\n",
    "        for name, p in self.named_parameters():\n",
    "            if p.requires_grad and \"embed\" not in name:\n",
    "                count += np.prod(list(p.shape))\n",
    "        print(\"{} #params: {}\".format(self.__class__.__name__, count))\n",
    "\n",
    "    def forward(self, x, mask, z) -> Categorical:\n",
    "        \"\"\"\n",
    "        :params x: [B, T, I] word representations\n",
    "        :params mask: [B, T] indicates valid positions\n",
    "        :params z: [B, T] binary selectors\n",
    "        :returns: one Categorical distribution per instance in the batch\n",
    "          each conditioning only on x_i for which z_i = 1\n",
    "        \"\"\"\n",
    "        embeddings = self.embed_layer(x)  # [B, T, E]\n",
    "        embedding_mask = z.float().unsqueeze(-1)  # [B, T, 1]\n",
    "        masked_embeddings = embeddings * embedding_mask  # [B, T, E]\n",
    "        lengths = mask.long().sum(1)  # [B]\n",
    "\n",
    "        # encode the sentence\n",
    "        _, final = self.enc_layer(masked_embeddings, mask, lengths)\n",
    "\n",
    "        # predict sentiment from final state(s)\n",
    "        log_probs = self.output_layer(final)  # [B, T, O]\n",
    "        return Categorical(log_probs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "p2waCCBF9MaH"
   },
   "source": [
    "## Inference\n",
    "\n",
    "\n",
    "Computing the log-likelihood of an observation requires marginalising over assignments of $z$:\n",
    "\n",
    "\\begin{align}\n",
    "P(y|x,\\theta,p_1) &= \\sum_{z_1 = 0}^1 \\cdots \\sum_{z_n=0}^1 P(z|p_1)\\times P(y|x,z, \\theta) \\\\\n",
    "&= \\sum_{z_1 = 0}^1 \\cdots \\sum_{z_n=0}^1 \\left( \\prod_{i=1}^n \\text{Bern}(z_i|p_1)\\right) \\times \\text{Cat}(y|f(x \\odot z; \\theta)) \n",
    "\\end{align}\n",
    "\n",
    "This is clearly intractable: there are $2^n$ possible assignments to $z$ and because the classifier conditions on all latent selectors, there's no way to simplify the expression.\n",
    "\n",
    "We will avoid computing this intractable marginal by instead employing an independently parameterised inference model.\n",
    "This inference model $Q(z|x, y, \\lambda)$ is an approximation to the true postrerior $P(z|x, y, \\theta, p_1)$, and we use $\\lambda$ to denote its parameters.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0jcVdYTg8Wun"
   },
   "source": [
    "We make a *mean field* assumption, whereby we model latent variables independently given the input:\n",
    "\\begin{align}\n",
    "Q(z|x, y, \\lambda) \n",
    "    &= \\prod_{i=1}^{n} Q(z_i|x; \\lambda) \\\\\n",
    "    &= \\prod_{i=1}^{n} \\text{Bern}(z_i|g_i(x; \\lambda)) \n",
    "\\end{align}\n",
    "\n",
    "where $g(x; \\lambda)$ is a NN that maps from $x = \\langle x_1, \\ldots, x_n\\rangle$ to $n$ Bernoulli parameters, each of which, is a probability value (thus $0 < g_i(x; \\lambda) < 1$).\n",
    "\n",
    "Note that though we could condition on $y$ for approximate posterior inference, we are opportunistically leaving it out. This way, $Q$ is directly available at test time for making predictions. The figure below is a graphical depiction of the inference model (we show a dashed arrow from $y$ to $z$ to remind you that in principle the label is also available).\n",
    "\n",
    "<img src=\"https://github.com/probabll/dgm4nlp/raw/master/notebooks/sst/img/inference.png\"  height=\"200\">\n",
    "\n",
    "Here is an example design for $g$:\n",
    "\\begin{align}\n",
    "\\mathbf x_i &= \\text{glove}(x_i) \\\\\n",
    "\\mathbf t_1^n, \\mathbf h &= \\text{encoder}(\\mathbf x_1^n; \\lambda_{\\text{enc}}) \\\\\n",
    "g_i(x; \\lambda) &= \\sigma(\\text{dense}_1(\\mathbf t_i; \\lambda_{\\text{output}}))\n",
    "\\end{align}\n",
    "where\n",
    "* $\\text{glove}$ is a pre-trained embedding function;\n",
    "* $\\text{dense}_1$ is a dense layer with a single output;\n",
    "* and $\\sigma(\\cdot)$ is the sigmoid function, necessary to parameterise a Bernoulli distribution.\n",
    "\n",
    "From now on we will write $Q(z|x, \\lambda)$, that is, without $y.\n",
    "\n",
    "Here we implement this product of Bernoulli distributions:\n",
    "\n",
    "* implement $g$ in the constructor \n",
    "* and the forward pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YLxfcAbuSiFo"
   },
   "outputs": [],
   "source": [
    "class ProductOfBernoullis(nn.Module):\n",
    "    \"\"\"\n",
    "    This is an inference network that parameterises independent Bernoulli distributions.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 embed: nn.Embedding,\n",
    "                 hidden_size: int=200,\n",
    "                 layer: str=\"bow\"):\n",
    "        \"\"\"\n",
    "        :param embed: an embedding layer\n",
    "        :param hidden_suze: hidden size for transformed inputs\n",
    "        :param layer: 'bow' for BoW encoding\n",
    "          you may alternatively implement and 'lstm' option\n",
    "          which uses a biLSTM to transform the inputs         \n",
    "        \"\"\"\n",
    "        super(ProductOfBernoullis, self).__init__()\n",
    "        # 1. we should have an embedding layer \n",
    "        # 2. we may transform the representations\n",
    "        # 3. and we should compute parameters for Bernoulli distributions\n",
    "        \n",
    "        emb_size = embed.weight.shape[1]\n",
    "        # Using twice the units just to make the output as large as that of a biLSTM\n",
    "        enc_size = hidden_size * 2  \n",
    "\n",
    "        self.embed_layer = nn.Sequential(embed)\n",
    "        self.enc_layer = get_encoder(layer, emb_size, hidden_size)\n",
    "        self.logit_layer = nn.Linear(enc_size, 1, bias=True)\n",
    "                \n",
    "        self.report_params()\n",
    "    \n",
    "    def report_params(self):\n",
    "        count = 0\n",
    "        for name, p in self.named_parameters():\n",
    "            if p.requires_grad and \"embed\" not in name:\n",
    "                count += np.prod(list(p.shape))\n",
    "        print(\"{} #params: {}\".format(self.__class__.__name__, count))\n",
    "\n",
    "    def forward(self, x, mask) -> Bernoulli:\n",
    "        \"\"\"\n",
    "        It takes a tensor of tokens (integers)\n",
    "         and predicts a Bernoulli distribution for each position.\n",
    "        \n",
    "        :param x: [B, T]\n",
    "        :param mask: [B, T]\n",
    "        :returns: Bernoulli\n",
    "        \"\"\"\n",
    "\n",
    "        # encode sentence\n",
    "        lengths = mask.long().sum(1)  # [B]\n",
    "        embeddings = self.embed_layer(x)  # [B, T, E]\n",
    "        h, _ = self.enc_layer(embeddings, mask, lengths)  # [B, T, d]\n",
    "\n",
    "        # compute parameters for Bernoulli p(z|x)\n",
    "        # Bernoulli distributions\n",
    "        logits = self.logit_layer(h).squeeze(-1)  # [B, T]\n",
    "\n",
    "        return Bernoulli(logits=logits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fcCu7vkKWvZX"
   },
   "source": [
    "## Parameter Estimation\n",
    "\n",
    "In variational inference, our objective is to maximise the *evidence lowerbound* (ELBO):\n",
    "\n",
    "\\begin{align}\n",
    "\\log P(y|x) &\\ge \\mathbb E_{Q(z|x, y, \\lambda)}\\left[ \\log P(y|x, z, \\theta, p_1) \\right] - \\text{KL}(Q(z|x, y, \\lambda) || P(z|p_1)) \\\\\n",
    "\\text{ELBO}&\\overset{\\text{MF}}{=}\\mathbb E_{Q(z|x, y, \\lambda)}\\left[ \\log P(y|x, z, \\theta, p_1) \\right] - \\sum_{i=1}^n \\text{KL}(Q(z_i|x, \\lambda) || P(z_i|p_1)) \n",
    "\\end{align}\n",
    "\n",
    "where the *mean field* assumption we made implies that the KL term is simply a sum of KL divergences from a Bernoulli posterior to a Bernoulli prior.\n",
    "\n",
    "Note that the ELBO remains intractable, namely, solving the expectation in closed form still requires $2^n$ evaluations of the classifier network. Though unlike the true posterior $P(z|x,y, \\lambda)$, the approximation $Q(z|x,\\lambda)$ is tractable (it does not require an intractable normalisation) and can be used to obtain gradient estimates based on samples.\n",
    "\n",
    "### Gradient of the classifier network\n",
    "\n",
    "For the classifier, we encounter no problem:\n",
    "\n",
    "\\begin{align}\n",
    "\\nabla_\\theta \\text{ELBO} &=\\nabla_\\theta\\sum_{z} Q(z|x, \\lambda)\\log P(y|x,z,\\theta) - \\underbrace{\\nabla_\\theta \\sum_{i=1}^n \\text{KL}(Q(z_i|x, \\lambda) || P(z_i|p_1))}_{\\color{blue}{0}}  \\\\\n",
    "&=\\sum_{z} Q(z|x, \\lambda)\\nabla_\\theta\\log P(y|x,z,\\theta) \\\\\n",
    "&= \\mathbb E_{Q(z|x, \\lambda)}\\left[\\nabla_\\theta\\log P(y|x,z,\\theta) \\right] \\\\\n",
    "&\\overset{\\text{MC}}{\\approx} \\frac{1}{S} \\sum_{s=1}^S \\nabla_\\theta \\log P(y|x, z^{(s)}, \\theta) \n",
    "\\end{align}\n",
    "where $z^{(s)} \\sim Q(z|x,\\lambda)$.\n",
    "\n",
    "\n",
    "### Gradient of the inference network\n",
    "\n",
    "For the inference model, we have to use the *score function estimator* (a.k.a. REINFORCE):\n",
    "\n",
    "\\begin{align}\n",
    "\\nabla_\\lambda \\text{ELBO} &=\\nabla_\\lambda\\sum_{z} Q(z|x, \\lambda)\\log P(y|x,z,\\theta) - \\nabla_\\lambda \\underbrace{\\sum_{i=1}^n \\text{KL}(Q(z_i|x, \\lambda) || P(z_i|p_1))}_{ \\color{blue}{\\text{tractable} }}  \\\\\n",
    "&=\\sum_{z} \\nabla_\\lambda Q(z|x, \\lambda)\\log P(y|x,z,\\theta) - \\sum_{i=1}^n \\nabla_\\lambda \\text{KL}(Q(z_i|x, \\lambda) || P(z_i|p_1))   \\\\\n",
    "&=\\sum_{z}  \\underbrace{Q(z|x, \\lambda) \\nabla_\\lambda \\log Q(z|x, \\lambda)}_{\\nabla_\\lambda Q(z|x, \\lambda)} \\log P(y|x,z,\\theta) - \\sum_{i=1}^n \\nabla_\\lambda \\text{KL}(Q(z_i|x, \\lambda) || P(z_i|p_1))   \\\\\n",
    "&= \\mathbb E_{Q(z|x, \\lambda)}\\left[ \\log P(y|x,z,\\theta) \\nabla_\\lambda \\log Q(z|x, \\lambda) \\right] - \\sum_{i=1}^n \\nabla_\\lambda \\text{KL}(Q(z_i|x, \\lambda) || P(z_i|p_1))   \\\\\n",
    "&\\overset{\\text{MC}}{\\approx} \\left(\\frac{1}{S} \\sum_{s=1}^S  \\log P(y|x, z^{(s)}, \\theta) \\nabla_\\lambda \\log Q(z^{(s)}|x, \\lambda)  \\right) - \\sum_{i=1}^n \\nabla_\\lambda \\text{KL}(Q(z_i|x, \\lambda) || P(z_i|p_1))  \n",
    "\\end{align}\n",
    "\n",
    "where $z^{(s)} \\sim Q(z|x,\\lambda)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6cdfkOYdC0LQ"
   },
   "source": [
    "## Implementation\n",
    "\n",
    "Let's implement the model and the loss (negative ELBO). We work with the notion of a *surrogate loss*, that is, a computation node whose gradients wrt to parameters are equivalent to the gradients we need.\n",
    "\n",
    "For a given sample $z \\sim Q(z|x, \\lambda)$, the following is a single-sample surrogate loss:\n",
    "\n",
    "\\begin{align}\n",
    "\\mathcal S(\\theta, \\lambda|x, y) = \\log P(y|x, z, \\theta) + \\color{red}{\\text{detach}(\\log P(y|x, z, \\theta) )}\\log Q(z|x, \\lambda) - \\sum_{i=1}^n \\text{KL}(Q(z_i|x, \\lambda) || P(z_i|\\phi))\n",
    "\\end{align}\n",
    "where we introduce an auxiliary function such that\n",
    "\\begin{align}\n",
    "\\text{detach}(f(\\alpha))  &= h(\\alpha) \\\\\n",
    "\\nabla_\\beta \\text{detach}(h(\\alpha))  &= 0 \n",
    "\\end{align}\n",
    "or in words, *detach* does not alter the forward call of its argument function $h$, but it alters $h$'s backward call by setting gradients to zero.\n",
    "\n",
    "Show that it's gradients wrt $\\theta$ and $\\lambda$ are exactly what we need:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FednEChaX6WI"
   },
   "source": [
    "\\begin{align}\n",
    "\\nabla_\\theta\\mathcal S(\\theta,\\lambda|x,y)=\\nabla_\\theta\\log P(y|x,z,\\theta)+0=\\nabla_\\theta\\text{ELBO}\n",
    "\\end{align}$$$$\\begin{align}\n",
    "\\nabla_\\lambda \\mathcal S(\\theta, \\lambda|x, y)= 0 + \\underbrace{\\log Q(z|x, \\lambda)\\nabla_\\lambda \\log P(y|x, z, \\theta)  + \\log P(y|x, z, \\theta) \\nabla_\\lambda \\log Q(z|x, \\lambda)}_{\\text{chain rule}}-\\sum_{i=1}^n \\nabla_\\lambda \\text{KL}(Q(z_i|x, \\lambda) || P(z_i|p_1))=\\\\ \n",
    "= 0+ 0 + \\log P(y|x, z, \\theta) \\nabla_\\lambda \\log Q(z|x, \\lambda)-\\sum_{i=1}^n \\nabla_\\lambda \\text{KL}(Q(z_i|x, \\lambda) || P(z_i|p_1))=\\nabla_\\lambda\\text{ELBO}\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OaUMKDShx9T0"
   },
   "source": [
    "Implement the forward pass and loss below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Cnwwk-7tfR02"
   },
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    \"\"\"\n",
    "    \n",
    "    Classifier model:\n",
    "        Z_i ~ Bern(p_1) for i in 1..n\n",
    "        Y|x,z ~ Cat(f([x_i if z_i 1 else 0 for i in 1..n ]))\n",
    "    \n",
    "    Inference model:\n",
    "        Z_i|x ~ Bern(b_i) for i in 1..n\n",
    "            where b_i = g_i(x)\n",
    "    \n",
    "    Objective:\n",
    "        Single-sample MC estimate of ELBO\n",
    "    \n",
    "    Loss: \n",
    "        Surrogate loss\n",
    "\n",
    "    Consists of:\n",
    "        - a product of Bernoulli distributions inference network\n",
    "        - a classifier network\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 vocab: object = None,\n",
    "                 vocab_size: int = 0,\n",
    "                 emb_size: int = 200,\n",
    "                 hidden_size: int = 200,\n",
    "                 num_classes: int = 5,\n",
    "                 prior_p1: float = 0.3,                 \n",
    "                 det_prior: bool = True,\n",
    "                 beta_shape: list = [0.6, 0.6],\n",
    "                 dropout: float = 0.1,\n",
    "                 layer_cls: str = 'bow',\n",
    "                 layer_inf: str = 'bow'):\n",
    "        \"\"\"\n",
    "        :param vocab: Vocabulary\n",
    "        :param vocab_size: necessary for embedding layer\n",
    "        :param emb_size: dimensionality of embedding layer\n",
    "        :param hidden_size: dimensionality of hidden layers\n",
    "        :param num_classes: number of classes\n",
    "        :param prior_p1: (scalar) prior Bernoulli parameter\n",
    "        :param det_prior: (boolean) whether the prior parameter is deterministic\n",
    "        :param beta_shape: (pair of positive scalars) \n",
    "            when the prior parameter is stochastic\n",
    "            it is sampled from a Beta distribution (ignore this at first)\n",
    "        :param dropout: (scalar) dropout rate\n",
    "        :param layer_cls: type of encoder for classification\n",
    "        :param layer_inf: type of encoder for inference\n",
    "        \"\"\"\n",
    "        super(Model, self).__init__()\n",
    "\n",
    "        self.vocab = vocab\n",
    "        self.embed = embed = nn.Embedding(vocab_size, emb_size, padding_idx=1)\n",
    "\n",
    "        self.cls_net = Classifier(\n",
    "            embed=embed, \n",
    "            hidden_size=hidden_size, \n",
    "            output_size=num_classes,\n",
    "            dropout=dropout, \n",
    "            layer=layer_cls\n",
    "        )\n",
    "        \n",
    "        self.inference_net = ProductOfBernoullis(\n",
    "            embed=embed, \n",
    "            hidden_size=hidden_size,\n",
    "            layer=layer_inf\n",
    "        )\n",
    "        \n",
    "        self._prior_p1 = prior_p1\n",
    "        self._det_prior = det_prior\n",
    "        self._beta_shape = beta_shape\n",
    "        \n",
    "    def get_prior_p1(self, p_min=0.001, p_max=0.999):\n",
    "        \"\"\"Return the prior Bernoulli parameter\"\"\"\n",
    "        if self._det_prior:\n",
    "            return torch.tensor(self._prior_p1)\n",
    "        else:\n",
    "            a, b = self._beta_shape\n",
    "            prior_p1 = np.random.beta(a, b)\n",
    "            prior_p1 = max(prior_p1, p_min)\n",
    "            prior_p1 = min(prior_p1, p_max)\n",
    "        return torch.tensor(prior_p1)\n",
    "\n",
    "    def predict(self, py: Categorical, **kwargs):\n",
    "        \"\"\"\n",
    "        Predict deterministically using argmax.\n",
    "        :param py: B Categorical distributions (one per instance in batch)\n",
    "        :return: predictions\n",
    "            [B] sentiment levels\n",
    "        \"\"\"\n",
    "        assert not self.training, \"should be in eval mode for prediction\"\n",
    "        return py.log_probs.argmax(-1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Generate a sequence z with inference model, \n",
    "         then predict with rationale xz, that is, x masked by z.\n",
    "\n",
    "        :param x: [B, T] documents        \n",
    "        :param mask: [B, T] indicates valid positions vs padded positions\n",
    "        :return: \n",
    "            Categorical distributions P(y|x, z)\n",
    "            Bernoulli distributions Q(z|x)\n",
    "            Single sample z ~ Q(z|x) used for the conditional P(y|x, z)\n",
    "        \"\"\"\n",
    "        bern = self.inference_net(x, x != 1)\n",
    "        z = bern.sample() if self.training else (bern.probs >= 0.5).byte()\n",
    "        res = self.cls_net(x, x != 1, z)\n",
    "        return res, bern, z\n",
    "\n",
    "    def get_loss(self,                   \n",
    "                 y, \n",
    "                 py: Categorical,\n",
    "                 qz: Bernoulli, \n",
    "                 z, \n",
    "                 mask,\n",
    "                 iter_i=0, \n",
    "                 # you may ignore the rest of the arguments for the time being\n",
    "                 #  leave them as they are\n",
    "                 kl_weight=1.0,\n",
    "                 min_kl=0.0,\n",
    "                 ll_mean=0.,\n",
    "                 ll_std=1.,\n",
    "                 **kwargs):\n",
    "        \"\"\"\n",
    "        This computes the loss for the whole model.\n",
    "\n",
    "        :param y: target labels [B]\n",
    "        :param py: conditionals P(y|x, z)\n",
    "        :param qz: approximate posteriors Q(z|x)\n",
    "        :param z: sample of binary selectors [B, T]\n",
    "        :param mask: indicates valid positions [B, T]\n",
    "        :param iter_i: indicates the iteration\n",
    "        :param kl_weight: (scalar) multiplies the KL term\n",
    "        :param min_kl: (scalar) sets a minimum for the KL (aka free bits)\n",
    "        :param ll_mean: (scalar) running average of reward\n",
    "        :param ll_std: (scalar) running standard deviation of reward\n",
    "        :return: loss (torch node), terms (dict)\n",
    "        \n",
    "            terms is an OrderedDict that holds the scalar items involved in the loss\n",
    "            e.g. `terms['ll'] = ll.item()` is the log-likelihood term\n",
    "            \n",
    "            Consider tracking the following:\n",
    "            Single-sample ELBO: terms['elbo']\n",
    "            Log-Likelihood log P(y|x,z): terms['ll']\n",
    "            KL: terms['kl']\n",
    "            Score function surrogate log P(y|z, x) log Q(z|x): terms['sf']            \n",
    "            Rate of selected words: terms['selected']\n",
    "        \"\"\"\n",
    "        float_mask = mask.float()\n",
    "        \n",
    "        ll = (py.log_pmf(y).unsqueeze(-1) - ll_mean) * float_mask / ll_std\n",
    "        kl = qz.kl(Bernoulli(probs=self.get_prior_p1())) * float_mask\n",
    "        sf = qz.log_pmf(z.float()) * ll.detach() * float_mask\n",
    "        elbo = ll + sf - kl_weight * kl\n",
    "        \n",
    "        terms = OrderedDict()\n",
    "        terms['elbo'] = elbo.mean().item()\n",
    "        terms['sf'] = sf.mean().item()\n",
    "        terms['selected'] = z.sum().item() * 1.0 / z.shape[0] / z.shape[1]\n",
    "        terms['kl'] = kl.mean().item()\n",
    "        terms['ll'] = ll.mean().item()\n",
    "        \n",
    "        return -elbo.mean(), terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wNQDXTpqWvZa"
   },
   "outputs": [],
   "source": [
    "# This will be used later for maintaining runnin averages of quantites like \n",
    "#  terms in the ELBO\n",
    "from collections import deque\n",
    "\n",
    "class MovingStats:\n",
    "    \n",
    "    def __init__(self, memory=-1):\n",
    "        self.data = deque([])\n",
    "        self.memory = memory\n",
    "        \n",
    "    def append(self, value):\n",
    "        if self.memory != 0:\n",
    "            if self.memory > 0 and len(self.data) == self.memory:\n",
    "                self.data.popleft()\n",
    "            self.data.append(value)\n",
    "        \n",
    "    def mean(self):\n",
    "        if len(self.data):\n",
    "            return np.mean([x for x in self.data])\n",
    "        else:\n",
    "            return 0.\n",
    "    \n",
    "    def std(self):\n",
    "        return np.std(self.data) if len(self.data) > 1 else 1.\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "081YSfU9WvZc"
   },
   "source": [
    "# Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9Pc80gseWvZd"
   },
   "outputs": [],
   "source": [
    "# some helper code for mini batching\n",
    "#  this will take care of annoying things such as \n",
    "#  sorting training instances by length (necessary for pytorch's LSTM, for example)\n",
    "from sst.util import make_kv_string, get_minibatch, prepare_minibatch, print_parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_WVr97kilIRV"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Configuration\n",
      "training_path        : ../sst/data/sst/train.txt\n",
      "dev_path             : ../sst/data/sst/dev.txt\n",
      "test_path            : ../sst/data/sst/test.txt\n",
      "word_vectors         : ../sst/data/sst/glove.840B.300d.filtered.txt\n",
      "prior_p1             :        0.3\n",
      "beta_a               :        0.6\n",
      "beta_b               :        0.6\n",
      "det_prior            :          1\n",
      "num_epochs           :         50\n",
      "print_every          :        100\n",
      "eval_every           :         -1\n",
      "batch_size           :         25\n",
      "eval_batch_size      :         25\n",
      "subphrases           :          0\n",
      "min_phrase_length    :          2\n",
      "lowercase            :          1\n",
      "fix_emb              :          1\n",
      "embed_size           :        300\n",
      "hidden_size          :        150\n",
      "num_layers           :          1\n",
      "dropout              :        0.5\n",
      "layer_inf            : bow       \n",
      "layer_cls            : bow       \n",
      "save_path            : data/results\n",
      "baseline_memory      :       1000\n",
      "min_kl               :        0.0\n",
      "kl_weight            :        0.0\n",
      "kl_inc               :      1e-05\n",
      "lr                   :     0.0002\n",
      "weight_decay         :      1e-05\n",
      "lr_decay             :        0.5\n",
      "patience             :          5\n",
      "cooldown             :          5\n",
      "threshold            :     0.0001\n",
      "min_lr               :      1e-05\n",
      "max_grad_norm        :        5.0\n",
      "Set eval_every to 341\n",
      "Loading data\n",
      "train 8544\n",
      "dev 1101\n",
      "test 2210\n",
      "\n",
      "# Example\n",
      "First dev example: Example(tokens=['it', \"'s\", 'a', 'lovely', 'film', 'with', 'lovely', 'performances', 'by', 'buy', 'and', 'accorsi', '.'], label=3, transitions=[0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1], token_labels=[2, 2, 2, 3, 2, 2, 3, 2, 2, 2, 2, 2, 2])\n",
      "First dev example tokens: ['it', \"'s\", 'a', 'lovely', 'film', 'with', 'lovely', 'performances', 'by', 'buy', 'and', 'accorsi', '.']\n",
      "First dev example label: 3\n"
     ]
    }
   ],
   "source": [
    "import torch.optim\n",
    "# We will use Adam\n",
    "from torch.optim import Adam\n",
    "# and a couple of tricks to reduce learning rate on plateau\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "# here is some helper code to evaluate your model\n",
    "from sst.evaluate import evaluate\n",
    "\n",
    "\n",
    "cfg = dict()\n",
    "\n",
    "# Data\n",
    "cfg['training_path'] = \"../sst/data/sst/train.txt\"\n",
    "cfg['dev_path'] = \"../sst/data/sst/dev.txt\"\n",
    "cfg['test_path'] = \"../sst/data/sst/test.txt\"\n",
    "cfg['word_vectors'] = '../sst/data/sst/glove.840B.300d.filtered.txt'\n",
    "# Model\n",
    "cfg['prior_p1'] = 0.3\n",
    "cfg['beta_a'] = 0.6\n",
    "cfg['beta_b'] = 0.6\n",
    "cfg['det_prior'] = True\n",
    "# Architecture\n",
    "cfg['num_epochs'] = 50\n",
    "cfg['print_every'] = 100\n",
    "cfg['eval_every'] = -1\n",
    "cfg['batch_size'] = 25\n",
    "cfg['eval_batch_size'] = 25\n",
    "cfg['subphrases'] = False\n",
    "cfg['min_phrase_length'] = 2\n",
    "cfg['lowercase'] = True\n",
    "cfg['fix_emb'] = True\n",
    "cfg['embed_size'] = 300\n",
    "cfg['hidden_size'] = 150\n",
    "cfg['num_layers'] = 1\n",
    "cfg['dropout'] = 0.5\n",
    "cfg['layer_inf'] = 'bow'\n",
    "cfg['layer_cls'] = 'bow'\n",
    "cfg['save_path'] = 'data/results'\n",
    "cfg['baseline_memory'] = 1000\n",
    "cfg['min_kl'] = 0.  # use more than 0 to enable free bits\n",
    "cfg['kl_weight'] = 0.0  # start from zero to enable annealing\n",
    "cfg['kl_inc'] = 0.00001\n",
    "# Optimiser (leave as is)\n",
    "cfg['lr'] = 0.0002\n",
    "cfg['weight_decay'] = 1e-5\n",
    "cfg['lr_decay'] = 0.5\n",
    "cfg['patience'] = 5\n",
    "cfg['cooldown'] = 5\n",
    "cfg['threshold'] = 1e-4\n",
    "cfg['min_lr'] = 1e-5\n",
    "cfg['max_grad_norm'] = 5.\n",
    "\n",
    "\n",
    "print('# Configuration')\n",
    "for k, v in cfg.items():\n",
    "    print(\"{:20} : {:10}\".format(k, v))\n",
    "\n",
    "\n",
    "iters_per_epoch = len(train_data) // cfg[\"batch_size\"]\n",
    "\n",
    "if cfg[\"eval_every\"] == -1:\n",
    "    eval_every = iters_per_epoch\n",
    "    print(\"Set eval_every to {}\".format(iters_per_epoch))\n",
    "\n",
    "\n",
    "# Let's load the data into memory.\n",
    "print(\"Loading data\")\n",
    "train_data = list(examplereader(\n",
    "    cfg['training_path'],\n",
    "    lower=cfg['lowercase'], \n",
    "    subphrases=cfg['subphrases'],\n",
    "    min_length=cfg['min_phrase_length']))\n",
    "dev_data = list(examplereader(cfg['dev_path'], lower=cfg['lowercase']))\n",
    "test_data = list(examplereader(cfg['test_path'], lower=cfg['lowercase']))\n",
    "\n",
    "print(\"train\", len(train_data))\n",
    "print(\"dev\", len(dev_data))\n",
    "print(\"test\", len(test_data))\n",
    "\n",
    "print('\\n# Example')\n",
    "example = dev_data[0]\n",
    "print(\"First dev example:\", example)\n",
    "print(\"First dev example tokens:\", example.tokens)\n",
    "print(\"First dev example label:\", example.label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6PMqtVj0WvZf",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def train():\n",
    "\n",
    "    # Create a vocabulary object to map str <-> int\n",
    "    vocab = Vocabulary()  # populated by load_glove\n",
    "    glove_path = cfg[\"word_vectors\"]\n",
    "    vectors = load_glove(glove_path, vocab)\n",
    "\n",
    "    # You may consider using tensorboardX\n",
    "    # writer = SummaryWriter(log_dir=cfg[\"save_path\"])\n",
    "\n",
    "    # Map the sentiment labels 0-4 to a more readable form (and the opposite)\n",
    "    i2t = [\"very negative\", \"negative\", \"neutral\", \"positive\", \"very positive\"]\n",
    "    t2i = OrderedDict({p: i for p, i in zip(i2t, range(len(i2t)))})\n",
    "\n",
    "\n",
    "    print('\\n# Constructing model')\n",
    "    model = Model(\n",
    "        vocab_size=len(vocab.w2i), \n",
    "        emb_size=cfg[\"embed_size\"],\n",
    "        hidden_size=cfg[\"hidden_size\"], \n",
    "        num_classes=len(t2i),\n",
    "        prior_p1=cfg['prior_p1'],\n",
    "        det_prior=cfg['det_prior'],\n",
    "        beta_shape=[cfg['beta_a'], cfg['beta_b']],\n",
    "        vocab=vocab, \n",
    "        dropout=cfg[\"dropout\"], \n",
    "        layer_cls=cfg[\"layer_cls\"],\n",
    "        layer_inf=cfg[\"layer_inf\"]\n",
    "    )\n",
    "\n",
    "    print('\\n# Loading embeddings')\n",
    "    with torch.no_grad():\n",
    "        model.embed.weight.data.copy_(torch.from_numpy(vectors))\n",
    "        if cfg[\"fix_emb\"]:\n",
    "            print(\"fixed word embeddings\")\n",
    "            model.embed.weight.requires_grad = False\n",
    "        model.embed.weight[1] = 0.  # padding zero\n",
    "\n",
    "        \n",
    "    # Congigure optimiser\n",
    "    optimizer = Adam(model.parameters(), lr=cfg[\"lr\"],\n",
    "                     weight_decay=cfg[\"weight_decay\"])\n",
    "    # and learning rate scheduler\n",
    "    scheduler = ReduceLROnPlateau(\n",
    "        optimizer, mode=\"min\", factor=cfg[\"lr_decay\"], patience=cfg[\"patience\"],\n",
    "        verbose=True, cooldown=cfg[\"cooldown\"], threshold=cfg[\"threshold\"],\n",
    "        min_lr=cfg[\"min_lr\"]\n",
    "    )\n",
    "\n",
    "    # Prepare a few auxiliary variables\n",
    "    iter_i = 0\n",
    "    train_loss = 0.\n",
    "    print_num = 0\n",
    "    losses = []\n",
    "    accuracies = []\n",
    "    best_eval = 1.0e9\n",
    "    best_iter = 0\n",
    "\n",
    "    model = model.to(device)\n",
    "\n",
    "    # Some debugging info\n",
    "    print(model)\n",
    "    print_parameters(model)\n",
    "\n",
    "    batch_size = cfg['batch_size']\n",
    "    eval_batch_size = cfg['eval_batch_size']\n",
    "    print_every = cfg['print_every']\n",
    "\n",
    "    # Parameters of tricks to better optimise the ELBO \n",
    "    kl_inc = cfg['kl_inc']\n",
    "    kl_weight = cfg['kl_weight']\n",
    "    min_kl = cfg['min_kl']\n",
    "    # Running estimates for baselines\n",
    "    ll_moving_stats = MovingStats(cfg['baseline_memory'])\n",
    "\n",
    "    while True:  # when we run out of examples, shuffle and continue\n",
    "        epoch = iter_i // iters_per_epoch\n",
    "        if epoch > cfg['num_epochs']:\n",
    "            break\n",
    "        \n",
    "        for batch in get_minibatch(train_data, batch_size=batch_size, shuffle=True):\n",
    "\n",
    "            epoch = iter_i // iters_per_epoch\n",
    "            if epoch > cfg['num_epochs']:\n",
    "                break\n",
    "\n",
    "            # forward pass\n",
    "            model.train()\n",
    "            x, y, _ = prepare_minibatch(batch, model.vocab, device=device)\n",
    "            \n",
    "            mask = (x != 1)\n",
    "            py, qz, z = model(x)\n",
    "\n",
    "            # \"KL annealing\"\n",
    "            kl_weight += kl_inc\n",
    "            if kl_weight > 1.:\n",
    "                kl_weight = 1.0\n",
    "                \n",
    "            loss, terms = model.get_loss(\n",
    "                y,\n",
    "                py=py, \n",
    "                qz=qz,\n",
    "                z=z,\n",
    "                mask=mask, \n",
    "                kl_weight=kl_weight,\n",
    "                min_kl=min_kl,\n",
    "                ll_mean=ll_moving_stats.mean(),\n",
    "                ll_std=ll_moving_stats.std(),\n",
    "                iter_i=iter_i)\n",
    "\n",
    "            train_loss += loss.item()\n",
    "            \n",
    "            # keep an running estimate of the reward (log P(y|x,z))\n",
    "            ll_moving_stats.append(terms['ll'])\n",
    "\n",
    "            # backward pass\n",
    "            model.zero_grad()  # erase previous gradients\n",
    "\n",
    "            loss.backward()  # compute new gradients\n",
    "\n",
    "            # gradient clipping generally helps\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=cfg['max_grad_norm'])\n",
    "\n",
    "            # update weights\n",
    "            optimizer.step()\n",
    "\n",
    "            print_num += 1\n",
    "            iter_i += 1\n",
    "\n",
    "            # print info\n",
    "            if iter_i % print_every == 0:\n",
    "\n",
    "                train_loss = train_loss / print_every\n",
    "\n",
    "                print_str = make_kv_string(terms)\n",
    "                print(\"Epoch %r Iter %r loss=%.4f %s\" %\n",
    "                      (epoch, iter_i, train_loss, print_str))\n",
    "                losses.append(train_loss)\n",
    "                print_num = 0\n",
    "                train_loss = 0.\n",
    "\n",
    "            # evaluate\n",
    "            if iter_i % eval_every == 0:\n",
    "\n",
    "                dev_eval, rationales = evaluate(\n",
    "                    model, dev_data, \n",
    "                    batch_size=eval_batch_size, \n",
    "                    device=device,\n",
    "                    cfg=cfg, iter_i=iter_i\n",
    "                )\n",
    "                accuracies.append(dev_eval[\"acc\"])\n",
    "\n",
    "                print(\"\\n# epoch %r iter %r: dev %s\" % (\n",
    "                    epoch, iter_i, make_kv_string(dev_eval)))\n",
    "                \n",
    "                for exid in range(3):\n",
    "                    print(' dev%d [gold=%d,pred=%d]:' % (exid, dev_data[exid].label, rationales[exid][1]),  \n",
    "                          ' '.join(rationales[exid][0]))\n",
    "                print()\n",
    "\n",
    "                # adjust learning rate\n",
    "                scheduler.step(dev_eval[\"loss\"])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "A5uYKcw-WvZl",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "# Constructing model\n",
      "Classifier #params: 1505\n",
      "ProductOfBernoullis #params: 301\n",
      "\n",
      "# Loading embeddings\n",
      "fixed word embeddings\n",
      "Model(\n",
      "  (embed): Embedding(20727, 300, padding_idx=1)\n",
      "  (cls_net): Classifier(\n",
      "    (embed_layer): Sequential(\n",
      "      (0): Embedding(20727, 300, padding_idx=1)\n",
      "    )\n",
      "    (enc_layer): BagOfWordsEncoder()\n",
      "    (output_layer): Sequential(\n",
      "      (0): Dropout(p=0.5)\n",
      "      (1): Linear(in_features=300, out_features=5, bias=True)\n",
      "      (2): LogSoftmax()\n",
      "    )\n",
      "  )\n",
      "  (inference_net): ProductOfBernoullis(\n",
      "    (embed_layer): Sequential(\n",
      "      (0): Embedding(20727, 300, padding_idx=1)\n",
      "    )\n",
      "    (enc_layer): BagOfWordsEncoder()\n",
      "    (logit_layer): Linear(in_features=300, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "embed.weight             [20727, 300] requires_grad=False\n",
      "cls_net.output_layer.1.weight [5, 300]     requires_grad=True\n",
      "cls_net.output_layer.1.bias [5]          requires_grad=True\n",
      "inference_net.logit_layer.weight [1, 300]     requires_grad=True\n",
      "inference_net.logit_layer.bias [1]          requires_grad=True\n",
      "\n",
      "Total parameters: 6219906\n",
      "\n",
      "Shuffling training data\n",
      "Epoch 0 Iter 100 loss=0.3438 elbo -0.3945 sf 0.8283 selected 0.4929 kl 0.0698 ll -1.2228\n",
      "Epoch 0 Iter 200 loss=0.3521 elbo -0.3050 sf 0.5897 selected 0.5482 kl 0.0924 ll -0.8946\n",
      "Epoch 0 Iter 300 loss=0.3444 elbo -0.3140 sf 0.6560 selected 0.5495 kl 0.0891 ll -0.9698\n",
      "\n",
      "# epoch 0 iter 341: dev loss 0.5275 elbo -0.5275 sf 0.3551 selected 0.9935 kl 0.1245 ll -0.7581 acc 0.3760\n",
      " dev0 [gold=3,pred=1]: **it** **'s** **a** **lovely** **film** **with** **lovely** **performances** **by** buy **and** **accorsi** **.**\n",
      " dev1 [gold=2,pred=3]: **no** **one** **goes** **unindicted** **here** **,** **which** **is** **probably** **for** **the** **best** **.**\n",
      " dev2 [gold=3,pred=1]: **and** **if** **you** **'re** **not** **nearly** **moved** **to** **tears** **by** **a** **couple** **of** **scenes** **,** **you** **'ve** **got** ice **water** **in** **your** veins **.**\n",
      "\n",
      "Shuffling training data\n",
      "Epoch 1 Iter 400 loss=0.3485 elbo -0.3129 sf 0.5309 selected 0.5814 kl 0.1215 ll -0.8435\n",
      "Epoch 1 Iter 500 loss=0.3969 elbo -0.4158 sf 0.5920 selected 0.6137 kl 0.1463 ll -1.0075\n",
      "Epoch 1 Iter 600 loss=0.3688 elbo -0.2503 sf 0.4279 selected 0.5787 kl 0.1339 ll -0.6778\n",
      "\n",
      "# epoch 1 iter 682: dev loss 0.6185 elbo -0.6185 sf 0.2874 selected 0.9871 kl 0.1753 ll -0.7306 acc 0.3860\n",
      " dev0 [gold=3,pred=1]: **it** **'s** **a** **lovely** **film** **with** **lovely** **performances** **by** buy **and** **accorsi** **.**\n",
      " dev1 [gold=2,pred=3]: **no** **one** **goes** **unindicted** **here** **,** **which** **is** **probably** **for** **the** **best** **.**\n",
      " dev2 [gold=3,pred=1]: **and** **if** **you** **'re** **not** **nearly** **moved** **to** **tears** **by** **a** **couple** **of** **scenes** **,** **you** **'ve** **got** **ice** **water** **in** **your** **veins** **.**\n",
      "\n",
      "Shuffling training data\n",
      "Epoch 2 Iter 700 loss=0.3769 elbo -0.3104 sf 0.5745 selected 0.6247 kl 0.1991 ll -0.8842\n",
      "Epoch 2 Iter 800 loss=0.3801 elbo -0.3374 sf 0.5472 selected 0.6086 kl 0.1613 ll -0.8839\n",
      "Epoch 2 Iter 900 loss=0.3974 elbo -0.4462 sf 0.5032 selected 0.6389 kl 0.2366 ll -0.9483\n",
      "Epoch 2 Iter 1000 loss=0.3942 elbo -0.3994 sf 0.5520 selected 0.6226 kl 0.1868 ll -0.9505\n",
      "\n",
      "# epoch 2 iter 1023: dev loss 0.6640 elbo -0.6640 sf 0.2578 selected 0.9546 kl 0.2035 ll -0.7183 acc 0.4005\n",
      " dev0 [gold=3,pred=1]: **it** **'s** **a** **lovely** **film** **with** **lovely** **performances** by buy **and** **accorsi** .\n",
      " dev1 [gold=2,pred=3]: **no** **one** **goes** unindicted here **,** **which** **is** **probably** for **the** **best** .\n",
      " dev2 [gold=3,pred=1]: **and** **if** **you** **'re** **not** **nearly** **moved** **to** **tears** by **a** **couple** **of** **scenes** **,** **you** **'ve** **got** **ice** **water** **in** **your** **veins** .\n",
      "\n",
      "Shuffling training data\n",
      "Epoch 3 Iter 1100 loss=0.3901 elbo -0.4427 sf 0.6979 selected 0.5829 kl 0.1445 ll -1.1398\n",
      "Epoch 3 Iter 1200 loss=0.4189 elbo -0.4375 sf 0.5473 selected 0.6627 kl 0.2202 ll -0.9834\n",
      "Epoch 3 Iter 1300 loss=0.3953 elbo -0.4379 sf 0.6160 selected 0.5749 kl 0.1418 ll -1.0531\n",
      "\n",
      "# epoch 3 iter 1364: dev loss 0.5965 elbo -0.5965 sf 0.2810 selected 0.9019 kl 0.1671 ll -0.7104 acc 0.3924\n",
      " dev0 [gold=3,pred=1]: **it** **'s** **a** **lovely** **film** **with** **lovely** **performances** by buy **and** **accorsi** .\n",
      " dev1 [gold=2,pred=3]: **no** **one** **goes** unindicted here , **which** **is** **probably** for **the** **best** .\n",
      " dev2 [gold=3,pred=1]: **and** **if** **you** **'re** **not** **nearly** moved to **tears** by **a** **couple** **of** **scenes** , **you** **'ve** **got** **ice** water in **your** **veins** .\n",
      "\n",
      "Shuffling training data\n",
      "Epoch 4 Iter 1400 loss=0.4060 elbo -0.3302 sf 0.3910 selected 0.6095 kl 0.1665 ll -0.7200\n",
      "Epoch 4 Iter 1500 loss=0.4087 elbo -0.3987 sf 0.4228 selected 0.6635 kl 0.1732 ll -0.8202\n",
      "Epoch 4 Iter 1600 loss=0.4140 elbo -0.3664 sf 0.6162 selected 0.6064 kl 0.1622 ll -0.9813\n",
      "Epoch 4 Iter 1700 loss=0.4084 elbo -0.4642 sf 0.5141 selected 0.6100 kl 0.2077 ll -0.9765\n",
      "\n",
      "# epoch 4 iter 1705: dev loss 0.6086 elbo -0.6086 sf 0.2676 selected 0.8853 kl 0.1741 ll -0.7021 acc 0.4105\n",
      " dev0 [gold=3,pred=1]: **it** 's **a** **lovely** **film** **with** **lovely** **performances** by buy **and** **accorsi** .\n",
      " dev1 [gold=2,pred=1]: **no** **one** **goes** **unindicted** here , **which** **is** **probably** for **the** **best** .\n",
      " dev2 [gold=3,pred=1]: **and** **if** **you** **'re** **not** **nearly** moved to **tears** by **a** **couple** **of** **scenes** , **you** **'ve** **got** **ice** **water** in **your** **veins** .\n",
      "\n",
      "Shuffling training data\n",
      "Epoch 5 Iter 1800 loss=0.3831 elbo -0.2934 sf 0.3741 selected 0.6246 kl 0.2257 ll -0.6654\n",
      "Epoch 5 Iter 1900 loss=0.4191 elbo -0.6326 sf 0.8397 selected 0.6259 kl 0.2227 ll -1.4702\n",
      "Epoch 5 Iter 2000 loss=0.4101 elbo -0.4233 sf 0.4929 selected 0.6010 kl 0.1602 ll -0.9147\n",
      "\n",
      "# epoch 5 iter 2046: dev loss 0.5807 elbo -0.5807 sf 0.2754 selected 0.8376 kl 0.1613 ll -0.6949 acc 0.4260\n",
      " dev0 [gold=3,pred=1]: **it** 's a **lovely** **film** **with** **lovely** **performances** by buy **and** **accorsi** .\n",
      " dev1 [gold=2,pred=1]: **no** **one** **goes** **unindicted** here **,** **which** **is** **probably** for the **best** .\n",
      " dev2 [gold=3,pred=1]: **and** **if** **you** **'re** **not** **nearly** moved to **tears** by a **couple** of **scenes** **,** **you** **'ve** **got** **ice** **water** in **your** **veins** .\n",
      "\n",
      "Shuffling training data\n",
      "Epoch 6 Iter 2100 loss=0.3971 elbo -0.4522 sf 0.5691 selected 0.5813 kl 0.1237 ll -1.0201\n",
      "Epoch 6 Iter 2200 loss=0.3857 elbo -0.2693 sf 0.4253 selected 0.5642 kl 0.1377 ll -0.6931\n",
      "Epoch 6 Iter 2300 loss=0.4072 elbo -0.2923 sf 0.5155 selected 0.5438 kl 0.1181 ll -0.8065\n",
      "\n",
      "# epoch 6 iter 2387: dev loss 0.5989 elbo -0.5989 sf 0.2478 selected 0.7518 kl 0.1476 ll -0.6991 acc 0.4251\n",
      " dev0 [gold=3,pred=1]: it 's a **lovely** **film** with **lovely** **performances** by buy and **accorsi** .\n",
      " dev1 [gold=2,pred=1]: **no** **one** **goes** **unindicted** here , which **is** **probably** for the **best** .\n",
      " dev2 [gold=3,pred=1]: and **if** you **'re** **not** **nearly** moved to **tears** by a **couple** of **scenes** , you **'ve** got ice water in your **veins** .\n",
      "\n",
      "Epoch     6: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Shuffling training data\n",
      "Epoch 7 Iter 2400 loss=0.4202 elbo -0.5115 sf 0.6443 selected 0.5271 kl 0.1507 ll -1.1539\n",
      "Epoch 7 Iter 2500 loss=0.4213 elbo -0.3847 sf 0.4941 selected 0.5467 kl 0.1485 ll -0.8769\n",
      "Epoch 7 Iter 2600 loss=0.4053 elbo -0.4153 sf 0.4245 selected 0.5547 kl 0.1631 ll -0.8377\n",
      "Epoch 7 Iter 2700 loss=0.4396 elbo -0.4218 sf 0.4447 selected 0.5613 kl 0.1774 ll -0.8642\n",
      "\n",
      "# epoch 7 iter 2728: dev loss 0.6280 elbo -0.6280 sf 0.2352 selected 0.7972 kl 0.1693 ll -0.6938 acc 0.4242\n",
      " dev0 [gold=3,pred=1]: **it** 's a **lovely** **film** with **lovely** **performances** by buy and **accorsi** .\n",
      " dev1 [gold=2,pred=1]: **no** **one** **goes** unindicted here , which **is** **probably** for the **best** .\n",
      " dev2 [gold=3,pred=1]: and **if** **you** **'re** **not** **nearly** moved to **tears** by a **couple** of **scenes** , **you** **'ve** **got** **ice** **water** in **your** **veins** .\n",
      "\n",
      "Shuffling training data\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8 Iter 2800 loss=0.4286 elbo -0.3662 sf 0.4344 selected 0.6195 kl 0.1767 ll -0.7982\n",
      "Epoch 8 Iter 2900 loss=0.3989 elbo -0.2740 sf 0.2721 selected 0.5800 kl 0.1871 ll -0.5434\n",
      "Epoch 8 Iter 3000 loss=0.4136 elbo -0.3209 sf 0.3806 selected 0.5567 kl 0.1534 ll -0.6992\n",
      "\n",
      "# epoch 8 iter 3069: dev loss 0.6140 elbo -0.6140 sf 0.2376 selected 0.7705 kl 0.1568 ll -0.6948 acc 0.4323\n",
      " dev0 [gold=3,pred=1]: **it** 's a **lovely** **film** with **lovely** **performances** by buy and **accorsi** .\n",
      " dev1 [gold=2,pred=1]: **no** **one** **goes** unindicted here , which **is** **probably** for the **best** .\n",
      " dev2 [gold=3,pred=1]: and **if** you **'re** **not** **nearly** moved to **tears** by a **couple** of **scenes** , you **'ve** **got** ice water in your **veins** .\n",
      "\n",
      "Shuffling training data\n",
      "Epoch 9 Iter 3100 loss=0.4210 elbo -0.4472 sf 0.4804 selected 0.5300 kl 0.1492 ll -0.9253\n",
      "Epoch 9 Iter 3200 loss=0.4061 elbo -0.2403 sf 0.3270 selected 0.5473 kl 0.1387 ll -0.5651\n",
      "Epoch 9 Iter 3300 loss=0.4026 elbo -0.3244 sf 0.4252 selected 0.5648 kl 0.1966 ll -0.7464\n",
      "Epoch 9 Iter 3400 loss=0.4282 elbo -0.4500 sf 0.4721 selected 0.5558 kl 0.1451 ll -0.9197\n",
      "\n",
      "# epoch 9 iter 3410: dev loss 0.6146 elbo -0.6146 sf 0.2422 selected 0.7763 kl 0.1622 ll -0.6946 acc 0.4260\n",
      " dev0 [gold=3,pred=1]: **it** 's a **lovely** **film** with **lovely** **performances** by buy and **accorsi** .\n",
      " dev1 [gold=2,pred=1]: **no** **one** **goes** unindicted here , which **is** **probably** for the **best** .\n",
      " dev2 [gold=3,pred=1]: and **if** you **'re** **not** **nearly** moved to **tears** by a **couple** of **scenes** , you **'ve** **got** ice **water** in **your** **veins** .\n",
      "\n",
      "Shuffling training data\n",
      "Epoch 10 Iter 3500 loss=0.4216 elbo -0.2702 sf 0.3367 selected 0.5647 kl 0.1724 ll -0.6039\n",
      "Epoch 10 Iter 3600 loss=0.4201 elbo -0.5393 sf 0.5167 selected 0.4894 kl 0.1216 ll -1.0539\n",
      "Epoch 10 Iter 3700 loss=0.4549 elbo -0.5415 sf 0.5245 selected 0.4861 kl 0.1085 ll -1.0640\n",
      "\n",
      "# epoch 10 iter 3751: dev loss 0.6039 elbo -0.6039 sf 0.2324 selected 0.7412 kl 0.1422 ll -0.6941 acc 0.4223\n",
      " dev0 [gold=3,pred=1]: it 's a **lovely** **film** with **lovely** **performances** by buy and **accorsi** .\n",
      " dev1 [gold=2,pred=1]: **no** **one** **goes** unindicted here , which **is** **probably** for the **best** .\n",
      " dev2 [gold=3,pred=1]: and if you **'re** **not** **nearly** moved to **tears** by a **couple** of **scenes** , you **'ve** got ice water in your **veins** .\n",
      "\n",
      "Shuffling training data\n",
      "Epoch 11 Iter 3800 loss=0.4345 elbo -0.2660 sf 0.3004 selected 0.5584 kl 0.0972 ll -0.5645\n",
      "Epoch 11 Iter 3900 loss=0.4440 elbo -0.3854 sf 0.3843 selected 0.5164 kl 0.1421 ll -0.7670\n",
      "Epoch 11 Iter 4000 loss=0.4091 elbo -0.2774 sf 0.3011 selected 0.5424 kl 0.1305 ll -0.5759\n",
      "\n",
      "# epoch 11 iter 4092: dev loss 0.6170 elbo -0.6170 sf 0.2298 selected 0.7552 kl 0.1539 ll -0.6929 acc 0.4233\n",
      " dev0 [gold=3,pred=1]: **it** 's a **lovely** **film** with **lovely** **performances** by buy and **accorsi** .\n",
      " dev1 [gold=2,pred=1]: **no** **one** **goes** unindicted here , which **is** **probably** for the **best** .\n",
      " dev2 [gold=3,pred=1]: and **if** you **'re** **not** **nearly** moved to **tears** by a **couple** of **scenes** , you **'ve** got ice water in **your** **veins** .\n",
      "\n",
      "Epoch 12 Iter 4100 loss=0.4296 elbo -0.4412 sf 0.5495 selected 0.5544 kl 0.1708 ll -0.9872\n",
      "Shuffling training data\n",
      "Epoch 12 Iter 4200 loss=0.4160 elbo -0.4487 sf 0.5276 selected 0.5146 kl 0.1013 ll -0.9742\n",
      "Epoch 12 Iter 4300 loss=0.4184 elbo -0.8954 sf 1.0432 selected 0.5270 kl 0.1674 ll -1.9350\n",
      "Epoch 12 Iter 4400 loss=0.4322 elbo -0.3979 sf 0.6030 selected 0.5233 kl 0.1040 ll -0.9986\n",
      "\n",
      "# epoch 12 iter 4433: dev loss 0.6135 elbo -0.6135 sf 0.2287 selected 0.7558 kl 0.1506 ll -0.6915 acc 0.4342\n",
      " dev0 [gold=3,pred=1]: **it** 's a **lovely** **film** with **lovely** **performances** by buy and **accorsi** .\n",
      " dev1 [gold=2,pred=1]: **no** **one** **goes** unindicted here , which **is** **probably** for the **best** .\n",
      " dev2 [gold=3,pred=1]: and **if** **you** **'re** **not** **nearly** moved to **tears** by a **couple** of **scenes** , **you** **'ve** **got** ice water in **your** **veins** .\n",
      "\n",
      "Shuffling training data\n",
      "Epoch 13 Iter 4500 loss=0.4210 elbo -0.5497 sf 0.5689 selected 0.5264 kl 0.1150 ll -1.1161\n",
      "Epoch 13 Iter 4600 loss=0.4173 elbo -0.1777 sf 0.2490 selected 0.5010 kl 0.1176 ll -0.4240\n",
      "Epoch 13 Iter 4700 loss=0.4427 elbo -0.4225 sf 0.6203 selected 0.5509 kl 0.1285 ll -1.0398\n",
      "\n",
      "# epoch 13 iter 4774: dev loss 0.6113 elbo -0.6113 sf 0.2213 selected 0.7315 kl 0.1397 ll -0.6928 acc 0.4233\n",
      " dev0 [gold=3,pred=1]: it 's a **lovely** **film** with **lovely** **performances** by buy and **accorsi** .\n",
      " dev1 [gold=2,pred=1]: **no** **one** **goes** unindicted here , which **is** **probably** for the **best** .\n",
      " dev2 [gold=3,pred=1]: and if you **'re** **not** **nearly** moved to **tears** by a **couple** of **scenes** , you **'ve** got ice water in **your** **veins** .\n",
      "\n",
      "Shuffling training data\n",
      "Epoch 14 Iter 4800 loss=0.4260 elbo -0.2289 sf 0.2204 selected 0.5137 kl 0.1126 ll -0.4466\n",
      "Epoch 14 Iter 4900 loss=0.4319 elbo -0.5992 sf 0.5672 selected 0.4955 kl 0.1752 ll -1.1622\n",
      "Epoch 14 Iter 5000 loss=0.4389 elbo -0.1405 sf 0.1848 selected 0.5108 kl 0.1119 ll -0.3225\n",
      "Epoch 14 Iter 5100 loss=0.4207 elbo -0.3356 sf 0.3613 selected 0.5218 kl 0.1196 ll -0.6938\n",
      "\n",
      "# epoch 14 iter 5115: dev loss 0.6090 elbo -0.6090 sf 0.2256 selected 0.7499 kl 0.1446 ll -0.6900 acc 0.4305\n",
      " dev0 [gold=3,pred=1]: **it** 's a **lovely** film with **lovely** **performances** by buy and **accorsi** .\n",
      " dev1 [gold=2,pred=1]: **no** **one** **goes** unindicted here , which **is** **probably** for the **best** .\n",
      " dev2 [gold=3,pred=1]: and **if** **you** **'re** **not** **nearly** moved to **tears** by a **couple** of **scenes** , **you** **'ve** **got** ice water in **your** **veins** .\n",
      "\n",
      "Shuffling training data\n",
      "Epoch 15 Iter 5200 loss=0.4295 elbo -0.3043 sf 0.4596 selected 0.5200 kl 0.1307 ll -0.7606\n",
      "Epoch 15 Iter 5300 loss=0.4146 elbo -0.5223 sf 0.6542 selected 0.4984 kl 0.1263 ll -1.1731\n",
      "Epoch 15 Iter 5400 loss=0.4353 elbo -0.2914 sf 0.3229 selected 0.5044 kl 0.1040 ll -0.6115\n",
      "\n",
      "# epoch 15 iter 5456: dev loss 0.6114 elbo -0.6114 sf 0.2085 selected 0.7035 kl 0.1264 ll -0.6936 acc 0.4242\n",
      " dev0 [gold=3,pred=1]: it 's a **lovely** film with **lovely** **performances** by buy and **accorsi** .\n",
      " dev1 [gold=2,pred=1]: **no** one **goes** unindicted here , which is **probably** for the **best** .\n",
      " dev2 [gold=3,pred=1]: and if you **'re** **not** **nearly** moved to **tears** by a **couple** of **scenes** , you **'ve** got ice water in **your** **veins** .\n",
      "\n",
      "Shuffling training data\n",
      "Epoch 16 Iter 5500 loss=0.4627 elbo -0.4709 sf 0.4715 selected 0.4969 kl 0.1066 ll -0.9395\n",
      "Epoch 16 Iter 5600 loss=0.4297 elbo -0.3825 sf 0.3587 selected 0.4973 kl 0.1112 ll -0.7380\n",
      "Epoch 16 Iter 5700 loss=0.4331 elbo -0.4371 sf 0.4973 selected 0.4671 kl 0.1287 ll -0.9308\n",
      "\n",
      "# epoch 16 iter 5797: dev loss 0.6186 elbo -0.6186 sf 0.2018 selected 0.6969 kl 0.1263 ll -0.6941 acc 0.4214\n",
      " dev0 [gold=3,pred=1]: it 's a **lovely** film with **lovely** **performances** by buy and **accorsi** .\n",
      " dev1 [gold=2,pred=1]: **no** one **goes** unindicted here , which is **probably** for the **best** .\n",
      " dev2 [gold=3,pred=1]: and if you **'re** **not** **nearly** moved to **tears** by a **couple** of **scenes** , you **'ve** got ice water in **your** **veins** .\n",
      "\n",
      "Epoch 17 Iter 5800 loss=0.4473 elbo -0.3278 sf 0.3714 selected 0.4837 kl 0.1037 ll -0.6962\n",
      "Shuffling training data\n",
      "Epoch 17 Iter 5900 loss=0.4508 elbo -0.3682 sf 0.3620 selected 0.5102 kl 0.1226 ll -0.7266\n",
      "Epoch 17 Iter 6000 loss=0.4585 elbo -0.4039 sf 0.3616 selected 0.4820 kl 0.1265 ll -0.7617\n",
      "Epoch 17 Iter 6100 loss=0.4439 elbo -0.4105 sf 0.3180 selected 0.5020 kl 0.1127 ll -0.7251\n",
      "\n",
      "# epoch 17 iter 6138: dev loss 0.6195 elbo -0.6195 sf 0.2053 selected 0.7084 kl 0.1323 ll -0.6925 acc 0.4223\n",
      " dev0 [gold=3,pred=1]: it 's a **lovely** film with **lovely** **performances** by buy and **accorsi** .\n",
      " dev1 [gold=2,pred=1]: **no** one **goes** unindicted here , which is **probably** for the **best** .\n",
      " dev2 [gold=3,pred=1]: and if you **'re** **not** **nearly** moved to **tears** by a **couple** of **scenes** , you **'ve** got ice water in **your** **veins** .\n",
      "\n",
      "Epoch    17: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Shuffling training data\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18 Iter 6200 loss=0.4446 elbo -0.6023 sf 0.4841 selected 0.4922 kl 0.1359 ll -1.0822\n",
      "Epoch 18 Iter 6300 loss=0.4351 elbo -0.5435 sf 0.5980 selected 0.4971 kl 0.1561 ll -1.1366\n",
      "Epoch 18 Iter 6400 loss=0.4536 elbo -0.6569 sf 0.4501 selected 0.5006 kl 0.1745 ll -1.1014\n",
      "\n",
      "# epoch 18 iter 6479: dev loss 0.6205 elbo -0.6205 sf 0.2033 selected 0.7068 kl 0.1321 ll -0.6917 acc 0.4305\n",
      " dev0 [gold=3,pred=1]: it 's a **lovely** film with **lovely** **performances** by buy and **accorsi** .\n",
      " dev1 [gold=2,pred=1]: **no** one **goes** unindicted here , which is **probably** for the **best** .\n",
      " dev2 [gold=3,pred=1]: and if you **'re** **not** **nearly** moved to **tears** by a **couple** of **scenes** , you **'ve** got ice water in **your** **veins** .\n",
      "\n",
      "Shuffling training data\n",
      "Epoch 19 Iter 6500 loss=0.4401 elbo -0.3208 sf 0.2862 selected 0.5143 kl 0.0973 ll -0.6038\n",
      "Epoch 19 Iter 6600 loss=0.4342 elbo -0.4449 sf 0.4699 selected 0.4846 kl 0.1346 ll -0.9104\n",
      "Epoch 19 Iter 6700 loss=0.4496 elbo -0.3839 sf 0.3457 selected 0.5005 kl 0.1283 ll -0.7253\n",
      "Epoch 19 Iter 6800 loss=0.4290 elbo -0.4731 sf 0.4501 selected 0.4627 kl 0.1012 ll -0.9198\n",
      "\n",
      "# epoch 19 iter 6820: dev loss 0.6217 elbo -0.6217 sf 0.1992 selected 0.7013 kl 0.1281 ll -0.6928 acc 0.4242\n",
      " dev0 [gold=3,pred=1]: it 's a **lovely** film with **lovely** **performances** by buy and **accorsi** .\n",
      " dev1 [gold=2,pred=1]: **no** one **goes** unindicted here , which is **probably** for the **best** .\n",
      " dev2 [gold=3,pred=1]: and if you **'re** **not** **nearly** moved to **tears** by a **couple** of **scenes** , you **'ve** got ice water in **your** **veins** .\n",
      "\n",
      "Shuffling training data\n",
      "Epoch 20 Iter 6900 loss=0.4610 elbo -0.4621 sf 0.4115 selected 0.4626 kl 0.1084 ll -0.8699\n",
      "Epoch 20 Iter 7000 loss=0.4278 elbo -0.3735 sf 0.3441 selected 0.4583 kl 0.1500 ll -0.7124\n",
      "Epoch 20 Iter 7100 loss=0.4478 elbo -0.4518 sf 0.3631 selected 0.4530 kl 0.1266 ll -0.8104\n",
      "\n",
      "# epoch 20 iter 7161: dev loss 0.6183 elbo -0.6183 sf 0.2039 selected 0.7034 kl 0.1293 ll -0.6930 acc 0.4287\n",
      " dev0 [gold=3,pred=1]: it 's a **lovely** film with **lovely** **performances** by buy and **accorsi** .\n",
      " dev1 [gold=2,pred=1]: **no** one **goes** unindicted here , which is **probably** for the **best** .\n",
      " dev2 [gold=3,pred=1]: and if you **'re** **not** **nearly** moved to **tears** by a **couple** of **scenes** , you **'ve** got ice water in **your** **veins** .\n",
      "\n",
      "Shuffling training data\n",
      "Epoch 21 Iter 7200 loss=0.4213 elbo -0.4743 sf 0.4636 selected 0.4974 kl 0.1258 ll -0.9333\n",
      "Epoch 21 Iter 7300 loss=0.4654 elbo -0.3176 sf 0.3658 selected 0.5112 kl 0.1342 ll -0.6786\n",
      "Epoch 21 Iter 7400 loss=0.4794 elbo -0.4465 sf 0.3794 selected 0.4876 kl 0.1228 ll -0.8213\n",
      "Epoch 21 Iter 7500 loss=0.4555 elbo -0.5384 sf 0.6317 selected 0.4776 kl 0.1359 ll -1.1650\n",
      "\n",
      "# epoch 21 iter 7502: dev loss 0.6140 elbo -0.6140 sf 0.2008 selected 0.6932 kl 0.1226 ll -0.6922 acc 0.4342\n",
      " dev0 [gold=3,pred=1]: it 's a **lovely** film with **lovely** **performances** by buy and **accorsi** .\n",
      " dev1 [gold=2,pred=1]: **no** one **goes** unindicted here , which is **probably** for the **best** .\n",
      " dev2 [gold=3,pred=1]: and if you **'re** **not** **nearly** moved to **tears** by a **couple** of **scenes** , you **'ve** got ice water in your **veins** .\n",
      "\n",
      "Shuffling training data\n",
      "Epoch 22 Iter 7600 loss=0.4591 elbo -0.5121 sf 0.4129 selected 0.4896 kl 0.1076 ll -0.9209\n",
      "Epoch 22 Iter 7700 loss=0.4443 elbo -0.4910 sf 0.5389 selected 0.4809 kl 0.1199 ll -1.0252\n",
      "Epoch 22 Iter 7800 loss=0.4444 elbo -0.4155 sf 0.4013 selected 0.4656 kl 0.1089 ll -0.8126\n",
      "\n",
      "# epoch 22 iter 7843: dev loss 0.6105 elbo -0.6105 sf 0.2050 selected 0.6991 kl 0.1229 ll -0.6926 acc 0.4233\n",
      " dev0 [gold=3,pred=1]: it 's a **lovely** film with **lovely** **performances** by buy and **accorsi** .\n",
      " dev1 [gold=2,pred=1]: **no** one **goes** unindicted here , which is **probably** for the **best** .\n",
      " dev2 [gold=3,pred=1]: and if you **'re** **not** **nearly** moved to **tears** by a **couple** of **scenes** , you **'ve** got ice water in **your** **veins** .\n",
      "\n",
      "Shuffling training data\n",
      "Epoch 23 Iter 7900 loss=0.4274 elbo -0.3722 sf 0.3271 selected 0.4913 kl 0.1062 ll -0.6951\n",
      "Epoch 23 Iter 8000 loss=0.4501 elbo -0.2570 sf 0.2398 selected 0.5120 kl 0.1055 ll -0.4925\n",
      "Epoch 23 Iter 8100 loss=0.4288 elbo -0.6324 sf 0.8150 selected 0.4678 kl 0.1341 ll -1.4420\n",
      "\n",
      "# epoch 23 iter 8184: dev loss 0.6059 elbo -0.6059 sf 0.2028 selected 0.6858 kl 0.1171 ll -0.6916 acc 0.4296\n",
      " dev0 [gold=3,pred=1]: it 's a **lovely** film with **lovely** **performances** by buy and **accorsi** .\n",
      " dev1 [gold=2,pred=1]: **no** one **goes** unindicted here , which is **probably** for the **best** .\n",
      " dev2 [gold=3,pred=1]: and if you **'re** not **nearly** moved to **tears** by a **couple** of **scenes** , you 've got ice water in **your** **veins** .\n",
      "\n",
      "Epoch 24 Iter 8200 loss=0.4344 elbo -0.4021 sf 0.5379 selected 0.4893 kl 0.1071 ll -0.9357\n",
      "Shuffling training data\n",
      "Epoch 24 Iter 8300 loss=0.4549 elbo -0.3199 sf 0.3543 selected 0.4878 kl 0.1015 ll -0.6701\n",
      "Epoch 24 Iter 8400 loss=0.4482 elbo -0.2623 sf 0.3693 selected 0.4968 kl 0.1153 ll -0.6267\n",
      "Epoch 24 Iter 8500 loss=0.4235 elbo -0.3965 sf 0.3246 selected 0.4313 kl 0.1455 ll -0.7150\n",
      "\n",
      "# epoch 24 iter 8525: dev loss 0.6011 elbo -0.6011 sf 0.2039 selected 0.6812 kl 0.1133 ll -0.6917 acc 0.4296\n",
      " dev0 [gold=3,pred=1]: it 's a **lovely** film with **lovely** **performances** by buy and **accorsi** .\n",
      " dev1 [gold=2,pred=1]: **no** one **goes** unindicted here , which is **probably** for the **best** .\n",
      " dev2 [gold=3,pred=1]: and if you **'re** not **nearly** moved to **tears** by a **couple** of **scenes** , you 've got ice water in your **veins** .\n",
      "\n",
      "Shuffling training data\n",
      "Epoch 25 Iter 8600 loss=0.4706 elbo -0.3323 sf 0.3076 selected 0.4800 kl 0.0952 ll -0.6358\n",
      "Epoch 25 Iter 8700 loss=0.4510 elbo -0.6866 sf 0.5762 selected 0.4839 kl 0.1063 ll -1.2582\n",
      "Epoch 25 Iter 8800 loss=0.4552 elbo -0.4644 sf 0.5434 selected 0.4966 kl 0.0977 ll -1.0035\n",
      "\n",
      "# epoch 25 iter 8866: dev loss 0.5954 elbo -0.5954 sf 0.2093 selected 0.6833 kl 0.1124 ll -0.6924 acc 0.4323\n",
      " dev0 [gold=3,pred=1]: it 's a **lovely** film with **lovely** **performances** by buy and **accorsi** .\n",
      " dev1 [gold=2,pred=1]: **no** one **goes** unindicted here , which is **probably** for the **best** .\n",
      " dev2 [gold=3,pred=1]: and if you **'re** not **nearly** moved to **tears** by a **couple** of **scenes** , you **'ve** got ice water in your **veins** .\n",
      "\n",
      "Shuffling training data\n",
      "Epoch 26 Iter 8900 loss=0.4178 elbo -0.4236 sf 0.3507 selected 0.4820 kl 0.0935 ll -0.7702\n",
      "Epoch 26 Iter 9000 loss=0.4515 elbo -0.5886 sf 0.6298 selected 0.4659 kl 0.1016 ll -1.2138\n",
      "Epoch 26 Iter 9100 loss=0.4250 elbo -0.5751 sf 0.5693 selected 0.4811 kl 0.1081 ll -1.1394\n",
      "Epoch 26 Iter 9200 loss=0.3983 elbo -0.3281 sf 0.2814 selected 0.4989 kl 0.1097 ll -0.6044\n",
      "\n",
      "# epoch 26 iter 9207: dev loss 0.5889 elbo -0.5889 sf 0.2174 selected 0.6970 kl 0.1149 ll -0.6914 acc 0.4278\n",
      " dev0 [gold=3,pred=1]: it 's a **lovely** film with **lovely** **performances** by buy and **accorsi** .\n",
      " dev1 [gold=2,pred=1]: **no** one **goes** unindicted here , which is **probably** for the **best** .\n",
      " dev2 [gold=3,pred=1]: and if you **'re** **not** **nearly** moved to **tears** by a **couple** of **scenes** , you **'ve** got ice **water** in **your** **veins** .\n",
      "\n",
      "Shuffling training data\n",
      "Epoch 27 Iter 9300 loss=0.4120 elbo -0.2254 sf 0.3356 selected 0.4610 kl 0.0860 ll -0.5569\n",
      "Epoch 27 Iter 9400 loss=0.4097 elbo -0.5945 sf 0.6321 selected 0.4503 kl 0.1294 ll -1.2205\n",
      "Epoch 27 Iter 9500 loss=0.4254 elbo -0.3183 sf 0.2981 selected 0.4485 kl 0.1194 ll -0.6107\n",
      "\n",
      "# epoch 27 iter 9548: dev loss 0.5900 elbo -0.5900 sf 0.2096 selected 0.6799 kl 0.1076 ll -0.6920 acc 0.4233\n",
      " dev0 [gold=3,pred=1]: it 's a **lovely** film with **lovely** **performances** by buy and **accorsi** .\n",
      " dev1 [gold=2,pred=1]: **no** one **goes** unindicted here , which is **probably** for the **best** .\n",
      " dev2 [gold=3,pred=1]: and if you **'re** not **nearly** moved to **tears** by a **couple** of **scenes** , you 've got ice water in your **veins** .\n",
      "\n",
      "Shuffling training data\n",
      "Epoch 28 Iter 9600 loss=0.4439 elbo -0.4218 sf 0.3757 selected 0.4818 kl 0.0783 ll -0.7938\n",
      "Epoch 28 Iter 9700 loss=0.4275 elbo -0.4733 sf 0.5239 selected 0.4570 kl 0.1022 ll -0.9923\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28 Iter 9800 loss=0.4459 elbo -0.7256 sf 0.6185 selected 0.4251 kl 0.1148 ll -1.3385\n",
      "\n",
      "# epoch 28 iter 9889: dev loss 0.5825 elbo -0.5825 sf 0.2173 selected 0.6846 kl 0.1065 ll -0.6933 acc 0.4269\n",
      " dev0 [gold=3,pred=1]: it 's a **lovely** film with **lovely** **performances** by buy and **accorsi** .\n",
      " dev1 [gold=2,pred=1]: **no** one **goes** unindicted here , which is **probably** for the **best** .\n",
      " dev2 [gold=3,pred=1]: and if you **'re** not **nearly** moved to **tears** by a **couple** of **scenes** , you 've got ice **water** in **your** **veins** .\n",
      "\n",
      "Epoch    28: reducing learning rate of group 0 to 2.5000e-04.\n",
      "Epoch 29 Iter 9900 loss=0.4324 elbo -0.5332 sf 0.4277 selected 0.4765 kl 0.1150 ll -0.9552\n",
      "Shuffling training data\n",
      "Epoch 29 Iter 10000 loss=0.4116 elbo -0.3722 sf 0.3832 selected 0.4913 kl 0.0923 ll -0.7507\n",
      "Epoch 29 Iter 10100 loss=0.4555 elbo -0.4053 sf 0.4131 selected 0.4681 kl 0.1153 ll -0.8126\n",
      "Epoch 29 Iter 10200 loss=0.4310 elbo -0.2440 sf 0.2877 selected 0.4955 kl 0.0956 ll -0.5268\n",
      "\n",
      "# epoch 29 iter 10230: dev loss 0.5804 elbo -0.5804 sf 0.2168 selected 0.6799 kl 0.1041 ll -0.6932 acc 0.4314\n",
      " dev0 [gold=3,pred=1]: it 's a **lovely** film with **lovely** **performances** by buy and **accorsi** .\n",
      " dev1 [gold=2,pred=1]: **no** one **goes** unindicted here , which is **probably** for the **best** .\n",
      " dev2 [gold=3,pred=1]: and if you **'re** not **nearly** moved to **tears** by a **couple** of **scenes** , you 've got ice **water** in **your** **veins** .\n",
      "\n",
      "Shuffling training data\n",
      "Epoch 30 Iter 10300 loss=0.4432 elbo -0.3463 sf 0.3416 selected 0.4495 kl 0.1000 ll -0.6827\n",
      "Epoch 30 Iter 10400 loss=0.4383 elbo -0.2276 sf 0.3175 selected 0.4673 kl 0.0894 ll -0.5404\n",
      "Epoch 30 Iter 10500 loss=0.4075 elbo -0.3233 sf 0.4399 selected 0.4930 kl 0.0804 ll -0.7589\n",
      "\n",
      "# epoch 30 iter 10571: dev loss 0.5765 elbo -0.5765 sf 0.2201 selected 0.6799 kl 0.1030 ll -0.6936 acc 0.4251\n",
      " dev0 [gold=3,pred=1]: it 's a **lovely** film with **lovely** **performances** by buy and **accorsi** .\n",
      " dev1 [gold=2,pred=1]: **no** one **goes** unindicted here , which is **probably** for the **best** .\n",
      " dev2 [gold=3,pred=1]: and if you **'re** not **nearly** moved to **tears** by a **couple** of **scenes** , you 've got ice **water** in **your** **veins** .\n",
      "\n",
      "Epoch 31 Iter 10600 loss=0.4210 elbo -0.3886 sf 0.4223 selected 0.4920 kl 0.1040 ll -0.8054\n",
      "Shuffling training data\n",
      "Epoch 31 Iter 10700 loss=0.4186 elbo -0.5488 sf 0.6955 selected 0.4656 kl 0.1179 ll -1.2380\n",
      "Epoch 31 Iter 10800 loss=0.4088 elbo -0.5097 sf 0.5018 selected 0.4903 kl 0.1170 ll -1.0051\n",
      "Epoch 31 Iter 10900 loss=0.4394 elbo -0.3113 sf 0.3397 selected 0.4584 kl 0.1119 ll -0.6448\n",
      "\n",
      "# epoch 31 iter 10912: dev loss 0.5748 elbo -0.5748 sf 0.2213 selected 0.6792 kl 0.1027 ll -0.6934 acc 0.4296\n",
      " dev0 [gold=3,pred=1]: it 's a **lovely** film with **lovely** **performances** by buy and **accorsi** .\n",
      " dev1 [gold=2,pred=1]: **no** one **goes** unindicted here , which is **probably** for the **best** .\n",
      " dev2 [gold=3,pred=1]: and if you **'re** not **nearly** moved to **tears** by a **couple** of **scenes** , you 've got ice **water** in **your** **veins** .\n",
      "\n",
      "Shuffling training data\n",
      "Epoch 32 Iter 11000 loss=0.4142 elbo -0.4160 sf 0.4196 selected 0.4878 kl 0.0904 ll -0.8306\n",
      "Epoch 32 Iter 11100 loss=0.3965 elbo -0.3215 sf 0.3706 selected 0.4791 kl 0.0898 ll -0.6871\n",
      "Epoch 32 Iter 11200 loss=0.4085 elbo -0.3367 sf 0.4946 selected 0.5059 kl 0.1154 ll -0.8249\n",
      "\n",
      "# epoch 32 iter 11253: dev loss 0.5767 elbo -0.5767 sf 0.2178 selected 0.6722 kl 0.1011 ll -0.6934 acc 0.4305\n",
      " dev0 [gold=3,pred=1]: it 's a **lovely** film with **lovely** **performances** by buy and **accorsi** .\n",
      " dev1 [gold=2,pred=1]: **no** one **goes** unindicted here , which is probably for the **best** .\n",
      " dev2 [gold=3,pred=1]: and if you **'re** not **nearly** moved to **tears** by a **couple** of **scenes** , you 've got ice **water** in **your** **veins** .\n",
      "\n",
      "Shuffling training data\n",
      "Epoch 33 Iter 11300 loss=0.4482 elbo -0.5214 sf 0.5334 selected 0.4375 kl 0.0944 ll -1.0495\n",
      "Epoch 33 Iter 11400 loss=0.4210 elbo -0.3676 sf 0.3967 selected 0.4746 kl 0.0717 ll -0.7602\n",
      "Epoch 33 Iter 11500 loss=0.4452 elbo -0.5431 sf 0.4914 selected 0.4705 kl 0.0970 ll -1.0290\n",
      "\n",
      "# epoch 33 iter 11594: dev loss 0.5712 elbo -0.5712 sf 0.2244 selected 0.6792 kl 0.1022 ll -0.6934 acc 0.4296\n",
      " dev0 [gold=3,pred=1]: it 's a **lovely** film with **lovely** **performances** by buy and **accorsi** .\n",
      " dev1 [gold=2,pred=1]: **no** one **goes** unindicted here , which is **probably** for the **best** .\n",
      " dev2 [gold=3,pred=1]: and if you **'re** not **nearly** moved to **tears** by a **couple** of **scenes** , you 've got ice **water** in **your** **veins** .\n",
      "\n",
      "Epoch 34 Iter 11600 loss=0.4147 elbo -0.6352 sf 0.6828 selected 0.4400 kl 0.1168 ll -1.3112\n",
      "Shuffling training data\n",
      "Epoch 34 Iter 11700 loss=0.4226 elbo -0.4088 sf 0.6402 selected 0.4778 kl 0.0903 ll -1.0437\n",
      "Epoch 34 Iter 11800 loss=0.4278 elbo -0.2631 sf 0.3395 selected 0.4700 kl 0.1009 ll -0.5967\n",
      "Epoch 34 Iter 11900 loss=0.3880 elbo -0.3488 sf 0.5784 selected 0.4747 kl 0.0999 ll -0.9212\n",
      "\n",
      "# epoch 34 iter 11935: dev loss 0.5657 elbo -0.5657 sf 0.2298 selected 0.6831 kl 0.1023 ll -0.6932 acc 0.4260\n",
      " dev0 [gold=3,pred=1]: it 's a **lovely** film with **lovely** **performances** by buy and **accorsi** .\n",
      " dev1 [gold=2,pred=1]: **no** one **goes** unindicted here , which is **probably** for the **best** .\n",
      " dev2 [gold=3,pred=1]: and if you **'re** not **nearly** moved to **tears** by a **couple** of **scenes** , you 've got ice **water** in **your** **veins** .\n",
      "\n",
      "Shuffling training data\n",
      "Epoch 35 Iter 12000 loss=0.4048 elbo -0.4140 sf 0.3930 selected 0.5051 kl 0.0891 ll -0.8017\n",
      "Epoch 35 Iter 12100 loss=0.4299 elbo -0.5713 sf 0.7713 selected 0.4847 kl 0.1064 ll -1.3362\n",
      "Epoch 35 Iter 12200 loss=0.4125 elbo -0.2778 sf 0.2819 selected 0.4548 kl 0.0826 ll -0.5547\n",
      "\n",
      "# epoch 35 iter 12276: dev loss 0.5621 elbo -0.5621 sf 0.2319 selected 0.6823 kl 0.1005 ll -0.6934 acc 0.4233\n",
      " dev0 [gold=3,pred=1]: it 's a **lovely** film with **lovely** **performances** by buy and **accorsi** .\n",
      " dev1 [gold=2,pred=1]: **no** one **goes** unindicted here , which is **probably** for the **best** .\n",
      " dev2 [gold=3,pred=1]: and if you **'re** not **nearly** moved to **tears** by a **couple** of **scenes** , you 've got **ice** **water** in **your** **veins** .\n",
      "\n",
      "Epoch 36 Iter 12300 loss=0.3999 elbo -0.4965 sf 0.5308 selected 0.4973 kl 0.1059 ll -1.0208\n",
      "Shuffling training data\n",
      "Epoch 36 Iter 12400 loss=0.4128 elbo -0.5987 sf 0.6563 selected 0.4834 kl 0.1138 ll -1.2479\n",
      "Epoch 36 Iter 12500 loss=0.4093 elbo -0.4255 sf 0.4830 selected 0.4941 kl 0.0580 ll -0.9048\n",
      "Epoch 36 Iter 12600 loss=0.4211 elbo -0.4851 sf 0.4905 selected 0.4732 kl 0.0723 ll -0.9711\n",
      "\n",
      "# epoch 36 iter 12617: dev loss 0.5603 elbo -0.5603 sf 0.2290 selected 0.6702 kl 0.0957 ll -0.6936 acc 0.4305\n",
      " dev0 [gold=3,pred=1]: it 's a **lovely** film with **lovely** **performances** by buy and **accorsi** .\n",
      " dev1 [gold=2,pred=1]: **no** one **goes** unindicted here , which is probably for the **best** .\n",
      " dev2 [gold=3,pred=1]: and if you **'re** not **nearly** moved to **tears** by a **couple** of **scenes** , you 've got ice **water** in **your** **veins** .\n",
      "\n",
      "Shuffling training data\n",
      "Epoch 37 Iter 12700 loss=0.4099 elbo -0.5481 sf 0.5947 selected 0.4869 kl 0.0988 ll -1.1365\n",
      "Epoch 37 Iter 12800 loss=0.4106 elbo -0.5695 sf 0.6097 selected 0.4905 kl 0.1025 ll -1.1727\n",
      "Epoch 37 Iter 12900 loss=0.4173 elbo -0.6210 sf 0.6663 selected 0.4297 kl 0.1000 ll -1.2809\n",
      "\n",
      "# epoch 37 iter 12958: dev loss 0.5584 elbo -0.5584 sf 0.2296 selected 0.6675 kl 0.0933 ll -0.6948 acc 0.4278\n",
      " dev0 [gold=3,pred=1]: it 's a **lovely** film with **lovely** **performances** by buy and **accorsi** .\n",
      " dev1 [gold=2,pred=1]: **no** one **goes** unindicted here , which is probably for the **best** .\n",
      " dev2 [gold=3,pred=1]: and if you 're not **nearly** moved to **tears** by a **couple** of **scenes** , you 've got ice **water** in **your** **veins** .\n",
      "\n",
      "Shuffling training data\n",
      "Epoch 38 Iter 13000 loss=0.3994 elbo -0.2821 sf 0.3672 selected 0.4438 kl 0.0910 ll -0.6434\n",
      "Epoch 38 Iter 13100 loss=0.4017 elbo -0.3646 sf 0.4082 selected 0.4738 kl 0.0817 ll -0.7675\n",
      "Epoch 38 Iter 13200 loss=0.4245 elbo -0.3352 sf 0.6342 selected 0.4675 kl 0.1007 ll -0.9628\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "# epoch 38 iter 13299: dev loss 0.5546 elbo -0.5546 sf 0.2317 selected 0.6681 kl 0.0921 ll -0.6942 acc 0.4296\n",
      " dev0 [gold=3,pred=1]: it 's a **lovely** film with **lovely** **performances** by buy and **accorsi** .\n",
      " dev1 [gold=2,pred=1]: **no** one **goes** unindicted here , which is probably for the **best** .\n",
      " dev2 [gold=3,pred=1]: and if you 're not **nearly** moved to **tears** by a **couple** of **scenes** , you 've got ice **water** in **your** **veins** .\n",
      "\n",
      "Epoch 39 Iter 13300 loss=0.4225 elbo -0.5533 sf 0.5827 selected 0.4564 kl 0.0807 ll -1.1307\n",
      "Shuffling training data\n",
      "Epoch 39 Iter 13400 loss=0.4010 elbo -0.2746 sf 0.3110 selected 0.4757 kl 0.0774 ll -0.5804\n",
      "Epoch 39 Iter 13500 loss=0.4174 elbo -0.3800 sf 0.4812 selected 0.4693 kl 0.0885 ll -0.8552\n",
      "Epoch 39 Iter 13600 loss=0.4167 elbo -0.4386 sf 0.6557 selected 0.4344 kl 0.1040 ll -1.0873\n",
      "\n",
      "# epoch 39 iter 13640: dev loss 0.5539 elbo -0.5539 sf 0.2310 selected 0.6612 kl 0.0881 ll -0.6969 acc 0.4260\n",
      " dev0 [gold=3,pred=1]: it 's a **lovely** film with **lovely** **performances** by buy and **accorsi** .\n",
      " dev1 [gold=2,pred=1]: **no** one **goes** unindicted here , which is probably for the **best** .\n",
      " dev2 [gold=3,pred=1]: and if you 're not **nearly** moved to **tears** by a **couple** of **scenes** , you 've got ice **water** in **your** **veins** .\n",
      "\n",
      "Epoch    39: reducing learning rate of group 0 to 1.2500e-04.\n",
      "Shuffling training data\n",
      "Epoch 40 Iter 13700 loss=0.4144 elbo -0.3441 sf 0.3967 selected 0.4955 kl 0.0611 ll -0.7366\n",
      "Epoch 40 Iter 13800 loss=0.4010 elbo -0.3062 sf 0.4472 selected 0.4822 kl 0.0900 ll -0.7472\n",
      "Epoch 40 Iter 13900 loss=0.3977 elbo -0.2553 sf 0.2170 selected 0.4856 kl 0.1007 ll -0.4654\n",
      "\n",
      "# epoch 40 iter 13981: dev loss 0.5528 elbo -0.5528 sf 0.2331 selected 0.6615 kl 0.0881 ll -0.6979 acc 0.4269\n",
      " dev0 [gold=3,pred=1]: it 's a **lovely** film with **lovely** **performances** by buy and **accorsi** .\n",
      " dev1 [gold=2,pred=1]: **no** one **goes** unindicted here , which is probably for the **best** .\n",
      " dev2 [gold=3,pred=1]: and if you 're not **nearly** moved to **tears** by a **couple** of **scenes** , you 've got ice **water** in **your** **veins** .\n",
      "\n",
      "Epoch 41 Iter 14000 loss=0.4061 elbo -0.4281 sf 0.4319 selected 0.4752 kl 0.0612 ll -0.8557\n",
      "Shuffling training data\n",
      "Epoch 41 Iter 14100 loss=0.4108 elbo -0.2809 sf 0.3158 selected 0.4611 kl 0.0863 ll -0.5907\n",
      "Epoch 41 Iter 14200 loss=0.4232 elbo -0.3610 sf 0.5207 selected 0.4686 kl 0.0944 ll -0.8750\n",
      "Epoch 41 Iter 14300 loss=0.4020 elbo -0.3959 sf 0.4820 selected 0.4829 kl 0.0733 ll -0.8726\n",
      "\n",
      "# epoch 41 iter 14322: dev loss 0.5514 elbo -0.5514 sf 0.2334 selected 0.6609 kl 0.0875 ll -0.6973 acc 0.4242\n",
      " dev0 [gold=3,pred=1]: it 's a **lovely** film with **lovely** **performances** by buy and **accorsi** .\n",
      " dev1 [gold=2,pred=1]: **no** one **goes** unindicted here , which is probably for the **best** .\n",
      " dev2 [gold=3,pred=1]: and if you 're not **nearly** moved to **tears** by a **couple** of **scenes** , you 've got ice **water** in **your** **veins** .\n",
      "\n",
      "Shuffling training data\n",
      "Epoch 42 Iter 14400 loss=0.4180 elbo -0.4190 sf 0.5423 selected 0.4475 kl 0.1060 ll -0.9537\n",
      "Epoch 42 Iter 14500 loss=0.4267 elbo -0.4019 sf 0.5120 selected 0.4579 kl 0.0851 ll -0.9077\n",
      "Epoch 42 Iter 14600 loss=0.4002 elbo -0.6435 sf 0.6498 selected 0.4800 kl 0.1001 ll -1.2860\n",
      "\n",
      "# epoch 42 iter 14663: dev loss 0.5490 elbo -0.5490 sf 0.2341 selected 0.6589 kl 0.0864 ll -0.6966 acc 0.4260\n",
      " dev0 [gold=3,pred=1]: it 's a **lovely** film with **lovely** **performances** by buy and **accorsi** .\n",
      " dev1 [gold=2,pred=1]: **no** one **goes** unindicted here , which is probably for the **best** .\n",
      " dev2 [gold=3,pred=1]: and if you 're not **nearly** moved to **tears** by a **couple** of **scenes** , you 've got ice **water** in **your** **veins** .\n",
      "\n",
      "Epoch 43 Iter 14700 loss=0.4071 elbo -0.3434 sf 0.4146 selected 0.4781 kl 0.0720 ll -0.7527\n",
      "Shuffling training data\n",
      "Epoch 43 Iter 14800 loss=0.3968 elbo -0.4255 sf 0.5398 selected 0.4633 kl 0.0763 ll -0.9596\n",
      "Epoch 43 Iter 14900 loss=0.4025 elbo -0.4459 sf 0.4809 selected 0.4832 kl 0.0865 ll -0.9204\n",
      "Epoch 43 Iter 15000 loss=0.4099 elbo -0.4340 sf 0.5033 selected 0.4613 kl 0.0776 ll -0.9314\n",
      "\n",
      "# epoch 43 iter 15004: dev loss 0.5453 elbo -0.5453 sf 0.2395 selected 0.6622 kl 0.0871 ll -0.6978 acc 0.4269\n",
      " dev0 [gold=3,pred=1]: it 's a **lovely** film with **lovely** **performances** by buy and **accorsi** .\n",
      " dev1 [gold=2,pred=1]: **no** one **goes** unindicted here , which is probably for the **best** .\n",
      " dev2 [gold=3,pred=1]: and if you 're not **nearly** moved to **tears** by a **couple** of **scenes** , you 've got ice **water** in **your** **veins** .\n",
      "\n",
      "Shuffling training data\n",
      "Epoch 44 Iter 15100 loss=0.4076 elbo -0.4417 sf 0.5367 selected 0.4930 kl 0.0875 ll -0.9718\n",
      "Epoch 44 Iter 15200 loss=0.3922 elbo -0.4537 sf 0.4676 selected 0.4612 kl 0.1040 ll -0.9134\n",
      "Epoch 44 Iter 15300 loss=0.4133 elbo -0.5177 sf 0.5687 selected 0.4758 kl 0.0801 ll -1.0803\n",
      "\n",
      "# epoch 44 iter 15345: dev loss 0.5429 elbo -0.5429 sf 0.2404 selected 0.6628 kl 0.0866 ll -0.6967 acc 0.4223\n",
      " dev0 [gold=3,pred=1]: it 's a **lovely** film with **lovely** **performances** by buy and **accorsi** .\n",
      " dev1 [gold=2,pred=1]: **no** one **goes** unindicted here , which is probably for the **best** .\n",
      " dev2 [gold=3,pred=1]: and if you 're not **nearly** moved to **tears** by a **couple** of **scenes** , you 've got ice **water** in **your** **veins** .\n",
      "\n",
      "Shuffling training data\n",
      "Epoch 45 Iter 15400 loss=0.3894 elbo -0.3975 sf 0.5067 selected 0.4168 kl 0.0779 ll -0.8982\n",
      "Epoch 45 Iter 15500 loss=0.3974 elbo -0.3240 sf 0.3406 selected 0.4705 kl 0.0647 ll -0.6596\n",
      "Epoch 45 Iter 15600 loss=0.4203 elbo -0.5567 sf 0.6660 selected 0.5111 kl 0.0832 ll -1.2162\n",
      "\n",
      "# epoch 45 iter 15686: dev loss 0.5407 elbo -0.5407 sf 0.2403 selected 0.6609 kl 0.0847 ll -0.6963 acc 0.4242\n",
      " dev0 [gold=3,pred=1]: it 's a **lovely** film with **lovely** **performances** by buy and **accorsi** .\n",
      " dev1 [gold=2,pred=1]: **no** one **goes** unindicted here , which is probably for the **best** .\n",
      " dev2 [gold=3,pred=1]: and if you 're not **nearly** moved to **tears** by a **couple** of **scenes** , you 've got ice **water** in **your** **veins** .\n",
      "\n",
      "Epoch 46 Iter 15700 loss=0.3973 elbo -0.4641 sf 0.5414 selected 0.4667 kl 0.0785 ll -0.9993\n",
      "Shuffling training data\n",
      "Epoch 46 Iter 15800 loss=0.4143 elbo -0.3813 sf 0.4072 selected 0.4859 kl 0.0852 ll -0.7817\n",
      "Epoch 46 Iter 15900 loss=0.3845 elbo -0.6819 sf 0.7200 selected 0.4361 kl 0.0864 ll -1.3950\n",
      "Epoch 46 Iter 16000 loss=0.3823 elbo -0.2546 sf 0.3048 selected 0.4298 kl 0.0590 ll -0.5547\n",
      "\n",
      "# epoch 46 iter 16027: dev loss 0.5385 elbo -0.5385 sf 0.2426 selected 0.6626 kl 0.0843 ll -0.6968 acc 0.4205\n",
      " dev0 [gold=3,pred=1]: it 's a **lovely** film with **lovely** **performances** by buy and **accorsi** .\n",
      " dev1 [gold=2,pred=1]: **no** one **goes** unindicted here , which is probably for the **best** .\n",
      " dev2 [gold=3,pred=1]: and if you 're not **nearly** moved to **tears** by a **couple** of **scenes** , you 've got **ice** **water** in **your** **veins** .\n",
      "\n",
      "Shuffling training data\n",
      "Epoch 47 Iter 16100 loss=0.3901 elbo -0.4296 sf 0.4698 selected 0.4673 kl 0.0739 ll -0.8934\n",
      "Epoch 47 Iter 16200 loss=0.3879 elbo -0.3428 sf 0.3683 selected 0.4587 kl 0.1027 ll -0.7028\n",
      "Epoch 47 Iter 16300 loss=0.3986 elbo -0.2707 sf 0.3593 selected 0.4744 kl 0.0833 ll -0.6232\n",
      "\n",
      "# epoch 47 iter 16368: dev loss 0.5345 elbo -0.5345 sf 0.2439 selected 0.6648 kl 0.0836 ll -0.6948 acc 0.4214\n",
      " dev0 [gold=3,pred=1]: it 's a **lovely** film with **lovely** **performances** by buy and **accorsi** .\n",
      " dev1 [gold=2,pred=1]: **no** one **goes** unindicted here , which is probably for the **best** .\n",
      " dev2 [gold=3,pred=1]: and if you 're not **nearly** moved to **tears** by a **couple** of **scenes** , you 've got **ice** **water** in **your** **veins** .\n",
      "\n",
      "Epoch 48 Iter 16400 loss=0.3869 elbo -0.2576 sf 0.3587 selected 0.4571 kl 0.0747 ll -0.6102\n",
      "Shuffling training data\n",
      "Epoch 48 Iter 16500 loss=0.3879 elbo -0.4727 sf 0.6460 selected 0.4674 kl 0.0945 ll -1.1110\n",
      "Epoch 48 Iter 16600 loss=0.3845 elbo -0.4154 sf 0.5130 selected 0.4882 kl 0.0871 ll -0.9212\n",
      "Epoch 48 Iter 16700 loss=0.3877 elbo -0.4254 sf 0.5454 selected 0.4646 kl 0.0737 ll -0.9647\n",
      "\n",
      "# epoch 48 iter 16709: dev loss 0.5324 elbo -0.5324 sf 0.2458 selected 0.6652 kl 0.0836 ll -0.6946 acc 0.4196\n",
      " dev0 [gold=3,pred=1]: it 's a **lovely** film with **lovely** **performances** by buy and **accorsi** .\n",
      " dev1 [gold=2,pred=1]: **no** one **goes** unindicted here , which is probably for the **best** .\n",
      " dev2 [gold=3,pred=1]: and if you 're not **nearly** moved to **tears** by a **couple** of **scenes** , you 've got **ice** **water** in **your** **veins** .\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shuffling training data\n",
      "Epoch 49 Iter 16800 loss=0.3988 elbo -0.5334 sf 0.7297 selected 0.4780 kl 0.0806 ll -1.2563\n",
      "Epoch 49 Iter 16900 loss=0.3968 elbo -0.4279 sf 0.5136 selected 0.4718 kl 0.0707 ll -0.9355\n",
      "Epoch 49 Iter 17000 loss=0.3958 elbo -0.3026 sf 0.4780 selected 0.4812 kl 0.0844 ll -0.7735\n",
      "\n",
      "# epoch 49 iter 17050: dev loss 0.5316 elbo -0.5316 sf 0.2469 selected 0.6655 kl 0.0834 ll -0.6950 acc 0.4214\n",
      " dev0 [gold=3,pred=1]: it 's a **lovely** film with **lovely** **performances** by buy and **accorsi** .\n",
      " dev1 [gold=2,pred=1]: **no** one **goes** unindicted here , which is probably for the **best** .\n",
      " dev2 [gold=3,pred=1]: and if you 're not **nearly** moved to **tears** by a **couple** of **scenes** , you 've got **ice** **water** in **your** **veins** .\n",
      "\n",
      "Epoch 50 Iter 17100 loss=0.3890 elbo -0.4505 sf 0.4192 selected 0.4750 kl 0.0751 ll -0.8633\n",
      "Shuffling training data\n",
      "Epoch 50 Iter 17200 loss=0.3940 elbo -0.3686 sf 0.4472 selected 0.4638 kl 0.0820 ll -0.8087\n",
      "Epoch 50 Iter 17300 loss=0.3896 elbo -0.5655 sf 0.7650 selected 0.4880 kl 0.0839 ll -1.3232\n",
      "\n",
      "# epoch 50 iter 17391: dev loss 0.5315 elbo -0.5315 sf 0.2455 selected 0.6632 kl 0.0817 ll -0.6954 acc 0.4223\n",
      " dev0 [gold=3,pred=1]: it 's a **lovely** film with **lovely** **performances** by buy and **accorsi** .\n",
      " dev1 [gold=2,pred=1]: **no** one **goes** unindicted here , which is probably for the **best** .\n",
      " dev2 [gold=3,pred=1]: and if you 're not **nearly** moved to **tears** by a **couple** of **scenes** , you 've got **ice** **water** in **your** **veins** .\n",
      "\n",
      "Epoch    50: reducing learning rate of group 0 to 6.2500e-05.\n"
     ]
    }
   ],
   "source": [
    "model = train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7w_Ko657vRGo"
   },
   "source": [
    "# Variance reduction\n",
    "\n",
    "**This is an extra**\n",
    "\n",
    "We can use a *control variate* to reduce the variance of our gradient estimates.\n",
    "\n",
    "Let's recap the idea in general terms. We are looking to solve some expectation\n",
    "\\begin{align}\n",
    "\\mu_f = \\mathbb E[f(Z)]\n",
    "\\end{align}\n",
    "but unfortunatelly, realising the full sum (or integral for continuous variables) is intractable. Thus we employ MC estimation\n",
    "\\begin{align}\n",
    "\\hat \\mu_f &\\overset{\\text{MC}}{\\approx} \\frac{1}{S} \\sum_{s=1}^S f(z_s) & \\text{where }z_s \\sim Q(z|x)\n",
    "\\end{align}\n",
    "Note that the variance of this estimate is\n",
    "\\begin{align}\n",
    "\\text{Var}(\\hat \\mu_f) &=  \\frac{1}{S}\\text{Var}(f(Z)) \\\\\n",
    "&= \\frac{1}{S} \\mathbb E[( f(Z) - \\mathbb E[f(Z)])^2]\n",
    "\\end{align}\n",
    "Note that this variance is such that it goes down as we sample more, in a rate $\\mathcal O(S^{-1})$.\n",
    "See that if we sample $10$ times more, we will only obtain an decrease in variance in the order of $10^{-1}$. This means that sampling more is generally not the most convenient way to decrease variance.\n",
    "\n",
    "*Digression* we can estimate the variance itself via MC, an unbiased estimate looks like\n",
    "\\begin{align}\n",
    "\\hat \\sigma^2_f = \\frac{1}{S(S-1)} \\sum_{s=1}^S (f(z_s) - \\hat \\mu_f)^2\n",
    "\\end{align}\n",
    "but not that this estimate is even hard to improve since it decreases with $\\mathcal O(S^{-2})$.\n",
    "\n",
    "Back to out main problem: let's try and improve the variance of our estimator to $\\mu_f$.\n",
    "\n",
    "It's a fact, and it can be shown trivially, that\n",
    "\\begin{align}\n",
    "\\mu_f &=  \\mathbb E[f(Z) - \\psi(Z)] + \\underbrace{\\mathbb E[\\psi(Z)]}_{\\mu_\\psi} \\\\\n",
    " &\\overset{\\text{MC}}{\\approx} \\underbrace{\\left(\\frac{1}{S} \\sum_{s=1}^S f(z_s) - \\psi(z_s) \\right) + \\mu_\\psi}_{\\hat c}\n",
    "\\end{align}\n",
    "where we assume the existence of some function $\\psi(z)$ for which the expected value $\\mu_\\psi$ is known and we estimate the expected difference $\\mathbb E[f(Z) - \\psi(Z)]$ via MC. We used this axuxiliary function, also known as a *control variate*, to derive a new estimator, which we will denote by $\\hat c$.\n",
    "\n",
    "The variance of this new estimator is show below:\n",
    "\n",
    "\\begin{align}\n",
    "\\text{Var}( \\hat c ) &= \\text{Var}(\\hat \\mu_{f-\\psi}) + 2\\underbrace{\\text{Cov}(\\hat \\mu_{f-\\psi}, \\mu_\\psi)}_{\\mathbb E[\\hat \\mu_{f-\\psi}  \\mu_\\psi] - \\mathbb E[\\hat \\mu_{f-\\psi}] \\mathbb E[\\mu_\\psi]} + \\underbrace{\\text{Var}(\\mu_\\psi)}_{\\color{blue}{0} } \\\\\n",
    "&= \\frac{1}{S}\\text{Var}(f- \\psi)  + 2 \\underbrace{\\left( \\mu_\\psi \\mu_{f-\\psi} - \\mu_{f-\\psi} \\mu_\\psi \\right)}_{\\color{blue}{0}} \n",
    "\\end{align}\n",
    "where the variance of $\\mu_\\psi$ is 0 because we know it in closed form (no need for MC estimation), and the covariance is $0$ as shown in the second row.\n",
    "\n",
    "That is, the variance of $\\hat c$ is essentially the variance of estimating $\\mathbb E[f(Z) - \\psi(Z)]$, which in turn depends on the variance \n",
    "\n",
    "\\begin{align}\n",
    "\\text{Var}(f-\\psi) &= \\text{Var}(f) - 2\\text{Cov}(f, \\psi) + \\text{Var}(\\psi)\n",
    "\\end{align}\n",
    "where we can see that if $\\text{Cov}(f, \\psi) > \\frac{\\text{Var}(\\psi)}{2}$ we achieve variance reduction as then $\\text{Var}(f-\\psi)$ would be smaller than $\\text{Var(f)}$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ovKcRnqH_PGp"
   },
   "source": [
    "\n",
    "## Baselines\n",
    "\n",
    "Baslines are control variates of a very simple form:\n",
    "\\begin{align}\n",
    "\\mathbb E[f(Z)] = \\mathbb E[f(Z) - C] + \\mathbb E[C]\n",
    "\\end{align}\n",
    "where $C$ is a constant with respect to $z$.\n",
    "\n",
    "In the context of the score function estimator, a baseline looks like a quantity $C(x; \\omega)$, this may be\n",
    "* just a constant;\n",
    "* or a function of the input (but not of the latent variable), which could be itself implemented as a neural network;\n",
    "* a combination of the two.\n",
    " \n",
    "\n",
    "Let's focus on the first term of the ELBO (so I'm omitting the KL term here). The gradient with respect to parameters of the inference model becomes:\n",
    "\n",
    "\\begin{align}\n",
    "&\\mathbb E_{Q(z|x, \\lambda)}\\left[ \\log P(x|z, \\theta) \\nabla_\\lambda \\log Q(z|x, \\lambda)\\right]\\\\\n",
    "&=\\mathbb E_{Q(z|x, \\lambda)}\\left[\\log P(x|z, \\theta) \\nabla_\\lambda \\log Q(z|x, \\lambda) - \\color{red}{C(x; \\omega)\\nabla_\\lambda \\log Q(z|x, \\lambda) }  \\right] + \\underbrace{\\mathbb E_{Q(z|x, \\lambda)}\\left[\\color{red}{C(x; \\omega)\\nabla_\\lambda \\log Q(z|x, \\lambda) }  \\right] }_{=0} \\\\\n",
    "&= \\mathbb E_{Q(z|x, \\lambda)}\\left[ \\color{blue}{\\left(\\log P(x|z, \\theta) - C(x; \\omega) \\right)}\\nabla_\\lambda \\log Q(z|x, \\lambda)\\right] \\\\\n",
    "&\n",
    "\\end{align}\n",
    "We can show that the last term is $0$\n",
    "\n",
    "\\begin{align}\n",
    "&\\mathbb E_{Q(z|x, \\lambda)}\\left[C(x; \\omega)\\nabla_\\lambda \\log Q(z|x, \\lambda)   \\right]  \\\\&= C(x; \\omega) \\mathbb E_{Q(z|x, \\lambda)}\\left[\\nabla_\\lambda \\log Q(z|x, \\lambda)   \\right]\\\\\n",
    "&= C(x; \\omega) \\mathbb E_{Q(z|x, \\lambda)}\\left[\\frac{1}{Q(z|x, \\lambda)} \\nabla_\\lambda Q(z|x, \\lambda)   \\right] \\\\\n",
    "&= C(x; \\omega) \\sum_z Q(z|x, \\lambda) \\frac{1}{Q(z|x, \\lambda)} \\nabla_\\lambda Q(z|x, \\lambda)   \\\\\n",
    "&= C(x; \\omega) \\sum_z\\nabla_\\lambda Q(z|x, \\lambda)  \\\\\n",
    "&= C(x; \\omega) \\nabla_\\lambda \\underbrace{\\sum_z Q(z|x, \\lambda)  }_{=1}\\\\\n",
    "&=0\n",
    "\\end{align}\n",
    "\n",
    "Examples of useful baselines:\n",
    "\n",
    "* a running average of the learning signal: at some iteration $t$ we can use a running average of $\\log P(x|z, \\theta)$ using parameter estimates $\\theta$ from iterations $i < t$, this is a baseline that likely leads to high correlation between control variate and learning signal and can lead to variance reduction;\n",
    "* another technique is to have an MLP with parameters $\\omega$ predict a scalar and train this MLP to approximate the learning signal $\\log P(x|z, \\theta)$ via regression:\n",
    "\\begin{align}\n",
    "\\arg\\max_\\omega \\left( C(x; \\omega) - \\log P(x|z, \\theta) \\right)^2\n",
    "\\end{align}\n",
    "its left as an extra to implement these ideas.\n",
    "\n",
    "One more note: we can also use something called a *multiplicative baseline* in the literature of reinforcement learning, whereby we incorporate a running estimate of the standard deviation of the learning signal computed based on the values attained on previous iterations:\n",
    "\\begin{align}\n",
    "\\mathbb E_{Q(z|x, \\lambda)}\\left[ \\frac{1}{\\hat\\sigma_{\\text{past}}}\\left(\\log P(x|z, \\theta) - \\hat \\mu_{\\text{past}}\\right)\\nabla_\\lambda \\log Q(z|x, \\lambda)\\right]\n",
    "\\end{align}\n",
    "this form of contorl variate aim at promoting the learning signal (or reward in reinforcement learning literature) to be distributed by $\\mathcal N(0, 1)$. Note that multiplying the reward by a constant does not bias the estimator, and in this case, may lead to variance reduction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_counters(model, data, batch_size, device, cfg, iter_i):\n",
    "    _, rationales = evaluate(\n",
    "        model,\n",
    "        data, \n",
    "        batch_size=batch_size, \n",
    "        device=device,\n",
    "        cfg=cfg,\n",
    "        iter_i=0\n",
    "    )\n",
    "    important_counter = Counter()\n",
    "    all_counter = Counter()\n",
    "    for example in rationales:\n",
    "        tokens = [\n",
    "            token if not token.startswith(\"**\") else token[2:-2]\n",
    "            for token in example[0]\n",
    "        ]\n",
    "        tags = nltk.pos_tag(tokens)\n",
    "        for tag, token in zip(tags, example[0]):\n",
    "            all_counter[tag[1]] += 1\n",
    "            if token.startswith(\"**\"):\n",
    "                important_counter[tag[1]] += 1\n",
    "    return important_counter, all_counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "important_counter, all_counter = make_counters(model, dev_data, len(dev_data), device, cfg, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Сколько разные части речи считаются важными:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('NN', 2367),\n",
       " ('JJ', 1865),\n",
       " ('RB', 851),\n",
       " ('NNS', 733),\n",
       " ('VBZ', 268),\n",
       " ('VBG', 255),\n",
       " ('VB', 210),\n",
       " ('VBN', 177),\n",
       " ('CC', 152),\n",
       " ('IN', 102),\n",
       " ('DT', 78),\n",
       " ('VBP', 63),\n",
       " ('VBD', 63),\n",
       " ('JJS', 55),\n",
       " ('NNP', 37),\n",
       " ('MD', 29),\n",
       " ('JJR', 24),\n",
       " (\"''\", 24),\n",
       " ('RBS', 21),\n",
       " (':', 18),\n",
       " ('PRP$', 16),\n",
       " ('RP', 16),\n",
       " ('RBR', 14),\n",
       " ('.', 10),\n",
       " ('FW', 7),\n",
       " ('CD', 6),\n",
       " ('UH', 4),\n",
       " ('PRP', 4),\n",
       " ('PDT', 1),\n",
       " ('EX', 1),\n",
       " ('POS', 1),\n",
       " ('WRB', 1)]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Сколько разные части речи считаются важными:\")\n",
    "important_counter.most_common()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Частота важности частей речи:\n",
      "RBS 1.0\n",
      "UH 1.0\n",
      "NNP 0.9487179487179487\n",
      "JJ 0.8183413777972796\n",
      "JJS 0.7971014492753623\n",
      "NNS 0.7293532338308458\n",
      "RB 0.6727272727272727\n",
      "VBG 0.6100478468899522\n",
      "NN 0.5848776871756857\n",
      "VBN 0.5379939209726444\n",
      "'' 0.47058823529411764\n",
      "FW 0.4666666666666667\n",
      "VBD 0.39622641509433965\n",
      "JJR 0.38095238095238093\n",
      "VB 0.35353535353535354\n",
      "VBZ 0.26044703595724006\n",
      "RBR 0.2413793103448276\n",
      "RP 0.22857142857142856\n",
      "VBP 0.22661870503597123\n",
      "CC 0.1946222791293214\n",
      "MD 0.14871794871794872\n",
      ": 0.09326424870466321\n",
      "PDT 0.07692307692307693\n",
      "PRP$ 0.05734767025089606\n",
      "IN 0.04737575476079889\n",
      "CD 0.045454545454545456\n",
      "DT 0.03687943262411347\n",
      "EX 0.02127659574468085\n",
      "WRB 0.014285714285714285\n",
      ". 0.009337068160597572\n",
      "PRP 0.006329113924050633\n",
      "POS 0.005747126436781609\n",
      ", 0.0\n",
      "WDT 0.0\n",
      "TO 0.0\n",
      "WP 0.0\n",
      "`` 0.0\n",
      "WP$ 0.0\n",
      "$ 0.0\n"
     ]
    }
   ],
   "source": [
    "fractions = []\n",
    "for token in all_counter:\n",
    "    fractions.append((token, important_counter[token] / all_counter[token]))\n",
    "print(\"Частота важности частей речи:\")\n",
    "for (token, frac) in sorted(fractions, key=lambda pair: -pair[1]):\n",
    "    print(token, frac)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "С большой частотой важными становятся слова, обозначающие действие или свойство. Это хорошо. Есть и не очень понятные примеры. Например междометия кажется не очень полезны для понимания смысла, но они всегда включаются в важные слова. Однако в целом междометий очень мало, поэтому это не критично."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "# Constructing model\n",
      "Classifier #params: 1505\n",
      "ProductOfBernoullis #params: 542701\n",
      "\n",
      "# Loading embeddings\n",
      "fixed word embeddings\n",
      "Model(\n",
      "  (embed): Embedding(20727, 300, padding_idx=1)\n",
      "  (cls_net): Classifier(\n",
      "    (embed_layer): Sequential(\n",
      "      (0): Embedding(20727, 300, padding_idx=1)\n",
      "    )\n",
      "    (enc_layer): BagOfWordsEncoder()\n",
      "    (output_layer): Sequential(\n",
      "      (0): Dropout(p=0.5)\n",
      "      (1): Linear(in_features=300, out_features=5, bias=True)\n",
      "      (2): LogSoftmax()\n",
      "    )\n",
      "  )\n",
      "  (inference_net): ProductOfBernoullis(\n",
      "    (embed_layer): Sequential(\n",
      "      (0): Embedding(20727, 300, padding_idx=1)\n",
      "    )\n",
      "    (enc_layer): LSTMEncoder(\n",
      "      (lstm): LSTM(300, 150, batch_first=True, bidirectional=True)\n",
      "    )\n",
      "    (logit_layer): Linear(in_features=300, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "embed.weight             [20727, 300] requires_grad=False\n",
      "cls_net.output_layer.1.weight [5, 300]     requires_grad=True\n",
      "cls_net.output_layer.1.bias [5]          requires_grad=True\n",
      "inference_net.enc_layer.lstm.weight_ih_l0 [600, 300]   requires_grad=True\n",
      "inference_net.enc_layer.lstm.weight_hh_l0 [600, 150]   requires_grad=True\n",
      "inference_net.enc_layer.lstm.bias_ih_l0 [600]        requires_grad=True\n",
      "inference_net.enc_layer.lstm.bias_hh_l0 [600]        requires_grad=True\n",
      "inference_net.enc_layer.lstm.weight_ih_l0_reverse [600, 300]   requires_grad=True\n",
      "inference_net.enc_layer.lstm.weight_hh_l0_reverse [600, 150]   requires_grad=True\n",
      "inference_net.enc_layer.lstm.bias_ih_l0_reverse [600]        requires_grad=True\n",
      "inference_net.enc_layer.lstm.bias_hh_l0_reverse [600]        requires_grad=True\n",
      "inference_net.logit_layer.weight [1, 300]     requires_grad=True\n",
      "inference_net.logit_layer.bias [1]          requires_grad=True\n",
      "\n",
      "Total parameters: 6762306\n",
      "\n",
      "Shuffling training data\n",
      "Epoch 0 Iter 100 loss=0.3367 elbo -0.3026 sf 0.6406 selected 0.4463 kl 0.0144 ll -0.9432\n",
      "Epoch 0 Iter 200 loss=0.3744 elbo -0.3697 sf 0.7776 selected 0.4649 kl 0.0175 ll -1.1472\n",
      "Epoch 0 Iter 300 loss=0.3596 elbo -0.3882 sf 0.8471 selected 0.5453 kl 0.0569 ll -1.2352\n",
      "\n",
      "# epoch 0 iter 341: dev loss 0.3591 elbo -0.3591 sf 0.5060 selected 0.9727 kl 0.0611 ll -0.8040 acc 0.2970\n",
      " dev0 [gold=3,pred=3]: **it** **'s** **a** **lovely** film with lovely performances **by** **buy** **and** **accorsi** **.**\n",
      " dev1 [gold=2,pred=3]: **no** **one** **goes** **unindicted** **here** **,** **which** **is** **probably** **for** **the** **best** **.**\n",
      " dev2 [gold=3,pred=3]: **and** **if** **you** **'re** **not** **nearly** **moved** **to** **tears** **by** **a** **couple** **of** **scenes** **,** **you** **'ve** **got** **ice** **water** **in** **your** **veins** **.**\n",
      "\n",
      "Shuffling training data\n",
      "Epoch 1 Iter 400 loss=0.3555 elbo -0.3481 sf 0.8088 selected 0.4974 kl 0.0712 ll -1.1566\n",
      "Epoch 1 Iter 500 loss=0.3510 elbo -0.3038 sf 0.6769 selected 0.5530 kl 0.0816 ll -0.9803\n",
      "Epoch 1 Iter 600 loss=0.3535 elbo -0.3970 sf 0.8673 selected 0.5097 kl 0.0706 ll -1.2639\n",
      "\n",
      "# epoch 1 iter 682: dev loss 0.3286 elbo -0.3286 sf 0.5237 selected 0.5958 kl 0.0399 ll -0.8124 acc 0.3088\n",
      " dev0 [gold=3,pred=1]: it **'s** a lovely film with lovely performances by **buy** and accorsi .\n",
      " dev1 [gold=2,pred=1]: **no** one goes **unindicted** **here** **,** which is probably for the **best** .\n",
      " dev2 [gold=3,pred=1]: and **if** **you** **'re** not nearly moved to tears by a couple **of** **scenes** **,** **you** **'ve** got ice water **in** **your** **veins** .\n",
      "\n",
      "Shuffling training data\n",
      "Epoch 2 Iter 700 loss=0.3471 elbo -0.3261 sf 0.7117 selected 0.4918 kl 0.0373 ll -1.0376\n",
      "Epoch 2 Iter 800 loss=0.3324 elbo -0.3579 sf 0.7757 selected 0.5107 kl 0.0829 ll -1.1329\n",
      "Epoch 2 Iter 900 loss=0.3428 elbo -0.3768 sf 0.7665 selected 0.5793 kl 0.1046 ll -1.1424\n",
      "Epoch 2 Iter 1000 loss=0.3560 elbo -0.2482 sf 0.5287 selected 0.5420 kl 0.0708 ll -0.7761\n",
      "\n",
      "# epoch 2 iter 1023: dev loss 0.4240 elbo -0.4240 sf 0.4499 selected 0.9733 kl 0.0812 ll -0.7926 acc 0.3361\n",
      " dev0 [gold=3,pred=3]: **it** **'s** a **lovely** film **with** **lovely** **performances** **by** **buy** **and** **accorsi** **.**\n",
      " dev1 [gold=2,pred=3]: **no** **one** **goes** **unindicted** **here** **,** **which** **is** **probably** **for** **the** **best** **.**\n",
      " dev2 [gold=3,pred=3]: **and** **if** **you** **'re** **not** **nearly** **moved** **to** **tears** **by** a **couple** **of** **scenes** **,** **you** **'ve** **got** **ice** **water** **in** **your** **veins** **.**\n",
      "\n",
      "Shuffling training data\n",
      "Epoch 3 Iter 1100 loss=0.3428 elbo -0.3923 sf 0.8126 selected 0.5323 kl 0.0725 ll -1.2041\n",
      "Epoch 3 Iter 1200 loss=0.3485 elbo -0.3114 sf 0.6350 selected 0.5467 kl 0.0871 ll -0.9453\n",
      "Epoch 3 Iter 1300 loss=0.3413 elbo -0.4154 sf 0.9350 selected 0.5169 kl 0.0578 ll -1.3497\n",
      "\n",
      "# epoch 3 iter 1364: dev loss 0.3431 elbo -0.3431 sf 0.4956 selected 0.7795 kl 0.0530 ll -0.7857 acc 0.3533\n",
      " dev0 [gold=3,pred=1]: **it** **'s** a **lovely** film **with** **lovely** **performances** by **buy** **and** **accorsi** .\n",
      " dev1 [gold=2,pred=3]: **no** one **goes** **unindicted** **here** **,** which is probably for the **best** .\n",
      " dev2 [gold=3,pred=1]: and **if** **you** **'re** not nearly moved to tears by a couple of **scenes** **,** **you** **'ve** **got** ice water in your veins .\n",
      "\n",
      "Shuffling training data\n",
      "Epoch 4 Iter 1400 loss=0.3349 elbo -0.2615 sf 0.6008 selected 0.5253 kl 0.0403 ll -0.8618\n",
      "Epoch 4 Iter 1500 loss=0.3431 elbo -0.2837 sf 0.6234 selected 0.5167 kl 0.0688 ll -0.9061\n",
      "Epoch 4 Iter 1600 loss=0.3495 elbo -0.2913 sf 0.6141 selected 0.5529 kl 0.0760 ll -0.9042\n",
      "Epoch 4 Iter 1700 loss=0.3391 elbo -0.3747 sf 0.8085 selected 0.5337 kl 0.0863 ll -1.1818\n",
      "\n",
      "# epoch 4 iter 1705: dev loss 0.4810 elbo -0.4810 sf 0.4042 selected 0.9976 kl 0.1015 ll -0.7837 acc 0.3424\n",
      " dev0 [gold=3,pred=1]: **it** **'s** **a** **lovely** **film** **with** **lovely** **performances** **by** **buy** **and** **accorsi** **.**\n",
      " dev1 [gold=2,pred=3]: **no** **one** **goes** **unindicted** **here** **,** **which** **is** **probably** **for** **the** **best** **.**\n",
      " dev2 [gold=3,pred=3]: **and** **if** **you** **'re** **not** **nearly** **moved** **to** **tears** **by** **a** **couple** **of** **scenes** **,** **you** **'ve** **got** **ice** **water** **in** **your** **veins** **.**\n",
      "\n",
      "Shuffling training data\n",
      "Epoch 5 Iter 1800 loss=0.3490 elbo -0.2945 sf 0.6776 selected 0.5411 kl 0.0702 ll -0.9709\n",
      "Epoch 5 Iter 1900 loss=0.3304 elbo -0.3793 sf 0.8609 selected 0.5497 kl 0.0714 ll -1.2388\n",
      "Epoch 5 Iter 2000 loss=0.3411 elbo -0.2172 sf 0.4486 selected 0.5427 kl 0.0463 ll -0.6649\n",
      "\n",
      "# epoch 5 iter 2046: dev loss 0.3843 elbo -0.3843 sf 0.4608 selected 0.8827 kl 0.0689 ll -0.7761 acc 0.3488\n",
      " dev0 [gold=3,pred=1]: **it** **'s** **a** **lovely** **film** **with** **lovely** **performances** **by** **buy** **and** **accorsi** .\n",
      " dev1 [gold=2,pred=3]: **no** **one** **goes** **unindicted** **here** , which is **probably** **for** **the** **best** .\n",
      " dev2 [gold=3,pred=1]: and **if** **you** **'re** **not** nearly moved to **tears** by **a** **couple** **of** **scenes** **,** **you** **'ve** **got** ice water **in** **your** veins .\n",
      "\n",
      "Shuffling training data\n",
      "Epoch 6 Iter 2100 loss=0.3405 elbo -0.4247 sf 0.9364 selected 0.5124 kl 0.0687 ll -1.3596\n",
      "Epoch 6 Iter 2200 loss=0.3393 elbo -0.3304 sf 0.6895 selected 0.4629 kl 0.0218 ll -1.0194\n",
      "Epoch 6 Iter 2300 loss=0.3327 elbo -0.3576 sf 0.7477 selected 0.4750 kl 0.0423 ll -1.1043\n",
      "\n",
      "# epoch 6 iter 2387: dev loss 0.3880 elbo -0.3880 sf 0.4498 selected 0.7752 kl 0.0680 ll -0.7698 acc 0.3597\n",
      " dev0 [gold=3,pred=1]: **it** **'s** **a** **lovely** **film** **with** **lovely** **performances** by **buy** **and** accorsi .\n",
      " dev1 [gold=2,pred=3]: **no** **one** **goes** **unindicted** **here** , which is **probably** **for** **the** **best** .\n",
      " dev2 [gold=3,pred=1]: **and** **if** **you** **'re** **not** nearly moved to tears by a **couple** **of** **scenes** , **you** **'ve** got ice water **in** **your** veins .\n",
      "\n",
      "Shuffling training data\n",
      "Epoch 7 Iter 2400 loss=0.3407 elbo -0.3784 sf 0.8069 selected 0.4973 kl 0.0797 ll -1.1834\n",
      "Epoch 7 Iter 2500 loss=0.3393 elbo -0.3967 sf 0.8161 selected 0.5430 kl 0.0745 ll -1.2109\n",
      "Epoch 7 Iter 2600 loss=0.3460 elbo -0.3696 sf 0.7432 selected 0.4900 kl 0.0333 ll -1.1119\n",
      "Epoch 7 Iter 2700 loss=0.3430 elbo -0.3311 sf 0.7244 selected 0.4985 kl 0.0438 ll -1.0544\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "# epoch 7 iter 2728: dev loss 0.3781 elbo -0.3781 sf 0.4489 selected 0.5902 kl 0.0507 ll -0.7763 acc 0.3733\n",
      " dev0 [gold=3,pred=1]: **it** **'s** **a** **lovely** **film** **with** **lovely** **performances** by **buy** and accorsi .\n",
      " dev1 [gold=2,pred=1]: no one **goes** unindicted **here** , which is probably for the **best** .\n",
      " dev2 [gold=3,pred=1]: and if you 're not nearly moved to tears by a couple of **scenes** , you 've got ice water in **your** veins .\n",
      "\n",
      "Epoch     7: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Shuffling training data\n",
      "Epoch 8 Iter 2800 loss=0.3288 elbo -0.2943 sf 0.6443 selected 0.4966 kl 0.0446 ll -0.9373\n",
      "Epoch 8 Iter 2900 loss=0.3321 elbo -0.2850 sf 0.5731 selected 0.4939 kl 0.0555 ll -0.8565\n",
      "Epoch 8 Iter 3000 loss=0.3291 elbo -0.2645 sf 0.5801 selected 0.5083 kl 0.0707 ll -0.8425\n",
      "\n",
      "# epoch 8 iter 3069: dev loss 0.4103 elbo -0.4103 sf 0.4269 selected 0.7054 kl 0.0708 ll -0.7664 acc 0.3742\n",
      " dev0 [gold=3,pred=1]: **it** **'s** **a** **lovely** **film** **with** **lovely** **performances** by **buy** **and** accorsi .\n",
      " dev1 [gold=2,pred=3]: **no** **one** **goes** unindicted **here** , which is **probably** for the **best** .\n",
      " dev2 [gold=3,pred=1]: and **if** **you** **'re** not nearly moved to tears by a **couple** of **scenes** , **you** **'ve** got ice water in **your** veins .\n",
      "\n",
      "Shuffling training data\n",
      "Epoch 9 Iter 3100 loss=0.3444 elbo -0.4489 sf 0.9761 selected 0.4938 kl 0.0760 ll -1.4226\n",
      "Epoch 9 Iter 3200 loss=0.3380 elbo -0.4369 sf 0.9082 selected 0.5356 kl 0.0820 ll -1.3425\n",
      "Epoch 9 Iter 3300 loss=0.3383 elbo -0.3113 sf 0.5999 selected 0.4857 kl 0.0621 ll -0.9091\n",
      "Epoch 9 Iter 3400 loss=0.3501 elbo -0.2653 sf 0.4925 selected 0.5227 kl 0.0543 ll -0.7560\n",
      "\n",
      "# epoch 9 iter 3410: dev loss 0.4145 elbo -0.4145 sf 0.4119 selected 0.2182 kl 0.0556 ll -0.7708 acc 0.3697\n",
      " dev0 [gold=3,pred=1]: **it** **'s** **a** **lovely** **film** **with** **lovely** **performances** by **buy** and accorsi .\n",
      " dev1 [gold=2,pred=1]: no one **goes** unindicted **here** , which is probably for the **best** .\n",
      " dev2 [gold=3,pred=1]: and **if** you **'re** not nearly moved to tears by a couple of **scenes** , you 've got ice water in your veins .\n",
      "\n",
      "Shuffling training data\n",
      "Epoch 10 Iter 3500 loss=0.3516 elbo -0.3652 sf 0.7331 selected 0.4471 kl 0.0287 ll -1.0973\n",
      "Epoch 10 Iter 3600 loss=0.3737 elbo -0.3535 sf 0.6665 selected 0.4615 kl 0.0386 ll -1.0186\n",
      "Epoch 10 Iter 3700 loss=0.3936 elbo -0.4161 sf 0.6678 selected 0.4295 kl 0.0325 ll -1.0827\n",
      "\n",
      "# epoch 10 iter 3751: dev loss 0.4259 elbo -0.4259 sf 0.3953 selected 0.1622 kl 0.0453 ll -0.7759 acc 0.3688\n",
      " dev0 [gold=3,pred=1]: **it** **'s** a **lovely** **film** **with** **lovely** **performances** by **buy** and accorsi .\n",
      " dev1 [gold=2,pred=3]: no one goes unindicted here , which is probably for the best .\n",
      " dev2 [gold=3,pred=1]: and if you **'re** not nearly moved to tears by a couple of scenes , you 've got ice water in your veins .\n",
      "\n",
      "Shuffling training data\n",
      "Epoch 11 Iter 3800 loss=0.3582 elbo -0.2848 sf 0.6344 selected 0.5074 kl 0.0547 ll -0.9171\n",
      "Epoch 11 Iter 3900 loss=0.3598 elbo -0.2563 sf 0.5742 selected 0.4800 kl 0.0420 ll -0.8288\n",
      "Epoch 11 Iter 4000 loss=0.3443 elbo -0.3247 sf 0.5734 selected 0.4427 kl 0.0364 ll -0.8967\n",
      "\n",
      "# epoch 11 iter 4092: dev loss 0.4703 elbo -0.4703 sf 0.3458 selected 0.0980 kl 0.0308 ll -0.7853 acc 0.3615\n",
      " dev0 [gold=3,pred=1]: it **'s** a **lovely** film **with** **lovely** **performances** by buy and accorsi .\n",
      " dev1 [gold=2,pred=1]: no one goes unindicted here , which is probably for the best .\n",
      " dev2 [gold=3,pred=1]: and if you 're not nearly moved to tears by a couple of scenes , you 've got ice water in your veins .\n",
      "\n",
      "Epoch 12 Iter 4100 loss=0.3920 elbo -0.3182 sf 0.5841 selected 0.4208 kl 0.0231 ll -0.9014\n",
      "Shuffling training data\n",
      "Epoch 12 Iter 4200 loss=0.4450 elbo -0.3372 sf 0.5509 selected 0.4110 kl 0.0232 ll -0.8871\n",
      "Epoch 12 Iter 4300 loss=0.3740 elbo -0.3163 sf 0.6678 selected 0.4791 kl 0.0300 ll -0.9828\n",
      "Epoch 12 Iter 4400 loss=0.3645 elbo -0.4125 sf 0.7788 selected 0.4421 kl 0.0292 ll -1.1900\n",
      "\n",
      "# epoch 12 iter 4433: dev loss 0.4337 elbo -0.4337 sf 0.3831 selected 0.1387 kl 0.0419 ll -0.7748 acc 0.3824\n",
      " dev0 [gold=3,pred=1]: it **'s** a **lovely** **film** **with** **lovely** **performances** by **buy** and accorsi .\n",
      " dev1 [gold=2,pred=1]: no one goes unindicted here , which is probably for the best .\n",
      " dev2 [gold=3,pred=1]: and if you 're not nearly moved to tears by a couple of scenes , you 've got ice water in your veins .\n",
      "\n",
      "Shuffling training data\n",
      "Epoch 13 Iter 4500 loss=0.3552 elbo -0.4644 sf 0.9133 selected 0.4958 kl 0.0411 ll -1.3758\n",
      "Epoch 13 Iter 4600 loss=0.3528 elbo -0.3096 sf 0.6360 selected 0.4753 kl 0.0735 ll -0.9422\n",
      "Epoch 13 Iter 4700 loss=0.3503 elbo -0.4631 sf 0.8962 selected 0.4853 kl 0.0586 ll -1.3565\n",
      "\n",
      "# epoch 13 iter 4774: dev loss 0.4365 elbo -0.4365 sf 0.3910 selected 0.3270 kl 0.0624 ll -0.7651 acc 0.3824\n",
      " dev0 [gold=3,pred=1]: **it** **'s** a **lovely** **film** **with** **lovely** **performances** **by** **buy** **and** **accorsi** .\n",
      " dev1 [gold=2,pred=1]: no one goes unindicted here , which is probably for the best .\n",
      " dev2 [gold=3,pred=1]: and if you **'re** not nearly moved to tears by a couple of **scenes** , you 've got ice water in your veins .\n",
      "\n",
      "Shuffling training data\n",
      "Epoch 14 Iter 4800 loss=0.3637 elbo -0.2860 sf 0.5652 selected 0.4754 kl 0.0586 ll -0.8484\n",
      "Epoch 14 Iter 4900 loss=0.3578 elbo -0.3377 sf 0.5775 selected 0.5019 kl 0.0781 ll -0.9114\n",
      "Epoch 14 Iter 5000 loss=0.3697 elbo -0.3405 sf 0.6302 selected 0.4770 kl 0.0726 ll -0.9671\n",
      "Epoch 14 Iter 5100 loss=0.3662 elbo -0.3467 sf 0.6125 selected 0.4671 kl 0.0754 ll -0.9553\n",
      "\n",
      "# epoch 14 iter 5115: dev loss 0.4599 elbo -0.4599 sf 0.3555 selected 0.1537 kl 0.0469 ll -0.7685 acc 0.3688\n",
      " dev0 [gold=3,pred=1]: **it** **'s** a **lovely** **film** **with** **lovely** **performances** **by** **buy** **and** accorsi .\n",
      " dev1 [gold=2,pred=1]: no one goes unindicted here , which is probably for the best .\n",
      " dev2 [gold=3,pred=1]: and if you **'re** not nearly moved to tears by a couple of scenes , you 've got ice water in your veins .\n",
      "\n",
      "Shuffling training data\n",
      "Epoch 15 Iter 5200 loss=0.3706 elbo -0.2651 sf 0.4827 selected 0.4709 kl 0.0417 ll -0.7457\n",
      "Epoch 15 Iter 5300 loss=0.3750 elbo -0.4842 sf 0.8135 selected 0.4400 kl 0.0461 ll -1.2953\n",
      "Epoch 15 Iter 5400 loss=0.3724 elbo -0.3757 sf 0.6477 selected 0.4244 kl 0.0371 ll -1.0214\n",
      "\n",
      "# epoch 15 iter 5456: dev loss 0.4377 elbo -0.4377 sf 0.3799 selected 0.2616 kl 0.0567 ll -0.7609 acc 0.3787\n",
      " dev0 [gold=3,pred=1]: **it** 's a **lovely** **film** **with** **lovely** **performances** by **buy** **and** **accorsi** .\n",
      " dev1 [gold=2,pred=1]: no one goes unindicted here , which is **probably** for the best .\n",
      " dev2 [gold=3,pred=1]: and if you **'re** not nearly moved to tears by a couple of scenes , you 've got ice water in your veins .\n",
      "\n",
      "Shuffling training data\n",
      "Epoch 16 Iter 5500 loss=0.3578 elbo -0.3288 sf 0.5623 selected 0.4751 kl 0.0472 ll -0.8886\n",
      "Epoch 16 Iter 5600 loss=0.3544 elbo -0.3578 sf 0.6526 selected 0.4498 kl 0.0361 ll -1.0084\n",
      "Epoch 16 Iter 5700 loss=0.3631 elbo -0.3672 sf 0.7654 selected 0.4761 kl 0.0340 ll -1.1307\n",
      "\n",
      "# epoch 16 iter 5797: dev loss 0.4734 elbo -0.4734 sf 0.3330 selected 0.0977 kl 0.0322 ll -0.7742 acc 0.3678\n",
      " dev0 [gold=3,pred=1]: it 's a **lovely** **film** **with** **lovely** **performances** by **buy** and accorsi .\n",
      " dev1 [gold=2,pred=1]: no one goes unindicted here , which is probably for the best .\n",
      " dev2 [gold=3,pred=1]: and if you 're not nearly moved to tears by a couple of scenes , you 've got ice water in your veins .\n",
      "\n",
      "Epoch 17 Iter 5800 loss=0.3722 elbo -0.3507 sf 0.4955 selected 0.4400 kl 0.0335 ll -0.8442\n",
      "Shuffling training data\n",
      "Epoch 17 Iter 5900 loss=0.3794 elbo -0.4433 sf 0.5728 selected 0.4138 kl 0.0406 ll -1.0138\n",
      "Epoch 17 Iter 6000 loss=0.3945 elbo -0.3689 sf 0.6476 selected 0.4677 kl 0.0587 ll -1.0130\n",
      "Epoch 17 Iter 6100 loss=0.3716 elbo -0.4626 sf 0.8230 selected 0.4660 kl 0.0482 ll -1.2827\n",
      "\n",
      "# epoch 17 iter 6138: dev loss 0.4426 elbo -0.4426 sf 0.3636 selected 0.3208 kl 0.0434 ll -0.7628 acc 0.3860\n",
      " dev0 [gold=3,pred=1]: it 's a **lovely** **film** **with** **lovely** **performances** by **buy** and **accorsi** .\n",
      " dev1 [gold=2,pred=1]: no one goes unindicted here , which is probably for the best .\n",
      " dev2 [gold=3,pred=1]: and if you **'re** not nearly moved to tears by a couple of scenes , you 've got ice water in your veins .\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shuffling training data\n",
      "Epoch 18 Iter 6200 loss=0.3682 elbo -0.3511 sf 0.6708 selected 0.4853 kl 0.0461 ll -1.0191\n",
      "Epoch 18 Iter 6300 loss=0.3459 elbo -0.5128 sf 1.0263 selected 0.4971 kl 0.0487 ll -1.5360\n",
      "Epoch 18 Iter 6400 loss=0.3616 elbo -0.3347 sf 0.6204 selected 0.4756 kl 0.0428 ll -0.9523\n",
      "\n",
      "# epoch 18 iter 6479: dev loss 0.4682 elbo -0.4682 sf 0.3362 selected 0.2331 kl 0.0374 ll -0.7670 acc 0.3688\n",
      " dev0 [gold=3,pred=1]: it 's a **lovely** **film** **with** **lovely** **performances** by **buy** **and** **accorsi** .\n",
      " dev1 [gold=2,pred=1]: no one goes unindicted here , which is probably for the **best** .\n",
      " dev2 [gold=3,pred=1]: and if you 're not nearly moved to tears by a couple of scenes , you 've got ice water in your veins .\n",
      "\n",
      "Epoch    18: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Shuffling training data\n",
      "Epoch 19 Iter 6500 loss=0.3710 elbo -0.3390 sf 0.6158 selected 0.4549 kl 0.0241 ll -0.9532\n",
      "Epoch 19 Iter 6600 loss=0.3968 elbo -0.4743 sf 0.7799 selected 0.4116 kl 0.0306 ll -1.2522\n",
      "Epoch 19 Iter 6700 loss=0.4039 elbo -0.4792 sf 0.6824 selected 0.4032 kl 0.0261 ll -1.1598\n",
      "Epoch 19 Iter 6800 loss=0.4105 elbo -0.3466 sf 0.5273 selected 0.4536 kl 0.0272 ll -0.8720\n",
      "\n",
      "# epoch 19 iter 6820: dev loss 0.4664 elbo -0.4664 sf 0.3380 selected 0.2312 kl 0.0385 ll -0.7659 acc 0.3724\n",
      " dev0 [gold=3,pred=1]: it 's a **lovely** **film** **with** **lovely** **performances** by **buy** **and** **accorsi** .\n",
      " dev1 [gold=2,pred=1]: no one goes unindicted here , which is probably for the **best** .\n",
      " dev2 [gold=3,pred=1]: and if you 're not nearly moved to tears by a couple of scenes , you 've got ice water in your veins .\n",
      "\n",
      "Shuffling training data\n",
      "Epoch 20 Iter 6900 loss=0.3874 elbo -0.4239 sf 0.7177 selected 0.4263 kl 0.0398 ll -1.1388\n",
      "Epoch 20 Iter 7000 loss=0.3682 elbo -0.3367 sf 0.5785 selected 0.4659 kl 0.0402 ll -0.9123\n",
      "Epoch 20 Iter 7100 loss=0.3970 elbo -0.3403 sf 0.6084 selected 0.4343 kl 0.0339 ll -0.9462\n",
      "\n",
      "# epoch 20 iter 7161: dev loss 0.4939 elbo -0.4939 sf 0.3092 selected 0.0810 kl 0.0285 ll -0.7747 acc 0.3597\n",
      " dev0 [gold=3,pred=1]: it 's a **lovely** film **with** **lovely** **performances** by **buy** and **accorsi** .\n",
      " dev1 [gold=2,pred=1]: no one goes unindicted here , which is probably for the best .\n",
      " dev2 [gold=3,pred=1]: and if you 're not nearly moved to tears by a couple of scenes , you 've got ice water in your veins .\n",
      "\n",
      "Shuffling training data\n",
      "Epoch 21 Iter 7200 loss=0.4203 elbo -0.3618 sf 0.4797 selected 0.4000 kl 0.0453 ll -0.8382\n",
      "Epoch 21 Iter 7300 loss=0.3963 elbo -0.3335 sf 0.5618 selected 0.4200 kl 0.0292 ll -0.8932\n",
      "Epoch 21 Iter 7400 loss=0.3934 elbo -0.4079 sf 0.7001 selected 0.4505 kl 0.0385 ll -1.1051\n",
      "Epoch 21 Iter 7500 loss=0.3838 elbo -0.3297 sf 0.6247 selected 0.4933 kl 0.0617 ll -0.9498\n",
      "\n",
      "# epoch 21 iter 7502: dev loss 0.4547 elbo -0.4547 sf 0.3538 selected 0.3880 kl 0.0531 ll -0.7554 acc 0.3787\n",
      " dev0 [gold=3,pred=1]: it 's a **lovely** **film** **with** **lovely** **performances** **by** **buy** **and** **accorsi** .\n",
      " dev1 [gold=2,pred=1]: no one goes unindicted here , which is **probably** for the **best** .\n",
      " dev2 [gold=3,pred=1]: and if you **'re** not nearly moved to tears by a couple of scenes , you **'ve** got ice water in your veins .\n",
      "\n",
      "Shuffling training data\n",
      "Epoch 22 Iter 7600 loss=0.3619 elbo -0.3365 sf 0.5643 selected 0.4543 kl 0.0375 ll -0.8979\n",
      "Epoch 22 Iter 7700 loss=0.3730 elbo -0.5377 sf 0.8542 selected 0.4245 kl 0.0471 ll -1.3883\n",
      "Epoch 22 Iter 7800 loss=0.3724 elbo -0.3683 sf 0.7632 selected 0.4346 kl 0.0359 ll -1.1286\n",
      "\n",
      "# epoch 22 iter 7843: dev loss 0.4593 elbo -0.4593 sf 0.3432 selected 0.2839 kl 0.0440 ll -0.7584 acc 0.3806\n",
      " dev0 [gold=3,pred=1]: it 's a **lovely** **film** **with** **lovely** **performances** by **buy** **and** **accorsi** .\n",
      " dev1 [gold=2,pred=1]: no one goes unindicted here , which is probably for the best .\n",
      " dev2 [gold=3,pred=1]: and if you 're not nearly moved to tears by a couple of scenes , you 've got ice water in your veins .\n",
      "\n",
      "Shuffling training data\n",
      "Epoch 23 Iter 7900 loss=0.3697 elbo -0.3483 sf 0.6524 selected 0.4194 kl 0.0387 ll -0.9977\n",
      "Epoch 23 Iter 8000 loss=0.3947 elbo -0.3419 sf 0.5847 selected 0.4422 kl 0.0437 ll -0.9231\n",
      "Epoch 23 Iter 8100 loss=0.3757 elbo -0.3345 sf 0.5696 selected 0.4452 kl 0.0388 ll -0.9010\n",
      "\n",
      "# epoch 23 iter 8184: dev loss 0.4689 elbo -0.4689 sf 0.3318 selected 0.1972 kl 0.0375 ll -0.7631 acc 0.3778\n",
      " dev0 [gold=3,pred=1]: it 's a **lovely** **film** **with** **lovely** **performances** by **buy** **and** **accorsi** .\n",
      " dev1 [gold=2,pred=1]: no one goes unindicted here , which is probably for the best .\n",
      " dev2 [gold=3,pred=1]: and if you 're not nearly moved to tears by a couple of scenes , you 've got ice water in your veins .\n",
      "\n",
      "Epoch 24 Iter 8200 loss=0.3912 elbo -0.4012 sf 0.5883 selected 0.4245 kl 0.0353 ll -0.9866\n",
      "Shuffling training data\n",
      "Epoch 24 Iter 8300 loss=0.3859 elbo -0.5138 sf 0.8553 selected 0.4158 kl 0.0448 ll -1.3654\n",
      "Epoch 24 Iter 8400 loss=0.3665 elbo -0.3028 sf 0.5110 selected 0.4400 kl 0.0325 ll -0.8111\n",
      "Epoch 24 Iter 8500 loss=0.3902 elbo -0.4734 sf 0.8278 selected 0.4324 kl 0.0363 ll -1.2981\n",
      "\n",
      "# epoch 24 iter 8525: dev loss 0.4763 elbo -0.4763 sf 0.3251 selected 0.2481 kl 0.0396 ll -0.7618 acc 0.3797\n",
      " dev0 [gold=3,pred=1]: it 's a **lovely** **film** **with** **lovely** **performances** by **buy** **and** **accorsi** .\n",
      " dev1 [gold=2,pred=1]: no one goes unindicted here , which is probably for the best .\n",
      " dev2 [gold=3,pred=1]: and if you 're not nearly moved to tears by a couple of scenes , you 've got ice water in your veins .\n",
      "\n",
      "Shuffling training data\n",
      "Epoch 25 Iter 8600 loss=0.3880 elbo -0.4513 sf 0.7078 selected 0.3980 kl 0.0299 ll -1.1565\n",
      "Epoch 25 Iter 8700 loss=0.4162 elbo -0.3318 sf 0.4770 selected 0.3767 kl 0.0381 ll -0.8055\n",
      "Epoch 25 Iter 8800 loss=0.3996 elbo -0.4254 sf 0.6675 selected 0.3920 kl 0.0330 ll -1.0900\n",
      "\n",
      "# epoch 25 iter 8866: dev loss 0.4798 elbo -0.4798 sf 0.3207 selected 0.1825 kl 0.0368 ll -0.7637 acc 0.3778\n",
      " dev0 [gold=3,pred=1]: it 's a **lovely** **film** **with** **lovely** **performances** by **buy** **and** **accorsi** .\n",
      " dev1 [gold=2,pred=1]: no one goes unindicted here , which is probably for the best .\n",
      " dev2 [gold=3,pred=1]: and if you 're not nearly moved to tears by a couple of scenes , you 've got ice water in your veins .\n",
      "\n",
      "Shuffling training data\n",
      "Epoch 26 Iter 8900 loss=0.4010 elbo -0.4616 sf 0.8074 selected 0.4258 kl 0.0283 ll -1.2666\n",
      "Epoch 26 Iter 9000 loss=0.3769 elbo -0.4059 sf 0.7207 selected 0.4500 kl 0.0298 ll -1.1239\n",
      "Epoch 26 Iter 9100 loss=0.3871 elbo -0.2834 sf 0.5436 selected 0.4574 kl 0.0388 ll -0.8234\n",
      "Epoch 26 Iter 9200 loss=0.3676 elbo -0.3325 sf 0.5803 selected 0.4183 kl 0.0379 ll -0.9093\n",
      "\n",
      "# epoch 26 iter 9207: dev loss 0.4693 elbo -0.4693 sf 0.3280 selected 0.1607 kl 0.0358 ll -0.7614 acc 0.3860\n",
      " dev0 [gold=3,pred=1]: it 's a **lovely** **film** **with** **lovely** **performances** by **buy** and **accorsi** .\n",
      " dev1 [gold=2,pred=1]: no one goes unindicted here , which is probably for the best .\n",
      " dev2 [gold=3,pred=1]: and if you 're not nearly moved to tears by a couple of scenes , you 've got ice water in your veins .\n",
      "\n",
      "Shuffling training data\n",
      "Epoch 27 Iter 9300 loss=0.3880 elbo -0.3748 sf 0.6274 selected 0.4078 kl 0.0245 ll -0.9999\n",
      "Epoch 27 Iter 9400 loss=0.3854 elbo -0.5496 sf 0.9027 selected 0.4411 kl 0.0393 ll -1.4486\n",
      "Epoch 27 Iter 9500 loss=0.3837 elbo -0.2940 sf 0.4820 selected 0.4450 kl 0.0306 ll -0.7732\n",
      "\n",
      "# epoch 27 iter 9548: dev loss 0.4737 elbo -0.4737 sf 0.3238 selected 0.1057 kl 0.0341 ll -0.7634 acc 0.3842\n",
      " dev0 [gold=3,pred=1]: it 's a **lovely** **film** **with** **lovely** **performances** by **buy** and **accorsi** .\n",
      " dev1 [gold=2,pred=1]: no one goes unindicted here , which is probably for the best .\n",
      " dev2 [gold=3,pred=1]: and if you 're not nearly moved to tears by a couple of scenes , you 've got ice water in your veins .\n",
      "\n",
      "Shuffling training data\n",
      "Epoch 28 Iter 9600 loss=0.3911 elbo -0.5363 sf 0.8489 selected 0.3990 kl 0.0228 ll -1.3830\n",
      "Epoch 28 Iter 9700 loss=0.4013 elbo -0.5156 sf 0.7770 selected 0.3859 kl 0.0317 ll -1.2896\n",
      "Epoch 28 Iter 9800 loss=0.3940 elbo -0.4911 sf 0.7918 selected 0.4090 kl 0.0203 ll -1.2808\n",
      "\n",
      "# epoch 28 iter 9889: dev loss 0.4718 elbo -0.4718 sf 0.3237 selected 0.1091 kl 0.0325 ll -0.7630 acc 0.3878\n",
      " dev0 [gold=3,pred=1]: it 's a **lovely** **film** **with** **lovely** **performances** by **buy** and **accorsi** .\n",
      " dev1 [gold=2,pred=1]: no one goes unindicted here , which is probably for the best .\n",
      " dev2 [gold=3,pred=1]: and if you 're not nearly moved to tears by a couple of scenes , you 've got ice water in your veins .\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29 Iter 9900 loss=0.3873 elbo -0.3530 sf 0.5888 selected 0.4489 kl 0.0323 ll -0.9385\n",
      "Shuffling training data\n",
      "Epoch 29 Iter 10000 loss=0.3820 elbo -0.2979 sf 0.4976 selected 0.4392 kl 0.0279 ll -0.7927\n",
      "Epoch 29 Iter 10100 loss=0.3875 elbo -0.2773 sf 0.4898 selected 0.4086 kl 0.0331 ll -0.7637\n",
      "Epoch 29 Iter 10200 loss=0.3927 elbo -0.3281 sf 0.5402 selected 0.4105 kl 0.0336 ll -0.8649\n",
      "\n",
      "# epoch 29 iter 10230: dev loss 0.4610 elbo -0.4610 sf 0.3329 selected 0.1402 kl 0.0359 ll -0.7580 acc 0.3833\n",
      " dev0 [gold=3,pred=1]: it 's a **lovely** **film** **with** **lovely** **performances** by **buy** and **accorsi** .\n",
      " dev1 [gold=2,pred=1]: no one goes unindicted here , which is **probably** for the best .\n",
      " dev2 [gold=3,pred=1]: and if you 're not nearly moved to tears by a couple of scenes , you 've got ice water in your veins .\n",
      "\n",
      "Epoch    29: reducing learning rate of group 0 to 2.5000e-05.\n",
      "Shuffling training data\n",
      "Epoch 30 Iter 10300 loss=0.3881 elbo -0.3529 sf 0.6931 selected 0.4547 kl 0.0380 ll -1.0421\n",
      "Epoch 30 Iter 10400 loss=0.3777 elbo -0.4730 sf 0.7532 selected 0.4245 kl 0.0233 ll -1.2239\n",
      "Epoch 30 Iter 10500 loss=0.3677 elbo -0.3399 sf 0.5432 selected 0.4328 kl 0.0370 ll -0.8792\n",
      "\n",
      "# epoch 30 iter 10571: dev loss 0.4742 elbo -0.4742 sf 0.3201 selected 0.1066 kl 0.0311 ll -0.7633 acc 0.3869\n",
      " dev0 [gold=3,pred=1]: it 's a **lovely** **film** **with** **lovely** **performances** by **buy** and **accorsi** .\n",
      " dev1 [gold=2,pred=1]: no one goes unindicted here , which is probably for the best .\n",
      " dev2 [gold=3,pred=1]: and if you 're not nearly moved to tears by a couple of scenes , you 've got ice water in your veins .\n",
      "\n",
      "Epoch 31 Iter 10600 loss=0.3961 elbo -0.3104 sf 0.4241 selected 0.4505 kl 0.0273 ll -0.7317\n",
      "Shuffling training data\n",
      "Epoch 31 Iter 10700 loss=0.3896 elbo -0.4046 sf 0.7488 selected 0.4063 kl 0.0312 ll -1.1500\n",
      "Epoch 31 Iter 10800 loss=0.4066 elbo -0.4394 sf 0.7211 selected 0.4242 kl 0.0266 ll -1.1576\n",
      "Epoch 31 Iter 10900 loss=0.3871 elbo -0.3724 sf 0.5322 selected 0.4046 kl 0.0195 ll -0.9026\n",
      "\n",
      "# epoch 31 iter 10912: dev loss 0.4730 elbo -0.4730 sf 0.3211 selected 0.1219 kl 0.0307 ll -0.7634 acc 0.3896\n",
      " dev0 [gold=3,pred=1]: it 's a **lovely** **film** **with** **lovely** **performances** by **buy** and **accorsi** .\n",
      " dev1 [gold=2,pred=1]: no one goes unindicted here , which is probably for the best .\n",
      " dev2 [gold=3,pred=1]: and if you 're not nearly moved to tears by a couple of scenes , you 've got ice water in your veins .\n",
      "\n",
      "Shuffling training data\n",
      "Epoch 32 Iter 11000 loss=0.3729 elbo -0.2392 sf 0.3352 selected 0.4480 kl 0.0225 ll -0.5719\n",
      "Epoch 32 Iter 11100 loss=0.3819 elbo -0.4353 sf 0.6803 selected 0.4124 kl 0.0255 ll -1.1128\n",
      "Epoch 32 Iter 11200 loss=0.4079 elbo -0.3318 sf 0.5016 selected 0.4223 kl 0.0268 ll -0.8304\n",
      "\n",
      "# epoch 32 iter 11253: dev loss 0.4788 elbo -0.4788 sf 0.3156 selected 0.1293 kl 0.0294 ll -0.7650 acc 0.3842\n",
      " dev0 [gold=3,pred=1]: it 's a **lovely** **film** **with** **lovely** **performances** by **buy** and **accorsi** .\n",
      " dev1 [gold=2,pred=1]: no one goes unindicted here , which is probably for the best .\n",
      " dev2 [gold=3,pred=1]: and if you 're not nearly moved to tears by a couple of scenes , you 've got ice water in your veins .\n",
      "\n",
      "Shuffling training data\n",
      "Epoch 33 Iter 11300 loss=0.3996 elbo -0.4840 sf 0.6562 selected 0.4011 kl 0.0315 ll -1.1367\n",
      "Epoch 33 Iter 11400 loss=0.4077 elbo -0.3408 sf 0.5306 selected 0.4302 kl 0.0256 ll -0.8685\n",
      "Epoch 33 Iter 11500 loss=0.3923 elbo -0.4148 sf 0.6263 selected 0.3909 kl 0.0259 ll -1.0382\n",
      "\n",
      "# epoch 33 iter 11594: dev loss 0.4827 elbo -0.4827 sf 0.3105 selected 0.0854 kl 0.0269 ll -0.7663 acc 0.3751\n",
      " dev0 [gold=3,pred=1]: it 's a **lovely** **film** **with** **lovely** **performances** by **buy** and **accorsi** .\n",
      " dev1 [gold=2,pred=1]: no one goes unindicted here , which is probably for the best .\n",
      " dev2 [gold=3,pred=1]: and if you 're not nearly moved to tears by a couple of scenes , you 've got ice water in your veins .\n",
      "\n",
      "Epoch 34 Iter 11600 loss=0.3881 elbo -0.3654 sf 0.5227 selected 0.4311 kl 0.0214 ll -0.8856\n",
      "Shuffling training data\n",
      "Epoch 34 Iter 11700 loss=0.4151 elbo -0.4401 sf 0.7042 selected 0.3873 kl 0.0160 ll -1.1425\n",
      "Epoch 34 Iter 11800 loss=0.4108 elbo -0.4125 sf 0.6236 selected 0.3926 kl 0.0153 ll -1.0343\n",
      "Epoch 34 Iter 11900 loss=0.4081 elbo -0.4569 sf 0.6704 selected 0.4000 kl 0.0255 ll -1.1242\n",
      "\n",
      "# epoch 34 iter 11935: dev loss 0.4833 elbo -0.4833 sf 0.3094 selected 0.0871 kl 0.0273 ll -0.7654 acc 0.3760\n",
      " dev0 [gold=3,pred=1]: it 's a **lovely** **film** **with** **lovely** **performances** by **buy** and **accorsi** .\n",
      " dev1 [gold=2,pred=1]: no one goes unindicted here , which is probably for the best .\n",
      " dev2 [gold=3,pred=1]: and if you 're not nearly moved to tears by a couple of scenes , you 've got ice water in your veins .\n",
      "\n",
      "Shuffling training data\n",
      "Epoch 35 Iter 12000 loss=0.4246 elbo -0.4914 sf 0.7828 selected 0.4302 kl 0.0228 ll -1.2715\n",
      "Epoch 35 Iter 12100 loss=0.3909 elbo -0.3159 sf 0.4764 selected 0.4279 kl 0.0240 ll -0.7894\n",
      "Epoch 35 Iter 12200 loss=0.4171 elbo -0.3509 sf 0.5514 selected 0.3695 kl 0.0176 ll -0.9002\n",
      "\n",
      "# epoch 35 iter 12276: dev loss 0.4944 elbo -0.4944 sf 0.2994 selected 0.0709 kl 0.0250 ll -0.7689 acc 0.3688\n",
      " dev0 [gold=3,pred=1]: it 's a **lovely** **film** **with** **lovely** **performances** by **buy** and **accorsi** .\n",
      " dev1 [gold=2,pred=1]: no one goes unindicted here , which is probably for the best .\n",
      " dev2 [gold=3,pred=1]: and if you 're not nearly moved to tears by a couple of scenes , you 've got ice water in your veins .\n",
      "\n",
      "Epoch 36 Iter 12300 loss=0.4017 elbo -0.3410 sf 0.5050 selected 0.4105 kl 0.0251 ll -0.8429\n",
      "Shuffling training data\n",
      "Epoch 36 Iter 12400 loss=0.4028 elbo -0.4286 sf 0.7223 selected 0.4056 kl 0.0299 ll -1.1472\n",
      "Epoch 36 Iter 12500 loss=0.4115 elbo -0.3152 sf 0.4531 selected 0.4498 kl 0.0283 ll -0.7648\n",
      "Epoch 36 Iter 12600 loss=0.3961 elbo -0.3637 sf 0.4867 selected 0.3967 kl 0.0242 ll -0.8474\n",
      "\n",
      "# epoch 36 iter 12617: dev loss 0.4818 elbo -0.4818 sf 0.3097 selected 0.0809 kl 0.0272 ll -0.7643 acc 0.3769\n",
      " dev0 [gold=3,pred=1]: it 's a **lovely** **film** **with** **lovely** **performances** by **buy** and **accorsi** .\n",
      " dev1 [gold=2,pred=1]: no one goes unindicted here , which is probably for the best .\n",
      " dev2 [gold=3,pred=1]: and if you 're not nearly moved to tears by a couple of scenes , you 've got ice water in your veins .\n",
      "\n",
      "Shuffling training data\n",
      "Epoch 37 Iter 12700 loss=0.4005 elbo -0.4976 sf 0.8094 selected 0.4300 kl 0.0277 ll -1.3035\n",
      "Epoch 37 Iter 12800 loss=0.4006 elbo -0.4785 sf 0.7199 selected 0.4306 kl 0.0371 ll -1.1937\n",
      "Epoch 37 Iter 12900 loss=0.4117 elbo -0.6543 sf 0.8956 selected 0.3544 kl 0.0208 ll -1.5473\n",
      "\n",
      "# epoch 37 iter 12958: dev loss 0.4944 elbo -0.4944 sf 0.2985 selected 0.0706 kl 0.0253 ll -0.7676 acc 0.3706\n",
      " dev0 [gold=3,pred=1]: it 's a **lovely** **film** **with** **lovely** **performances** by **buy** and accorsi .\n",
      " dev1 [gold=2,pred=1]: no one goes unindicted here , which is probably for the best .\n",
      " dev2 [gold=3,pred=1]: and if you 're not nearly moved to tears by a couple of scenes , you 've got ice water in your veins .\n",
      "\n",
      "Shuffling training data\n",
      "Epoch 38 Iter 13000 loss=0.4102 elbo -0.4315 sf 0.6055 selected 0.4082 kl 0.0226 ll -1.0341\n",
      "Epoch 38 Iter 13100 loss=0.4198 elbo -0.3591 sf 0.5404 selected 0.3836 kl 0.0194 ll -0.8969\n",
      "Epoch 38 Iter 13200 loss=0.4054 elbo -0.3955 sf 0.6809 selected 0.4151 kl 0.0234 ll -1.0733\n",
      "\n",
      "# epoch 38 iter 13299: dev loss 0.4986 elbo -0.4986 sf 0.2945 selected 0.0690 kl 0.0252 ll -0.7679 acc 0.3697\n",
      " dev0 [gold=3,pred=1]: it 's a **lovely** **film** **with** **lovely** **performances** by **buy** and **accorsi** .\n",
      " dev1 [gold=2,pred=1]: no one goes unindicted here , which is probably for the best .\n",
      " dev2 [gold=3,pred=1]: and if you 're not nearly moved to tears by a couple of scenes , you 've got ice water in your veins .\n",
      "\n",
      "Epoch 39 Iter 13300 loss=0.4376 elbo -0.4160 sf 0.5670 selected 0.4039 kl 0.0194 ll -0.9804\n",
      "Shuffling training data\n",
      "Epoch 39 Iter 13400 loss=0.4231 elbo -0.4326 sf 0.6014 selected 0.3800 kl 0.0262 ll -1.0305\n",
      "Epoch 39 Iter 13500 loss=0.3939 elbo -0.2050 sf 0.2994 selected 0.4173 kl 0.0221 ll -0.5014\n",
      "Epoch 39 Iter 13600 loss=0.3941 elbo -0.4380 sf 0.6431 selected 0.4021 kl 0.0255 ll -1.0776\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "# epoch 39 iter 13640: dev loss 0.4806 elbo -0.4806 sf 0.3092 selected 0.0829 kl 0.0280 ll -0.7617 acc 0.3842\n",
      " dev0 [gold=3,pred=1]: it 's a **lovely** **film** **with** **lovely** **performances** by **buy** **and** **accorsi** .\n",
      " dev1 [gold=2,pred=1]: no one goes unindicted here , which is probably for the best .\n",
      " dev2 [gold=3,pred=1]: and if you 're not nearly moved to tears by a couple of scenes , you 've got ice water in your veins .\n",
      "\n",
      "Shuffling training data\n",
      "Epoch 40 Iter 13700 loss=0.3833 elbo -0.4911 sf 0.8743 selected 0.4242 kl 0.0211 ll -1.3626\n",
      "Epoch 40 Iter 13800 loss=0.3944 elbo -0.2609 sf 0.5201 selected 0.4086 kl 0.0304 ll -0.7769\n",
      "Epoch 40 Iter 13900 loss=0.4092 elbo -0.5335 sf 0.8533 selected 0.4037 kl 0.0276 ll -1.3830\n",
      "\n",
      "# epoch 40 iter 13981: dev loss 0.4769 elbo -0.4769 sf 0.3126 selected 0.0867 kl 0.0291 ll -0.7604 acc 0.3842\n",
      " dev0 [gold=3,pred=1]: it 's a **lovely** **film** **with** **lovely** **performances** by **buy** **and** **accorsi** .\n",
      " dev1 [gold=2,pred=1]: no one goes unindicted here , which is probably for the best .\n",
      " dev2 [gold=3,pred=1]: and if you 're not nearly moved to tears by a couple of scenes , you 've got ice water in your veins .\n",
      "\n",
      "Epoch    40: reducing learning rate of group 0 to 1.2500e-05.\n",
      "Epoch 41 Iter 14000 loss=0.4039 elbo -0.4935 sf 0.8416 selected 0.3794 kl 0.0207 ll -1.3322\n",
      "Shuffling training data\n",
      "Epoch 41 Iter 14100 loss=0.4042 elbo -0.4660 sf 0.6016 selected 0.3977 kl 0.0278 ll -1.0637\n",
      "Epoch 41 Iter 14200 loss=0.3818 elbo -0.3395 sf 0.5605 selected 0.3851 kl 0.0271 ll -0.8961\n",
      "Epoch 41 Iter 14300 loss=0.4067 elbo -0.3266 sf 0.5545 selected 0.4021 kl 0.0267 ll -0.8773\n",
      "\n",
      "# epoch 41 iter 14322: dev loss 0.4824 elbo -0.4824 sf 0.3073 selected 0.0788 kl 0.0270 ll -0.7627 acc 0.3797\n",
      " dev0 [gold=3,pred=1]: it 's a **lovely** **film** **with** **lovely** **performances** by **buy** and **accorsi** .\n",
      " dev1 [gold=2,pred=1]: no one goes unindicted here , which is probably for the best .\n",
      " dev2 [gold=3,pred=1]: and if you 're not nearly moved to tears by a couple of scenes , you 've got ice water in your veins .\n",
      "\n",
      "Shuffling training data\n",
      "Epoch 42 Iter 14400 loss=0.4089 elbo -0.5482 sf 0.8192 selected 0.4181 kl 0.0260 ll -1.3637\n",
      "Epoch 42 Iter 14500 loss=0.3899 elbo -0.3413 sf 0.5713 selected 0.4119 kl 0.0190 ll -0.9098\n",
      "Epoch 42 Iter 14600 loss=0.4057 elbo -0.3214 sf 0.4703 selected 0.3859 kl 0.0276 ll -0.7876\n",
      "\n",
      "# epoch 42 iter 14663: dev loss 0.4881 elbo -0.4881 sf 0.3018 selected 0.0722 kl 0.0252 ll -0.7647 acc 0.3715\n",
      " dev0 [gold=3,pred=1]: it 's a **lovely** **film** **with** **lovely** **performances** by **buy** and **accorsi** .\n",
      " dev1 [gold=2,pred=1]: no one goes unindicted here , which is probably for the best .\n",
      " dev2 [gold=3,pred=1]: and if you 're not nearly moved to tears by a couple of scenes , you 've got ice water in your veins .\n",
      "\n",
      "Epoch 43 Iter 14700 loss=0.4025 elbo -0.6174 sf 0.9944 selected 0.3832 kl 0.0279 ll -1.6077\n",
      "Shuffling training data\n",
      "Epoch 43 Iter 14800 loss=0.4011 elbo -0.4294 sf 0.6694 selected 0.4164 kl 0.0213 ll -1.0957\n",
      "Epoch 43 Iter 14900 loss=0.4134 elbo -0.4969 sf 0.6282 selected 0.3648 kl 0.0173 ll -1.1225\n",
      "Epoch 43 Iter 15000 loss=0.4116 elbo -0.4254 sf 0.7460 selected 0.4041 kl 0.0214 ll -1.1681\n",
      "\n",
      "# epoch 43 iter 15004: dev loss 0.4954 elbo -0.4954 sf 0.2970 selected 0.0621 kl 0.0226 ll -0.7699 acc 0.3724\n",
      " dev0 [gold=3,pred=1]: it 's a **lovely** **film** **with** **lovely** **performances** by **buy** and accorsi .\n",
      " dev1 [gold=2,pred=1]: no one goes unindicted here , which is probably for the best .\n",
      " dev2 [gold=3,pred=1]: and if you 're not nearly moved to tears by a couple of scenes , you 've got ice water in your veins .\n",
      "\n",
      "Shuffling training data\n",
      "Epoch 44 Iter 15100 loss=0.4118 elbo -0.3096 sf 0.4712 selected 0.4150 kl 0.0162 ll -0.7784\n",
      "Epoch 44 Iter 15200 loss=0.4228 elbo -0.4915 sf 0.7604 selected 0.3624 kl 0.0204 ll -1.2488\n",
      "Epoch 44 Iter 15300 loss=0.4071 elbo -0.5001 sf 0.8040 selected 0.3943 kl 0.0237 ll -1.3004\n",
      "\n",
      "# epoch 44 iter 15345: dev loss 0.4982 elbo -0.4982 sf 0.2944 selected 0.0587 kl 0.0216 ll -0.7711 acc 0.3751\n",
      " dev0 [gold=3,pred=1]: it 's a **lovely** **film** with **lovely** **performances** by **buy** and accorsi .\n",
      " dev1 [gold=2,pred=1]: no one goes unindicted here , which is probably for the best .\n",
      " dev2 [gold=3,pred=1]: and if you 're not nearly moved to tears by a couple of scenes , you 've got ice water in your veins .\n",
      "\n",
      "Shuffling training data\n",
      "Epoch 45 Iter 15400 loss=0.4168 elbo -0.4285 sf 0.6473 selected 0.3600 kl 0.0214 ll -1.0725\n",
      "Epoch 45 Iter 15500 loss=0.4127 elbo -0.4287 sf 0.6280 selected 0.3920 kl 0.0202 ll -1.0535\n",
      "Epoch 45 Iter 15600 loss=0.4198 elbo -0.2493 sf 0.4499 selected 0.4298 kl 0.0225 ll -0.6956\n",
      "\n",
      "# epoch 45 iter 15686: dev loss 0.4768 elbo -0.4768 sf 0.3117 selected 0.0752 kl 0.0251 ll -0.7633 acc 0.3733\n",
      " dev0 [gold=3,pred=1]: it 's a **lovely** **film** **with** **lovely** **performances** by **buy** and **accorsi** .\n",
      " dev1 [gold=2,pred=1]: no one goes unindicted here , which is probably for the best .\n",
      " dev2 [gold=3,pred=1]: and if you 're not nearly moved to tears by a couple of scenes , you 've got ice water in your veins .\n",
      "\n",
      "Epoch 46 Iter 15700 loss=0.3981 elbo -0.4320 sf 0.6680 selected 0.3851 kl 0.0230 ll -1.0964\n",
      "Shuffling training data\n",
      "Epoch 46 Iter 15800 loss=0.4016 elbo -0.4245 sf 0.6647 selected 0.4211 kl 0.0350 ll -1.0837\n",
      "Epoch 46 Iter 15900 loss=0.3738 elbo -0.3646 sf 0.5577 selected 0.4024 kl 0.0231 ll -0.9187\n",
      "Epoch 46 Iter 16000 loss=0.4003 elbo -0.6182 sf 1.1304 selected 0.3925 kl 0.0271 ll -1.7443\n",
      "\n",
      "# epoch 46 iter 16027: dev loss 0.4705 elbo -0.4705 sf 0.3176 selected 0.0782 kl 0.0256 ll -0.7624 acc 0.3778\n",
      " dev0 [gold=3,pred=1]: it 's a **lovely** **film** **with** **lovely** **performances** by **buy** and **accorsi** .\n",
      " dev1 [gold=2,pred=1]: no one goes unindicted here , which is probably for the best .\n",
      " dev2 [gold=3,pred=1]: and if you 're not nearly moved to tears by a couple of scenes , you 've got ice water in your veins .\n",
      "\n",
      "Shuffling training data\n",
      "Epoch 47 Iter 16100 loss=0.3886 elbo -0.4982 sf 0.8499 selected 0.4041 kl 0.0315 ll -1.3431\n",
      "Epoch 47 Iter 16200 loss=0.4013 elbo -0.2653 sf 0.4842 selected 0.4188 kl 0.0300 ll -0.7446\n",
      "Epoch 47 Iter 16300 loss=0.3879 elbo -0.3205 sf 0.4859 selected 0.3939 kl 0.0172 ll -0.8036\n",
      "\n",
      "# epoch 47 iter 16368: dev loss 0.4729 elbo -0.4729 sf 0.3156 selected 0.0724 kl 0.0239 ll -0.7646 acc 0.3733\n",
      " dev0 [gold=3,pred=1]: it 's a **lovely** **film** **with** **lovely** **performances** by **buy** and **accorsi** .\n",
      " dev1 [gold=2,pred=1]: no one goes unindicted here , which is probably for the best .\n",
      " dev2 [gold=3,pred=1]: and if you 're not nearly moved to tears by a couple of scenes , you 've got ice water in your veins .\n",
      "\n",
      "Epoch 48 Iter 16400 loss=0.3900 elbo -0.1938 sf 0.3290 selected 0.4525 kl 0.0249 ll -0.5187\n",
      "Shuffling training data\n",
      "Epoch 48 Iter 16500 loss=0.3813 elbo -0.2805 sf 0.5772 selected 0.4151 kl 0.0168 ll -0.8549\n",
      "Epoch 48 Iter 16600 loss=0.3963 elbo -0.2656 sf 0.4525 selected 0.3891 kl 0.0257 ll -0.7139\n",
      "Epoch 48 Iter 16700 loss=0.3990 elbo -0.3558 sf 0.5428 selected 0.4000 kl 0.0177 ll -0.8957\n",
      "\n",
      "# epoch 48 iter 16709: dev loss 0.4743 elbo -0.4743 sf 0.3145 selected 0.0691 kl 0.0230 ll -0.7658 acc 0.3697\n",
      " dev0 [gold=3,pred=1]: it 's a **lovely** **film** **with** **lovely** **performances** by **buy** and **accorsi** .\n",
      " dev1 [gold=2,pred=1]: no one goes unindicted here , which is probably for the best .\n",
      " dev2 [gold=3,pred=1]: and if you 're not nearly moved to tears by a couple of scenes , you 've got ice water in your veins .\n",
      "\n",
      "Shuffling training data\n",
      "Epoch 49 Iter 16800 loss=0.3845 elbo -0.2926 sf 0.4207 selected 0.4267 kl 0.0230 ll -0.7094\n",
      "Epoch 49 Iter 16900 loss=0.3966 elbo -0.3870 sf 0.5572 selected 0.4010 kl 0.0170 ll -0.9413\n",
      "Epoch 49 Iter 17000 loss=0.3952 elbo -0.4576 sf 0.6871 selected 0.3970 kl 0.0195 ll -1.1414\n",
      "\n",
      "# epoch 49 iter 17050: dev loss 0.4735 elbo -0.4735 sf 0.3147 selected 0.0671 kl 0.0225 ll -0.7657 acc 0.3751\n",
      " dev0 [gold=3,pred=1]: it 's a **lovely** **film** **with** **lovely** **performances** by **buy** and **accorsi** .\n",
      " dev1 [gold=2,pred=1]: no one goes unindicted here , which is probably for the best .\n",
      " dev2 [gold=3,pred=1]: and if you 're not nearly moved to tears by a couple of scenes , you 've got ice water in your veins .\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50 Iter 17100 loss=0.3730 elbo -0.3208 sf 0.4527 selected 0.4044 kl 0.0245 ll -0.7693\n",
      "Shuffling training data\n",
      "Epoch 50 Iter 17200 loss=0.3930 elbo -0.3657 sf 0.5999 selected 0.4082 kl 0.0197 ll -0.9622\n",
      "Epoch 50 Iter 17300 loss=0.3864 elbo -0.3793 sf 0.6709 selected 0.4171 kl 0.0209 ll -1.0465\n",
      "\n",
      "# epoch 50 iter 17391: dev loss 0.4869 elbo -0.4869 sf 0.3030 selected 0.0552 kl 0.0197 ll -0.7703 acc 0.3778\n",
      " dev0 [gold=3,pred=1]: it 's a **lovely** **film** with **lovely** **performances** by **buy** and accorsi .\n",
      " dev1 [gold=2,pred=1]: no one goes unindicted here , which is probably for the best .\n",
      " dev2 [gold=3,pred=1]: and if you 're not nearly moved to tears by a couple of scenes , you 've got ice water in your veins .\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cfg['layer_inf'] = 'lstm'\n",
    "lstm_model = train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь посмотрим то же самое для lstm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_important_counter, lstm_all_counter = make_counters(lstm_model, dev_data, len(dev_data), device, cfg, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Сколько разные части речи считаются важными:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Сколько разные части речи считаются важными:\")\n",
    "lstm_important_counter.most_common()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Частота важности частей речи:\n",
      "PRP 0.0\n",
      "VBZ 0.0\n",
      "DT 0.0\n",
      "JJ 0.0\n",
      "NN 0.0\n",
      "IN 0.0\n",
      "NNS 0.0\n",
      "CC 0.0\n",
      ". 0.0\n",
      "VBN 0.0\n",
      "RB 0.0\n",
      ", 0.0\n",
      "WDT 0.0\n",
      "JJS 0.0\n",
      "VBP 0.0\n",
      "TO 0.0\n",
      "PRP$ 0.0\n",
      "VB 0.0\n",
      "CD 0.0\n",
      "VBG 0.0\n",
      "NNP 0.0\n",
      "POS 0.0\n",
      "MD 0.0\n",
      ": 0.0\n",
      "RBR 0.0\n",
      "VBD 0.0\n",
      "WRB 0.0\n",
      "WP 0.0\n",
      "EX 0.0\n",
      "JJR 0.0\n",
      "RP 0.0\n",
      "`` 0.0\n",
      "'' 0.0\n",
      "RBS 0.0\n",
      "WP$ 0.0\n",
      "PDT 0.0\n",
      "FW 0.0\n",
      "UH 0.0\n",
      "$ 0.0\n"
     ]
    }
   ],
   "source": [
    "fractions = []\n",
    "for token in all_counter:\n",
    "    fractions.append((token, lstm_important_counter[token] / lstm_all_counter[token]))\n",
    "print(\"Частота важности частей речи:\")\n",
    "for (token, frac) in sorted(fractions, key=lambda pair: -pair[1]):\n",
    "    print(token, frac)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "SST.ipynb",
   "provenance": [],
   "toc_visible": true,
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
