{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/probabll/dgm4nlp/blob/master/notebooks/sst/SST.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Udt3kHMdWvYe"
   },
   "source": [
    "We will need to import some helper code, so we need to run this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "U8eXUCRiWvYi"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "nb_dir = os.path.split(os.getcwd())[0]\n",
    "if nb_dir not in sys.path:\n",
    "    sys.path.append(nb_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rYhItDYMZi6a"
   },
   "source": [
    "# Colab\n",
    "\n",
    "We will need to download some data for this notebook, so if you are using [colab](https://colab.research.google.com), set the `using_colab` flag below to `True` in order to clone our [github repo](https://github.com/probabll/dgm4nlp)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_shCMftIx1rW"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "README.md                     kolka_working_SST-Copy1.ipynb\r\n",
      "SST.ipynb                     kolka_working_SST.ipynb\r\n",
      "__init__.py                   my_first_SST.ipynb\r\n",
      "\u001b[1m\u001b[36m__pycache__\u001b[m\u001b[m                   \u001b[1m\u001b[36mnn\u001b[m\u001b[m\r\n",
      "\u001b[1m\u001b[36mdata\u001b[m\u001b[m                          \u001b[31mplotting.py\u001b[m\u001b[m\r\n",
      "evaluate.py                   sstutil.py\r\n",
      "\u001b[1m\u001b[36mimg\u001b[m\u001b[m                           util.py\r\n",
      "kolka_2_SST.ipynb\r\n"
     ]
    }
   ],
   "source": [
    "using_colab = not True\n",
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "N-fFME2OW22i"
   },
   "outputs": [],
   "source": [
    "if using_colab:\n",
    "  !rm -fr dgm4nlp sst\n",
    "  !git clone https://github.com/probabll/dgm4nlp.git\n",
    "  !cp -R dgm4nlp/notebooks/sst ./  \n",
    "  !ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "l_7NCZlZacNu"
   },
   "source": [
    "Now we can start our lab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "s9mH-rUhWvYq"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "# CPU should be fine for this lab\n",
    "device = torch.device('cpu')  \n",
    "#device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')  \n",
    "from collections import OrderedDict\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "okMoxTJ9bWjc"
   },
   "source": [
    "# Sentiment Classification \n",
    "\n",
    "\n",
    "We are going to augment a sentiment classifier with a layer of discrete latent variables which will help us improve the model's interpretability. But first, let's quickly review the baseline task.\n",
    "\n",
    "\n",
    "In sentiment classification, we have some text input $x = \\langle x_1, \\ldots, x_n \\rangle$, e.g. a sentence or short paragraph, which expresses a certain sentiment $y$, i.e. one of $K$ classes, towards a subject (e.g. a film or a product). \n",
    "\n",
    "\n",
    "\n",
    "We can learn a sentiment classifier by learning a categorical distribution over classes for a given input:\n",
    "\n",
    "\\begin{align}\n",
    "Y|x &\\sim \\text{Cat}(f(x; \\theta))\n",
    "\\end{align}\n",
    "\n",
    "where the Categorical pmf is $\\text{Cat}(y|\\pi) = \\pi_y$.\n",
    "\n",
    "A categorical distribution over $K$ classes is parameterised by a $K$-dimensional probability vector, here we use a neural network $f$ to map from the input to this probability vector. Technically we say *a neural network parameterise our model*, that is, it computes the parameters of our categorical observation model. The figure below is a graphical depiction of the model: circled nodes are random variables (a shaded node is an observed variable), uncircled nodes are deterministic, a plate indicates multiple draws.\n",
    "\n",
    "<img src=\"https://github.com/probabll/dgm4nlp/raw/master/notebooks/sst/img/classifier.png\"  height=\"100\">\n",
    "\n",
    "The neural network (NN) $f(\\cdot; \\theta)$ has parameters of its own, i.e. the weights of the various architecture blocks used, which we denoted generically by $\\theta$.\n",
    "\n",
    "Suppose we have a dataset $\\mathcal D = \\{(x^{(1)}, y^{(1)}), \\ldots, (x^{(N)}, y^{(N)})\\}$ containing $N$ i.i.d. observations. Then we can use the log-likelihood function \n",
    "\\begin{align}\n",
    "\\mathcal L(\\theta|\\mathcal D) &= \\sum_{k=1}^{N} \\log P(y^{(k)}|x^{(k)}, \\theta) \\\\\n",
    "&= \\sum_{k=1}^{N} \\log \\text{Cat}(y^{(k)}|f(x^{(k)}; \\theta))\n",
    "\\end{align}\n",
    " to estimate $\\theta$ by maximisation:\n",
    " \\begin{align}\n",
    " \\theta^\\star = \\arg\\max_{\\theta \\in \\Theta} \\mathcal L(\\theta|\\mathcal D) ~ .\n",
    " \\end{align}\n",
    " \n",
    "\n",
    "We can use stochastic gradient-ascent to find a local optimum of $\\mathcal L(\\theta|\\mathcal D)$, which only requires a gradient estimate:\n",
    "\n",
    "\\begin{align}\n",
    "\\nabla_\\theta \\mathcal L(\\theta|\\mathcal D) &= \\sum_{k=1}^{|\\mathcal D|} \\nabla_\\theta  \\log P(y^{(k)}|x^{(k)}, \\theta) \\\\ \n",
    "&= \\sum_{k=1}^{|\\mathcal D|} \\frac{1}{N} N \\nabla_\\theta  \\log P(y^{(k)}|x^{(k)}, \\theta)  \\\\\n",
    "&= \\mathbb E_{\\mathcal U(1/N)} \\left[ N \\nabla_\\theta  \\log P(y^{(K)}|x^{(K)}, \\theta) \\right]  \\\\\n",
    "&\\overset{\\text{MC}}{\\approx} \\frac{N}{M} \\sum_{m=1}^M \\nabla_\\theta  \\log P(y^{(k_m)}|x^{(k_m)}, \\theta) \\\\\n",
    "&\\text{where }K_m \\sim \\mathcal U(1/N)\n",
    "\\end{align}\n",
    "\n",
    "This is a Monte Carlo (MC) estimate of the gradient computed on $M$ data points selected uniformly at random from $\\mathcal D$.\n",
    "\n",
    "For as long as $f$ remains differentiable wrt to its inputs and parameters, we can rely on automatic differentiation to obtain gradient estimates.\n",
    "\n",
    "In what follows we show how to design $f$ and how to extend this basic model to a latent-variable model.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4LUjyO-39zan"
   },
   "source": [
    "## Data\n",
    "\n",
    "We provide you some code to load the data (see `sst.sstutil.examplereader`). Play with the snippet below and inspect a few training instances:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4z8Bt5no9z6w"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data\n",
      "train 8544\n",
      "dev 1101\n",
      "test 2210\n",
      "\n",
      "# Examples\n",
      "First dev example: Example(tokens=['It', \"'s\", 'a', 'lovely', 'film', 'with', 'lovely', 'performances', 'by', 'Buy', 'and', 'Accorsi', '.'], label=3, transitions=[0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1], token_labels=[2, 2, 2, 3, 2, 2, 3, 2, 2, 2, 2, 2, 2])\n",
      "First dev example tokens: ['It', \"'s\", 'a', 'lovely', 'film', 'with', 'lovely', 'performances', 'by', 'Buy', 'and', 'Accorsi', '.']\n",
      "First dev example label: 3\n"
     ]
    }
   ],
   "source": [
    "from sst.sstutil import examplereader, Vocabulary, load_glove    \n",
    "\n",
    "\n",
    "# Let's load the data into memory.\n",
    "print(\"Loading data\")\n",
    "train_data = list(examplereader('../sst/data/sst/train.txt'))\n",
    "dev_data = list(examplereader('../sst/data/sst/dev.txt'))\n",
    "test_data = list(examplereader('../sst/data/sst/test.txt'))\n",
    "\n",
    "print(\"train\", len(train_data))\n",
    "print(\"dev\", len(dev_data))\n",
    "print(\"test\", len(test_data))\n",
    "\n",
    "print('\\n# Examples')\n",
    "example = dev_data[0]\n",
    "print(\"First dev example:\", example)\n",
    "print(\"First dev example tokens:\", example.tokens)\n",
    "print(\"First dev example label:\", example.label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lB2lEsNuWvYx"
   },
   "source": [
    "## Architecture\n",
    "\n",
    "\n",
    "The function $f$ conditions on a high-dimensional input (i.e. text), so we need to convert it to continuous real vectors. This is the job an *encoder*. \n",
    "\n",
    "**Embedding Layer**\n",
    "\n",
    "The first step is to convert the words in $x$ to vectors, which in this lab we will do with a pre-trained embedding layer (we will use GloVe).\n",
    "\n",
    "We will denote the embedding of the $i$th word of the input by:\n",
    "\n",
    "\\begin{equation}\n",
    "\\mathbf x_i = \\text{glove}(x_i)\n",
    "\\end{equation}\n",
    "\n",
    "**Encoder Layer**\n",
    "\n",
    "In this lab, an encoder takes a sequence of input vectors $\\mathbf x_1^n$, each $I$-dimensional, and produces a sequence of output vectors $\\mathbf t_1^n$, each $O$-dimensional and a summary vector $\\mathbf h \\in \\mathbb R^O$:\n",
    "\n",
    "\\begin{equation}\n",
    "    \\mathbf t_1^n, \\mathbf h = \\text{encoder}(\\mathbf x_1^n; \\theta_{\\text{enc}})\n",
    "\\end{equation}\n",
    "\n",
    "where we use $\\theta_{\\text{enc}}$ to denote the subset of parameters in $\\theta$ that are specific to this encoder block. \n",
    "\n",
    "*Remark:* in practice for a correct batched implementation, our encoders also take a mask matrix and a vector of lengths.\n",
    "\n",
    "Examples of encoding functions can be a feed-forward NN (with an aggregator based on sum or average/max pooling) or a recurrent NN (e.g. an LSTM/GRU). Other architectures are also possible.\n",
    "\n",
    "**Output Layer**\n",
    "\n",
    "From our summary vector $\\mathbf h$, we need to parameterise a categorical distribution over $K$ classes, thus we use\n",
    "\n",
    "\\begin{align}\n",
    "f(x; \\theta) &= \\text{softmax}(\\text{dense}_K(\\mathbf h; \\theta_{\\text{output}}))\n",
    "\\end{align}\n",
    "\n",
    "where $\\text{dense}_K$ is a dense layer with $K=5$ outputs and $\\theta_{\\text{output}}$ corresponds to its parameters (weight matrix and bias vector). Note that we need to use the softmax activation function in order to guarantee that the output of $f$ is a normalised probability vector.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Kc15Nv2i41cq"
   },
   "source": [
    "## Implementation\n",
    "\n",
    "To leave an indication of the shape of tensors in the code, we use the following convention\n",
    "\n",
    "```python\n",
    "[B, T, D]\n",
    "```\n",
    "\n",
    "where `B` stands for `batch_size`, `T` stands for `time` (or rather *maximum sequence length*), and `D` is the size of the representation.\n",
    "\n",
    "\n",
    "Consider the following abstract Encoder class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xwEPXT2MWvYz",
    "tags": [
     "encoders"
    ]
   },
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    \"\"\"\n",
    "    An Encoder for us is a function that\n",
    "      1. transforms a sequence of I-dimensional vectors into a sequence of O-dimensional vectors\n",
    "      2. summarises a sequence of I-dimensional vectors into one O-dimensional vector\n",
    "      \n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    def __init__(self):\n",
    "        super(Encoder, self).__init__()\n",
    "        \n",
    "    def forward(self, inputs, mask, lengths):\n",
    "        \"\"\"\n",
    "        The input is a batch-first tensor of token ids. Here is an example:\n",
    "        \n",
    "        Example of inputs (though rather than words, we have word ids):\n",
    "            INPUTS                     MASK       LENGTHS\n",
    "            [the nice cat -PAD-]    -> [1 1 1 0]  [3]\n",
    "            [the nice dog running]  -> [1 1 1 1]  [4]\n",
    "            \n",
    "        Note that:\n",
    "              mask =  inputs == 1\n",
    "              lengths = mask.sum(dim=-1)\n",
    "        \n",
    "        :param inputs: [B, T, I]\n",
    "        :param mask: [B, T]\n",
    "        :param lengths: [B]\n",
    "        :returns: [B, T, O], [B, O]\n",
    "            where the first tensor is the transformed input\n",
    "            and the second tensor is a summary of all inputs\n",
    "        \"\"\"\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WA5wmkcRg9Am"
   },
   "source": [
    "Let's start easy, implement a *bag of words* encoder:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "U-9hLQ0lF5SG"
   },
   "outputs": [],
   "source": [
    "class BagOfWordsEncoder(Encoder):\n",
    "    \"\"\"\n",
    "    This encoder does not transform the input sequence, \n",
    "     and its summary output is just a sum.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(BagOfWordsEncoder, self).__init__()\n",
    "        \n",
    "    def forward(self, inputs, mask, lengths=None, **kwargs):\n",
    "        reshaped_mask = mask.float().unsqueeze(-1)  # shape: [B, T, 1]\n",
    "        return inputs, (inputs * reshaped_mask).sum(1) / (reshaped_mask.sum(1) + 1e-8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IS7x0hLrUXfN"
   },
   "source": [
    "You can also consider implementing\n",
    "\n",
    "* a feed-forward encoder with average pooling\n",
    "* and a biLSTM encoder\n",
    "\n",
    "but these are certainly optional."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BpOGFpK_Uo0-"
   },
   "outputs": [],
   "source": [
    "class FFEncoder(Encoder):\n",
    "    \"\"\"\n",
    "    A typical feed-forward NN with tanh hidden activations.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self,\n",
    "                 input_size,\n",
    "                 output_size, \n",
    "                 activation=None, \n",
    "                 hidden_sizes=[], \n",
    "                 aggregator='sum',\n",
    "                 dropout=0.5):\n",
    "        \"\"\"\n",
    "        :param input_size: int\n",
    "        :param output_size: int\n",
    "        :param hidden_sizes: list of integers (dimensionality of hidden layers)\n",
    "        :param aggregator: 'sum' or 'avg'\n",
    "        :param dropout: dropout rate\n",
    "        \"\"\"\n",
    "        super(FFEncoder, self).__init__()\n",
    "        layers = []\n",
    "        \n",
    "        def add_droput(name):\n",
    "            if dropout > 0:\n",
    "                layers.append((name, nn.Dropout(p=dropout)))\n",
    "        \n",
    "        if len(hidden_sizes) > 0:                    \n",
    "            for i, size in enumerate(hidden_sizes):\n",
    "                add_droput('dropout_{}'.format(i))\n",
    "                layers.append(('linear_{}'.format(i), nn.Linear(input_size, size)))\n",
    "                layers.append(('tanh_{}'.format(i), nn.Tanh()))\n",
    "                input_size = size\n",
    "\n",
    "        last_output_size = hidden_sizes[-1] if len(hidden_sizes) > 0 else input_size\n",
    "        add_droput('final_dropout')\n",
    "        layers.append(('final_linear', nn.Linear(last_output_size, output_size)))       \n",
    "        self.layer = nn.Sequential(OrderedDict(layers))\n",
    "\n",
    "        self.activation = activation\n",
    "\n",
    "        if not aggregator in ['sum', 'avg']:\n",
    "            raise ValueError(\"I can only aggregate outputs using 'sum' or 'avg'\")\n",
    "        self.aggregator = aggregator\n",
    "        \n",
    "    def forward(self, inputs, mask, lengths):\n",
    "        outputs = self.layer(inputs)  # shape: [B, T, O]\n",
    "        if not self.activation is None:\n",
    "            outputs = self.activation(outputs)  # shape: [B, T, O]\n",
    "        reshaped_mask = mask.float().unsqueeze(-1)  # shape: [B, T, 1]\n",
    "        summary = (outputs * reshaped_mask).sum(dim=1)  # shape: [B, O]\n",
    "        if self.aggregator == 'avg':\n",
    "            reshaped_lens = lengths.float().unsqueeze(-1)  # shape: [B, 1]\n",
    "            summary /= reshaped_lens  # shape: [B, O]\n",
    "        return outputs, summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IxQ5djZ_VAvK"
   },
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "\n",
    "\n",
    "class LSTMEncoder(Encoder):\n",
    "    \"\"\"\n",
    "    This module encodes a sequence into a single vector using an LSTM,\n",
    "     it also returns the hidden states at each time step.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 in_features,\n",
    "                 hidden_size: int=200,\n",
    "                 batch_first: bool=True,\n",
    "                 bidirectional: bool=True):\n",
    "        \"\"\"\n",
    "        :param in_features:\n",
    "        :param hidden_size:\n",
    "        :param batch_first:\n",
    "        :param bidirectional:\n",
    "        \"\"\"\n",
    "        super(LSTMEncoder, self).__init__()\n",
    "        self.lstm = nn.LSTM(\n",
    "            in_features,\n",
    "            hidden_size,\n",
    "            batch_first=batch_first,\n",
    "            bidirectional=bidirectional\n",
    "        )\n",
    "        self.batch_first = batch_first\n",
    "\n",
    "    def forward(self, x, mask, lengths):\n",
    "        \"\"\"\n",
    "        Encode sentence x\n",
    "        :param x: sequence of word embeddings, shape [B, T, E]\n",
    "        :param mask: byte mask that is 0 for invalid positions, shape [B, T]\n",
    "        :param lengths: the lengths of each input sequence [B]\n",
    "        :return:\n",
    "        \"\"\"\n",
    "\n",
    "        packed_sequence = pack_padded_sequence(x, lengths, batch_first=self.batch_first)\n",
    "        outputs, (hx, cx) = self.lstm(packed_sequence)\n",
    "        outputs, _ = pad_packed_sequence(outputs, batch_first=self.batch_first)\n",
    "\n",
    "        # classify from concatenation of final states\n",
    "        if self.lstm.bidirectional:\n",
    "            final = torch.cat([hx[-2], hx[-1]], dim=-1)\n",
    "        else:  # classify from final state\n",
    "            final = hx[-1]\n",
    "\n",
    "        return outputs, final"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "s_zz5zIyVkSh"
   },
   "source": [
    "Here is some helper code to select and return an encoder:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "59ZU6JddVjMV"
   },
   "outputs": [],
   "source": [
    "def get_encoder(layer, in_features, hidden_size, bidirectional=True):\n",
    "    \"\"\"Returns the requested layer.\"\"\"\n",
    "\n",
    "    # TODO: make pass and average layers\n",
    "    if layer == \"bow\":\n",
    "        return BagOfWordsEncoder()\n",
    "    elif layer == 'ff':\n",
    "        return FFEncoder(\n",
    "            in_features, \n",
    "            2 * hidden_size,   # for convenience\n",
    "            hidden_sizes=[hidden_size], \n",
    "            aggregator='avg'\n",
    "        )\n",
    "    elif layer == \"lstm\":\n",
    "        return LSTMEncoder(\n",
    "            in_features, \n",
    "            hidden_size,\n",
    "            bidirectional=bidirectional\n",
    "        )\n",
    "    else:\n",
    "        raise ValueError(\"Unknown layer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kY8LZiMN5CHW"
   },
   "source": [
    "# Sentiment Classification with Latent Rationale\n",
    "\n",
    "A latent rationale is a compact and informative fragment of the input based on which a NN classifier makes its decisions. [Lei et al (2016)](http://aclweb.org/anthology/D16-1011) proposed to induce such rationales along with a regression model for multi-aspect sentiment analsysis, their model is trained via REINFORCE on a dataset of beer reviews.\n",
    "\n",
    "*Remark:* the model we will develop here can be seen as a probabilistic version of their model. The rest of this notebook focus on our own probabilitisc view of the model.\n",
    "\n",
    "The picture below depicts our latent-variable model for rationale extraction:\n",
    "\n",
    "<img src=\"https://github.com/probabll/dgm4nlp/raw/master/notebooks/sst/img/rationale.png\"  height=\"200\">\n",
    "\n",
    "where we augment the model with a collection of latent variables $z = \\langle z_1, \\ldots, z_n\\rangle$ where $z_i$ is a binary latent variable. Each latent variable $z_i$ regulates whether or not the input $x_i$ is available to the classifier.  We use $x \\odot z$ to denote the selected words, which, in the terminology of Lei et al, is a latent rationale.\n",
    "\n",
    "Again the classifier parameterises a Categorical distribution over $K=5$ outcomes, though this time it can encode only a selection of the input:\n",
    "\n",
    "\\begin{align}\n",
    "    Z_i & \\sim \\text{Bern}(p_1) \\\\\n",
    "    Y|z,x &\\sim \\text{Cat}(f(x \\odot z; \\theta))\n",
    "\\end{align}\n",
    "\n",
    "where we have a shared and fixed Bernoulli prior (with parameter $p_1$) for all $n$ latent variables.\n",
    "\n",
    "\n",
    "Here is an example design for $f$:\n",
    "\n",
    "\\begin{align}\n",
    "\\mathbf x_i &= z_i \\, \\text{glove}(x_i) \\\\\n",
    "\\mathbf t_1^n, \\mathbf h &= \\text{encoder}(\\mathbf x_1^n; \\theta_{\\text{enc}}) \\\\\n",
    "f(x \\odot z; \\theta) &= \\text{softmax}(\\text{dense}_K(\\mathbf h; \\theta_{\\text{output}}))\n",
    "\\end{align}\n",
    "\n",
    "where:\n",
    "* $z_i$ either leaves $\\mathbf x_i$ unchanged or turns it into a vector of zeros;\n",
    "* the encoder only sees features from selected inputs, i.e. $x_i$ for which $z_i = 1$;\n",
    "* $\\text{dense}_K$ is a linear layer with $K=5$ outputs.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hDHNxLHMWvY-"
   },
   "source": [
    "## Prior\n",
    "\n",
    "\n",
    "Our prior is a Bernoulli with fixed parameter $0 < p_1 < 1$:\n",
    "\n",
    "\\begin{align}\n",
    "Z_i & \\sim \\text{Bern}(p_1)\n",
    "\\end{align}\n",
    "\n",
    "whose pmf is $\\text{Bern}(z_i|p_1) = p_1^{z_i}\\times (1-p_1)^{1-z_i}$.\n",
    "\n",
    "As we will be using Bernoulli priors and posteriors, it is a good idea to implement a Bernoulli class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iCBcHnTsOuDr"
   },
   "outputs": [],
   "source": [
    "class Bernoulli:\n",
    "    \"\"\"\n",
    "    This class encapsulates a collection of Bernoulli distributions. \n",
    "    Each Bernoulli is uniquely specified by p_1, where\n",
    "        Bernoulli(X=x|p_1) = pow(p_1, x) + pow(1 - p_1, 1 - x)\n",
    "    is the Bernoulli probability mass function (pmf).    \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, logits=None, probs=None):\n",
    "        \"\"\"\n",
    "        We can specify a Bernoulli distribution via a logit or a probability. \n",
    "         You need to specify at least one, and if you specify both, beware that\n",
    "         in this implementation logits will be used.\n",
    "         \n",
    "        Recall that: probs = sigmoid(logits).\n",
    "         \n",
    "        :param logits: a tensor of logits (a logit is defined as log (p_1 / p_0))\n",
    "            where p_0 = 1 - p_1\n",
    "        :param probs: a tensor of probabilities, each in (0, 1)\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        #print(\"probs:\", type(probs))\n",
    "        #print(\"logits:\", type(logits))\n",
    "                \n",
    "        if probs is None and logits is None:\n",
    "            raise ValueError('I need probabilities or logits')   \n",
    "        \n",
    "        self.probs = probs if logits is None else torch.sigmoid(logits)\n",
    "    \n",
    "    def sample(self):\n",
    "        \"\"\"Returns a sample with the same shape as the parameters\"\"\"\n",
    "        return torch.bernoulli(self.probs)\n",
    "    \n",
    "    def log_pmf(self, x):\n",
    "        \"\"\"\n",
    "        Assess the log probability of a sample. \n",
    "        :param x: either a single sample (0 or 1) or a tensor of samples with the same shape as the parameters.\n",
    "        :returns: tensor with log probabilities with the same shape as parameters\n",
    "            (if the input is a single sample we broadcast it to the shape of the parameters)\n",
    "        \"\"\"\n",
    "        return x * torch.log(self.probs) + (1 - x) * torch.log(1. - self.probs)\n",
    "    \n",
    "    def kl(self, other: 'Bernoulli'):\n",
    "        \"\"\"\n",
    "        Compute the KL divergence between two Bernoulli distributions (from self to other).\n",
    "        \n",
    "        :return: KL[self||other] with same shape parameters\n",
    "        \"\"\"\n",
    "        return self.probs * (torch.log(self.probs) - torch.log(other.probs)) \\\n",
    "            + (1 - self.probs) * (torch.log(1 - self.probs) - torch.log(1 - other.probs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "u0yfkCZlWvZP"
   },
   "source": [
    "## Classifier\n",
    "\n",
    "The classifier encodes only a selection of the input, which we denote $x \\odot z$, and parameterises a Categorical distribution over $5$ outcomes (sentiment levels).\n",
    "\n",
    "Thus let's implement a Categorical distribution (we will only need to be able to assess its lgo pmf):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "F-6JLDnBQcdg"
   },
   "outputs": [],
   "source": [
    "class Categorical:\n",
    "    \n",
    "    def __init__(self, log_probs):\n",
    "        # [B, K]: class probs\n",
    "        self.log_probs = log_probs\n",
    "        \n",
    "    def log_pmf(self, x):\n",
    "        \"\"\"\n",
    "        :param x: [B] integers (targets)\n",
    "        :returns: [B] scalars (log probabilities)\n",
    "        \"\"\"\n",
    "        return torch.gather(self.log_probs, 1, x.unsqueeze(-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CdrM_YRI8xBF"
   },
   "source": [
    "and a classifier architecture:\n",
    "\n",
    "* implement the forward method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sz7GaKbgRCd8"
   },
   "outputs": [],
   "source": [
    "class Classifier(nn.Module):\n",
    "    \"\"\"\n",
    "    The Encoder takes an input text (and rationale z) and computes p(y|x,z)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 embed: nn.Embedding=None,\n",
    "                 hidden_size: int=200,\n",
    "                 output_size: int=1,\n",
    "                 dropout: float=0.1,\n",
    "                 layer: str=\"pass\"):\n",
    "\n",
    "        super(Classifier, self).__init__()\n",
    "\n",
    "        emb_size = embed.weight.shape[1]\n",
    "        enc_size = hidden_size * 2\n",
    "        # Here we embed the words\n",
    "        self.embed_layer = nn.Sequential(embed)\n",
    "\n",
    "        self.enc_layer = get_encoder(layer, emb_size, hidden_size)\n",
    "\n",
    "        # and here we predict categorical parameters\n",
    "        self.output_layer = nn.Sequential(\n",
    "            nn.Dropout(p=dropout),\n",
    "            nn.Linear(enc_size, output_size),\n",
    "            nn.LogSoftmax(dim=-1)\n",
    "        )\n",
    "\n",
    "        self.report_params()\n",
    "\n",
    "    def report_params(self):\n",
    "        count = 0\n",
    "        for name, p in self.named_parameters():\n",
    "            if p.requires_grad and \"embed\" not in name:\n",
    "                count += np.prod(list(p.shape))\n",
    "        print(\"{} #params: {}\".format(self.__class__.__name__, count))\n",
    "\n",
    "    def forward(self, x, mask, z) -> Categorical:\n",
    "        \"\"\"\n",
    "        :params x: [B, T, I] word representations\n",
    "        :params mask: [B, T] indicates valid positions\n",
    "        :params z: [B, T] binary selectors\n",
    "        :returns: one Categorical distribution per instance in the batch\n",
    "          each conditioning only on x_i for which z_i = 1\n",
    "        \"\"\"\n",
    "        embeddings = self.embed_layer(x)  # [B, T, E]\n",
    "        embedding_mask = z.float().unsqueeze(-1)  # [B, T, 1]\n",
    "        masked_embeddings = embeddings * embedding_mask  # [B, T, E]\n",
    "        lengths = mask.long().sum(1)\n",
    "\n",
    "        # encode the sentence\n",
    "        _, final = self.enc_layer(masked_embeddings, mask, lengths)\n",
    "\n",
    "        # predict sentiment from final state(s)\n",
    "        log_probs = self.output_layer(final)        \n",
    "        return Categorical(log_probs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "p2waCCBF9MaH"
   },
   "source": [
    "## Inference\n",
    "\n",
    "\n",
    "Computing the log-likelihood of an observation requires marginalising over assignments of $z$:\n",
    "\n",
    "\\begin{align}\n",
    "P(y|x,\\theta,p_1) &= \\sum_{z_1 = 0}^1 \\cdots \\sum_{z_n=0}^1 P(z|p_1)\\times P(y|x,z, \\theta) \\\\\n",
    "&= \\sum_{z_1 = 0}^1 \\cdots \\sum_{z_n=0}^1 \\left( \\prod_{i=1}^n \\text{Bern}(z_i|p_1)\\right) \\times \\text{Cat}(y|f(x \\odot z; \\theta)) \n",
    "\\end{align}\n",
    "\n",
    "This is clearly intractable: there are $2^n$ possible assignments to $z$ and because the classifier conditions on all latent selectors, there's no way to simplify the expression.\n",
    "\n",
    "We will avoid computing this intractable marginal by instead employing an independently parameterised inference model.\n",
    "This inference model $Q(z|x, y, \\lambda)$ is an approximation to the true postrerior $P(z|x, y, \\theta, p_1)$, and we use $\\lambda$ to denote its parameters.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0jcVdYTg8Wun"
   },
   "source": [
    "We make a *mean field* assumption, whereby we model latent variables independently given the input:\n",
    "\\begin{align}\n",
    "Q(z|x, y, \\lambda) \n",
    "    &= \\prod_{i=1}^{n} Q(z_i|x; \\lambda) \\\\\n",
    "    &= \\prod_{i=1}^{n} \\text{Bern}(z_i|g_i(x; \\lambda)) \n",
    "\\end{align}\n",
    "\n",
    "where $g(x; \\lambda)$ is a NN that maps from $x = \\langle x_1, \\ldots, x_n\\rangle$ to $n$ Bernoulli parameters, each of which, is a probability value (thus $0 < g_i(x; \\lambda) < 1$).\n",
    "\n",
    "Note that though we could condition on $y$ for approximate posterior inference, we are opportunistically leaving it out. This way, $Q$ is directly available at test time for making predictions. The figure below is a graphical depiction of the inference model (we show a dashed arrow from $y$ to $z$ to remind you that in principle the label is also available).\n",
    "\n",
    "<img src=\"https://github.com/probabll/dgm4nlp/raw/master/notebooks/sst/img/inference.png\"  height=\"200\">\n",
    "\n",
    "Here is an example design for $g$:\n",
    "\\begin{align}\n",
    "\\mathbf x_i &= \\text{glove}(x_i) \\\\\n",
    "\\mathbf t_1^n, \\mathbf h &= \\text{encoder}(\\mathbf x_1^n; \\lambda_{\\text{enc}}) \\\\\n",
    "g_i(x; \\lambda) &= \\sigma(\\text{dense}_1(\\mathbf t_i; \\lambda_{\\text{output}}))\n",
    "\\end{align}\n",
    "where\n",
    "* $\\text{glove}$ is a pre-trained embedding function;\n",
    "* $\\text{dense}_1$ is a dense layer with a single output;\n",
    "* and $\\sigma(\\cdot)$ is the sigmoid function, necessary to parameterise a Bernoulli distribution.\n",
    "\n",
    "From now on we will write $Q(z|x, \\lambda)$, that is, without $y.\n",
    "\n",
    "Here we implement this product of Bernoulli distributions:\n",
    "\n",
    "* implement $g$ in the constructor \n",
    "* and the forward pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YLxfcAbuSiFo"
   },
   "outputs": [],
   "source": [
    "class ProductOfBernoullis(nn.Module):\n",
    "    \"\"\"\n",
    "    This is an inference network that parameterises independent Bernoulli distributions.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 embed: nn.Embedding,\n",
    "                 hidden_size: int=200,\n",
    "                 layer: str=\"bow\"):\n",
    "        \"\"\"\n",
    "        :param embed: an embedding layer\n",
    "        :param hidden_suze: hidden size for transformed inputs\n",
    "        :param layer: 'bow' for BoW encoding\n",
    "          you may alternatively implement and 'lstm' option\n",
    "          which uses a biLSTM to transform the inputs         \n",
    "        \"\"\"\n",
    "        super(ProductOfBernoullis, self).__init__()\n",
    "        # 1. we should have an embedding layer \n",
    "        # 2. we may transform the representations\n",
    "        # 3. and we should compute parameters for Bernoulli distributions\n",
    "        \n",
    "        emb_size = embed.weight.shape[1]\n",
    "        # Using twice the units just to make the output as large as that of a biLSTM\n",
    "        enc_size = hidden_size * 2  \n",
    "\n",
    "        self.embed_layer = nn.Sequential(embed)\n",
    "        self.enc_layer = get_encoder(layer, emb_size, hidden_size)\n",
    "        self.logit_layer = nn.Linear(enc_size, 1, bias=True)\n",
    "        \n",
    "        self.report_params()\n",
    "        \n",
    "        self.report_params()\n",
    "    \n",
    "    def report_params(self):\n",
    "        count = 0\n",
    "        for name, p in self.named_parameters():\n",
    "            if p.requires_grad and \"embed\" not in name:\n",
    "                count += np.prod(list(p.shape))\n",
    "        print(\"{} #params: {}\".format(self.__class__.__name__, count))\n",
    "\n",
    "    def forward(self, x, mask) -> Bernoulli:\n",
    "        \"\"\"\n",
    "        It takes a tensor of tokens (integers)\n",
    "         and predicts a Bernoulli distribution for each position.\n",
    "        \n",
    "        :param x: [B, T]\n",
    "        :param mask: [B, T]\n",
    "        :returns: Bernoulli\n",
    "        \"\"\"\n",
    "\n",
    "        # encode sentence\n",
    "        lengths = mask.long().sum(1)  # [B]\n",
    "        embeddings = self.embed_layer(x)  # [B, T, E]\n",
    "        h, _ = self.enc_layer(embeddings, mask, lengths)  # [B, T, d]\n",
    "\n",
    "        # compute parameters for Bernoulli p(z|x)\n",
    "        # Bernoulli distributions\n",
    "        logits = self.logit_layer(h).squeeze(-1)  # [B, T]\n",
    "\n",
    "        return Bernoulli(logits=logits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fcCu7vkKWvZX"
   },
   "source": [
    "## Parameter Estimation\n",
    "\n",
    "In variational inference, our objective is to maximise the *evidence lowerbound* (ELBO):\n",
    "\n",
    "\\begin{align}\n",
    "\\log P(y|x) &\\ge \\mathbb E_{Q(z|x, y, \\lambda)}\\left[ \\log P(y|x, z, \\theta, p_1) \\right] - \\text{KL}(Q(z|x, y, \\lambda) || P(z|p_1)) \\\\\n",
    "\\text{ELBO}&\\overset{\\text{MF}}{=}\\mathbb E_{Q(z|x, y, \\lambda)}\\left[ \\log P(y|x, z, \\theta, p_1) \\right] - \\sum_{i=1}^n \\text{KL}(Q(z_i|x, \\lambda) || P(z_i|p_1)) \n",
    "\\end{align}\n",
    "\n",
    "where the *mean field* assumption we made implies that the KL term is simply a sum of KL divergences from a Bernoulli posterior to a Bernoulli prior.\n",
    "\n",
    "Note that the ELBO remains intractable, namely, solving the expectation in closed form still requires $2^n$ evaluations of the classifier network. Though unlike the true posterior $P(z|x,y, \\lambda)$, the approximation $Q(z|x,\\lambda)$ is tractable (it does not require an intractable normalisation) and can be used to obtain gradient estimates based on samples.\n",
    "\n",
    "### Gradient of the classifier network\n",
    "\n",
    "For the classifier, we encounter no problem:\n",
    "\n",
    "\\begin{align}\n",
    "\\nabla_\\theta \\text{ELBO} &=\\nabla_\\theta\\sum_{z} Q(z|x, \\lambda)\\log P(y|x,z,\\theta) - \\underbrace{\\nabla_\\theta \\sum_{i=1}^n \\text{KL}(Q(z_i|x, \\lambda) || P(z_i|p_1))}_{\\color{blue}{0}}  \\\\\n",
    "&=\\sum_{z} Q(z|x, \\lambda)\\nabla_\\theta\\log P(y|x,z,\\theta) \\\\\n",
    "&= \\mathbb E_{Q(z|x, \\lambda)}\\left[\\nabla_\\theta\\log P(y|x,z,\\theta) \\right] \\\\\n",
    "&\\overset{\\text{MC}}{\\approx} \\frac{1}{S} \\sum_{s=1}^S \\nabla_\\theta \\log P(y|x, z^{(s)}, \\theta) \n",
    "\\end{align}\n",
    "where $z^{(s)} \\sim Q(z|x,\\lambda)$.\n",
    "\n",
    "\n",
    "### Gradient of the inference network\n",
    "\n",
    "For the inference model, we have to use the *score function estimator* (a.k.a. REINFORCE):\n",
    "\n",
    "\\begin{align}\n",
    "\\nabla_\\lambda \\text{ELBO} &=\\nabla_\\lambda\\sum_{z} Q(z|x, \\lambda)\\log P(y|x,z,\\theta) - \\nabla_\\lambda \\underbrace{\\sum_{i=1}^n \\text{KL}(Q(z_i|x, \\lambda) || P(z_i|p_1))}_{ \\color{blue}{\\text{tractable} }}  \\\\\n",
    "&=\\sum_{z} \\nabla_\\lambda Q(z|x, \\lambda)\\log P(y|x,z,\\theta) - \\sum_{i=1}^n \\nabla_\\lambda \\text{KL}(Q(z_i|x, \\lambda) || P(z_i|p_1))   \\\\\n",
    "&=\\sum_{z}  \\underbrace{Q(z|x, \\lambda) \\nabla_\\lambda \\log Q(z|x, \\lambda)}_{\\nabla_\\lambda Q(z|x, \\lambda)} \\log P(y|x,z,\\theta) - \\sum_{i=1}^n \\nabla_\\lambda \\text{KL}(Q(z_i|x, \\lambda) || P(z_i|p_1))   \\\\\n",
    "&= \\mathbb E_{Q(z|x, \\lambda)}\\left[ \\log P(y|x,z,\\theta) \\nabla_\\lambda \\log Q(z|x, \\lambda) \\right] - \\sum_{i=1}^n \\nabla_\\lambda \\text{KL}(Q(z_i|x, \\lambda) || P(z_i|p_1))   \\\\\n",
    "&\\overset{\\text{MC}}{\\approx} \\left(\\frac{1}{S} \\sum_{s=1}^S  \\log P(y|x, z^{(s)}, \\theta) \\nabla_\\lambda \\log Q(z^{(s)}|x, \\lambda)  \\right) - \\sum_{i=1}^n \\nabla_\\lambda \\text{KL}(Q(z_i|x, \\lambda) || P(z_i|p_1))  \n",
    "\\end{align}\n",
    "\n",
    "where $z^{(s)} \\sim Q(z|x,\\lambda)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6cdfkOYdC0LQ"
   },
   "source": [
    "## Implementation\n",
    "\n",
    "Let's implement the model and the loss (negative ELBO). We work with the notion of a *surrogate loss*, that is, a computation node whose gradients wrt to parameters are equivalent to the gradients we need.\n",
    "\n",
    "For a given sample $z \\sim Q(z|x, \\lambda)$, the following is a single-sample surrogate loss:\n",
    "\n",
    "\\begin{align}\n",
    "\\mathcal S(\\theta, \\lambda|x, y) = \\log P(y|x, z, \\theta) + \\color{red}{\\text{detach}(\\log P(y|x, z, \\theta) )}\\log Q(z|x, \\lambda) - \\sum_{i=1}^n \\text{KL}(Q(z_i|x, \\lambda) || P(z_i|\\phi))\n",
    "\\end{align}\n",
    "where we introduce an auxiliary function such that\n",
    "\\begin{align}\n",
    "\\text{detach}(f(\\alpha))  &= h(\\alpha) \\\\\n",
    "\\nabla_\\beta \\text{detach}(h(\\alpha))  &= 0 \n",
    "\\end{align}\n",
    "or in words, *detach* does not alter the forward call of its argument function $h$, but it alters $h$'s backward call by setting gradients to zero.\n",
    "\n",
    "Show that it's gradients wrt $\\theta$ and $\\lambda$ are exactly what we need:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FednEChaX6WI"
   },
   "source": [
    "\\begin{align}\n",
    "\\nabla_\\theta\\mathcal S(\\theta,\\lambda|x,y)=\\nabla_\\theta\\log P(y|x,z,\\theta)+0=\\nabla_\\theta\\text{ELBO}\n",
    "\\end{align}$$$$\\begin{align}\n",
    "\\nabla_\\lambda \\mathcal S(\\theta, \\lambda|x, y)= 0 + \\underbrace{\\log Q(z|x, \\lambda)\\nabla_\\lambda \\log P(y|x, z, \\theta)  + \\log P(y|x, z, \\theta) \\nabla_\\lambda \\log Q(z|x, \\lambda)}_{\\text{chain rule}}-\\sum_{i=1}^n \\nabla_\\lambda \\text{KL}(Q(z_i|x, \\lambda) || P(z_i|p_1))=\\\\ \n",
    "= 0+ 0 + \\log P(y|x, z, \\theta) \\nabla_\\lambda \\log Q(z|x, \\lambda)-\\sum_{i=1}^n \\nabla_\\lambda \\text{KL}(Q(z_i|x, \\lambda) || P(z_i|p_1))=\\nabla_\\lambda\\text{ELBO}\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OaUMKDShx9T0"
   },
   "source": [
    "Implement the forward pass and loss below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Cnwwk-7tfR02"
   },
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    \"\"\"\n",
    "    \n",
    "    Classifier model:\n",
    "        Z_i ~ Bern(p_1) for i in 1..n\n",
    "        Y|x,z ~ Cat(f([x_i if z_i 1 else 0 for i in 1..n ]))\n",
    "    \n",
    "    Inference model:\n",
    "        Z_i|x ~ Bern(b_i) for i in 1..n\n",
    "            where b_i = g_i(x)\n",
    "    \n",
    "    Objective:\n",
    "        Single-sample MC estimate of ELBO\n",
    "    \n",
    "    Loss: \n",
    "        Surrogate loss\n",
    "\n",
    "    Consists of:\n",
    "        - a product of Bernoulli distributions inference network\n",
    "        - a classifier network\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 vocab: object = None,\n",
    "                 vocab_size: int = 0,\n",
    "                 emb_size: int = 200,\n",
    "                 hidden_size: int = 200,\n",
    "                 num_classes: int = 5,\n",
    "                 prior_p1: float = 0.3,                 \n",
    "                 det_prior: bool = True,\n",
    "                 beta_shape: list = [0.6, 0.6],\n",
    "                 dropout: float = 0.1,\n",
    "                 layer_cls: str = 'bow',\n",
    "                 layer_inf: str = 'bow'):\n",
    "        \"\"\"\n",
    "        :param vocab: Vocabulary\n",
    "        :param vocab_size: necessary for embedding layer\n",
    "        :param emb_size: dimensionality of embedding layer\n",
    "        :param hidden_size: dimensionality of hidden layers\n",
    "        :param num_classes: number of classes\n",
    "        :param prior_p1: (scalar) prior Bernoulli parameter\n",
    "        :param det_prior: (boolean) whether the prior parameter is deterministic\n",
    "        :param beta_shape: (pair of positive scalars) \n",
    "            when the prior parameter is stochastic\n",
    "            it is sampled from a Beta distribution (ignore this at first)\n",
    "        :param dropout: (scalar) dropout rate\n",
    "        :param layer_cls: type of encoder for classification\n",
    "        :param layer_inf: type of encoder for inference\n",
    "        \"\"\"\n",
    "        super(Model, self).__init__()\n",
    "\n",
    "        self.vocab = vocab\n",
    "        self.embed = embed = nn.Embedding(vocab_size, emb_size, padding_idx=1)\n",
    "\n",
    "        self.cls_net = Classifier(\n",
    "            embed=embed, \n",
    "            hidden_size=hidden_size, \n",
    "            output_size=num_classes,\n",
    "            dropout=dropout, \n",
    "            layer=layer_cls\n",
    "        )\n",
    "        \n",
    "        self.inference_net = ProductOfBernoullis(\n",
    "            embed=embed, \n",
    "            hidden_size=hidden_size,\n",
    "            layer=layer_inf\n",
    "        )\n",
    "        \n",
    "        self._prior_p1 = prior_p1\n",
    "        self._det_prior = det_prior\n",
    "        self._beta_shape = beta_shape\n",
    "        \n",
    "    def get_prior_p1(self, p_min=0.001, p_max=0.999):\n",
    "        \"\"\"Return the prior Bernoulli parameter\"\"\"\n",
    "        if self._det_prior:\n",
    "            return torch.tensor(self._prior_p1)\n",
    "        else:\n",
    "            a, b = self._beta_shape\n",
    "            prior_p1 = np.random.beta(a, b)\n",
    "            prior_p1 = max(prior_p1, p_min)\n",
    "            prior_p1 = min(prior_p1, p_max)\n",
    "        return torch.tensor(prior_p1)\n",
    "\n",
    "    def predict(self, py: Categorical, **kwargs):\n",
    "        \"\"\"\n",
    "        Predict deterministically using argmax.\n",
    "        :param py: B Categorical distributions (one per instance in batch)\n",
    "        :return: predictions\n",
    "            [B] sentiment levels\n",
    "        \"\"\"\n",
    "        assert not self.training, \"should be in eval mode for prediction\"\n",
    "        return py.log_probs.argmax(-1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Generate a sequence z with inference model, \n",
    "         then predict with rationale xz, that is, x masked by z.\n",
    "\n",
    "        :param x: [B, T] documents        \n",
    "        :param mask: [B, T] indicates valid positions vs padded positions\n",
    "        :return: \n",
    "            Categorical distributions P(y|x, z)\n",
    "            Bernoulli distributions Q(z|x)\n",
    "            Single sample z ~ Q(z|x) used for the conditional P(y|x, z)\n",
    "        \"\"\"\n",
    "        bern = self.inference_net(x, x != 1)\n",
    "        z = bern.sample()\n",
    "        res = self.cls_net(x, x != 1, z)\n",
    "        return res, bern, z\n",
    "\n",
    "    def get_loss(self,                   \n",
    "                 y, \n",
    "                 py: Categorical,\n",
    "                 qz: Bernoulli, \n",
    "                 z, \n",
    "                 mask,\n",
    "                 iter_i=0, \n",
    "                 # you may ignore the rest of the arguments for the time being\n",
    "                 #  leave them as they are\n",
    "                 kl_weight=1.0,\n",
    "                 min_kl=0.0,\n",
    "                 ll_mean=0.,\n",
    "                 ll_std=1.,\n",
    "                 **kwargs):\n",
    "        \"\"\"\n",
    "        This computes the loss for the whole model.\n",
    "\n",
    "        :param y: target labels [B]\n",
    "        :param py: conditionals P(y|x, z)\n",
    "        :param qz: approximate posteriors Q(z|x)\n",
    "        :param z: sample of binary selectors [B, T]\n",
    "        :param mask: indicates valid positions [B, T]\n",
    "        :param iter_i: indicates the iteration\n",
    "        :param kl_weight: (scalar) multiplies the KL term\n",
    "        :param min_kl: (scalar) sets a minimum for the KL (aka free bits)\n",
    "        :param ll_mean: (scalar) running average of reward\n",
    "        :param ll_std: (scalar) running standard deviation of reward\n",
    "        :return: loss (torch node), terms (dict)\n",
    "        \n",
    "            terms is an OrderedDict that holds the scalar items involved in the loss\n",
    "            e.g. `terms['ll'] = ll.item()` is the log-likelihood term\n",
    "            \n",
    "            Consider tracking the following:\n",
    "            Single-sample ELBO: terms['elbo']\n",
    "            Log-Likelihood log P(y|x,z): terms['ll']\n",
    "            KL: terms['kl']\n",
    "            Score function surrogate log P(y|z, x) log Q(z|x): terms['sf']            \n",
    "            Rate of selected words: terms['selected']\n",
    "        \"\"\"\n",
    "        \n",
    "        ll = py.log_pmf(y).unsqueeze(-1)\n",
    "        kl = qz.kl(Bernoulli(probs=self.get_prior_p1()))\n",
    "        sf = qz.log_pmf(z.type(torch.FloatTensor)) * (ll.detach() - ll_mean) / ll_std\n",
    "        elba =  ll + sf - kl_weight * kl\n",
    "        \n",
    "        terms = OrderedDict()\n",
    "        terms['elbo'] = (ll - kl).sum().item()\n",
    "        terms['sf'] = sf.sum().item()\n",
    "        terms['selected'] = z.sum().item() * 1.0 / z.shape[0] / z.shape[1]\n",
    "        terms['kl'] = kl.sum().item()\n",
    "        terms['ll'] = ll=ll.sum().item()\n",
    "        \n",
    "        return -elba.sum(), terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wNQDXTpqWvZa"
   },
   "outputs": [],
   "source": [
    "# This will be used later for maintaining runnin averages of quantites like \n",
    "#  terms in the ELBO\n",
    "from collections import deque\n",
    "\n",
    "class MovingStats:\n",
    "    \n",
    "    def __init__(self, memory=-1):\n",
    "        self.data = deque([])\n",
    "        self.memory = memory\n",
    "        \n",
    "    def append(self, value):\n",
    "        if self.memory != 0:\n",
    "            if self.memory > 0 and len(self.data) == self.memory:\n",
    "                self.data.popleft()\n",
    "            self.data.append(value)\n",
    "        \n",
    "    def mean(self):\n",
    "        if len(self.data):\n",
    "            return np.mean([x for x in self.data])\n",
    "        else:\n",
    "            return 0.\n",
    "    \n",
    "    def std(self):\n",
    "        return np.std(self.data) if len(self.data) > 1 else 1.\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "081YSfU9WvZc"
   },
   "source": [
    "# Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9Pc80gseWvZd"
   },
   "outputs": [],
   "source": [
    "# some helper code for mini batching\n",
    "#  this will take care of annoying things such as \n",
    "#  sorting training instances by length (necessary for pytorch's LSTM, for example)\n",
    "from sst.util import make_kv_string, get_minibatch, prepare_minibatch, print_parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_WVr97kilIRV"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Configuration\n",
      "training_path        : ../sst/data/sst/train.txt\n",
      "dev_path             : ../sst/data/sst/dev.txt\n",
      "test_path            : ../sst/data/sst/test.txt\n",
      "word_vectors         : ../sst/data/sst/glove.840B.300d.filtered.txt\n",
      "prior_p1             :        0.3\n",
      "beta_a               :        0.6\n",
      "beta_b               :        0.6\n",
      "det_prior            :          1\n",
      "num_epochs           :         50\n",
      "print_every          :        100\n",
      "eval_every           :         -1\n",
      "batch_size           :         25\n",
      "eval_batch_size      :         25\n",
      "subphrases           :          0\n",
      "min_phrase_length    :          2\n",
      "lowercase            :          1\n",
      "fix_emb              :          1\n",
      "embed_size           :        300\n",
      "hidden_size          :        150\n",
      "num_layers           :          1\n",
      "dropout              :        0.5\n",
      "layer_inf            : bow       \n",
      "layer_cls            : bow       \n",
      "save_path            : data/results\n",
      "baseline_memory      :       1000\n",
      "min_kl               :        0.0\n",
      "kl_weight            :        0.0\n",
      "kl_inc               :      1e-05\n",
      "lr                   :     0.0002\n",
      "weight_decay         :      1e-05\n",
      "lr_decay             :        0.5\n",
      "patience             :          5\n",
      "cooldown             :          5\n",
      "threshold            :     0.0001\n",
      "min_lr               :      1e-05\n",
      "max_grad_norm        :        5.0\n",
      "Set eval_every to 341\n",
      "Loading data\n",
      "train 8544\n",
      "dev 1101\n",
      "test 2210\n",
      "\n",
      "# Example\n",
      "First dev example: Example(tokens=['it', \"'s\", 'a', 'lovely', 'film', 'with', 'lovely', 'performances', 'by', 'buy', 'and', 'accorsi', '.'], label=3, transitions=[0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1], token_labels=[2, 2, 2, 3, 2, 2, 3, 2, 2, 2, 2, 2, 2])\n",
      "First dev example tokens: ['it', \"'s\", 'a', 'lovely', 'film', 'with', 'lovely', 'performances', 'by', 'buy', 'and', 'accorsi', '.']\n",
      "First dev example label: 3\n"
     ]
    }
   ],
   "source": [
    "import torch.optim\n",
    "# We will use Adam\n",
    "from torch.optim import Adam\n",
    "# and a couple of tricks to reduce learning rate on plateau\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "# here is some helper code to evaluate your model\n",
    "from sst.evaluate import evaluate\n",
    "\n",
    "\n",
    "cfg = dict()\n",
    "\n",
    "# Data\n",
    "cfg['training_path'] = \"../sst/data/sst/train.txt\"\n",
    "cfg['dev_path'] = \"../sst/data/sst/dev.txt\"\n",
    "cfg['test_path'] = \"../sst/data/sst/test.txt\"\n",
    "cfg['word_vectors'] = '../sst/data/sst/glove.840B.300d.filtered.txt'\n",
    "# Model\n",
    "cfg['prior_p1'] = 0.3\n",
    "cfg['beta_a'] = 0.6\n",
    "cfg['beta_b'] = 0.6\n",
    "cfg['det_prior'] = True\n",
    "# Architecture\n",
    "cfg['num_epochs'] = 50\n",
    "cfg['print_every'] = 100\n",
    "cfg['eval_every'] = -1\n",
    "cfg['batch_size'] = 25\n",
    "cfg['eval_batch_size'] = 25\n",
    "cfg['subphrases'] = False\n",
    "cfg['min_phrase_length'] = 2\n",
    "cfg['lowercase'] = True\n",
    "cfg['fix_emb'] = True\n",
    "cfg['embed_size'] = 300\n",
    "cfg['hidden_size'] = 150\n",
    "cfg['num_layers'] = 1\n",
    "cfg['dropout'] = 0.5\n",
    "cfg['layer_inf'] = 'bow'\n",
    "cfg['layer_cls'] = 'bow'\n",
    "cfg['save_path'] = 'data/results'\n",
    "cfg['baseline_memory'] = 1000\n",
    "cfg['min_kl'] = 0.  # use more than 0 to enable free bits\n",
    "cfg['kl_weight'] = 0.0  # start from zero to enable annealing\n",
    "cfg['kl_inc'] = 0.00001  \n",
    "# Optimiser (leave as is)\n",
    "cfg['lr'] = 0.0002\n",
    "cfg['weight_decay'] = 1e-5\n",
    "cfg['lr_decay'] = 0.5\n",
    "cfg['patience'] = 5\n",
    "cfg['cooldown'] = 5\n",
    "cfg['threshold'] = 1e-4\n",
    "cfg['min_lr'] = 1e-5\n",
    "cfg['max_grad_norm'] = 5.\n",
    "\n",
    "\n",
    "print('# Configuration')\n",
    "for k, v in cfg.items():\n",
    "    print(\"{:20} : {:10}\".format(k, v))\n",
    "\n",
    "\n",
    "iters_per_epoch = len(train_data) // cfg[\"batch_size\"]\n",
    "\n",
    "if cfg[\"eval_every\"] == -1:\n",
    "    eval_every = iters_per_epoch\n",
    "    print(\"Set eval_every to {}\".format(iters_per_epoch))\n",
    "\n",
    "\n",
    "# Let's load the data into memory.\n",
    "print(\"Loading data\")\n",
    "train_data = list(examplereader(\n",
    "    cfg['training_path'],\n",
    "    lower=cfg['lowercase'], \n",
    "    subphrases=cfg['subphrases'],\n",
    "    min_length=cfg['min_phrase_length']))\n",
    "dev_data = list(examplereader(cfg['dev_path'], lower=cfg['lowercase']))\n",
    "test_data = list(examplereader(cfg['test_path'], lower=cfg['lowercase']))\n",
    "\n",
    "print(\"train\", len(train_data))\n",
    "print(\"dev\", len(dev_data))\n",
    "print(\"test\", len(test_data))\n",
    "\n",
    "print('\\n# Example')\n",
    "example = dev_data[0]\n",
    "print(\"First dev example:\", example)\n",
    "print(\"First dev example tokens:\", example.tokens)\n",
    "print(\"First dev example label:\", example.label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6PMqtVj0WvZf",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def train():\n",
    "\n",
    "    # Create a vocabulary object to map str <-> int\n",
    "    vocab = Vocabulary()  # populated by load_glove\n",
    "    glove_path = cfg[\"word_vectors\"]\n",
    "    vectors = load_glove(glove_path, vocab)\n",
    "\n",
    "    # You may consider using tensorboardX\n",
    "    # writer = SummaryWriter(log_dir=cfg[\"save_path\"])\n",
    "\n",
    "    # Map the sentiment labels 0-4 to a more readable form (and the opposite)\n",
    "    i2t = [\"very negative\", \"negative\", \"neutral\", \"positive\", \"very positive\"]\n",
    "    t2i = OrderedDict({p: i for p, i in zip(i2t, range(len(i2t)))})\n",
    "\n",
    "\n",
    "    print('\\n# Constructing model')\n",
    "    model = Model(\n",
    "        vocab_size=len(vocab.w2i), \n",
    "        emb_size=cfg[\"embed_size\"],\n",
    "        hidden_size=cfg[\"hidden_size\"], \n",
    "        num_classes=len(t2i),\n",
    "        prior_p1=cfg['prior_p1'],\n",
    "        det_prior=cfg['det_prior'],\n",
    "        beta_shape=[cfg['beta_a'], cfg['beta_b']],\n",
    "        vocab=vocab, \n",
    "        dropout=cfg[\"dropout\"], \n",
    "        layer_cls=cfg[\"layer_cls\"],\n",
    "        layer_inf=cfg[\"layer_inf\"]\n",
    "    )\n",
    "\n",
    "    print('\\n# Loading embeddings')\n",
    "    with torch.no_grad():\n",
    "        model.embed.weight.data.copy_(torch.from_numpy(vectors))\n",
    "        if cfg[\"fix_emb\"]:\n",
    "            print(\"fixed word embeddings\")\n",
    "            model.embed.weight.requires_grad = False\n",
    "        model.embed.weight[1] = 0.  # padding zero\n",
    "\n",
    "        \n",
    "    # Congigure optimiser\n",
    "    optimizer = Adam(model.parameters(), lr=cfg[\"lr\"],\n",
    "                     weight_decay=cfg[\"weight_decay\"])\n",
    "    # and learning rate scheduler\n",
    "    scheduler = ReduceLROnPlateau(\n",
    "        optimizer, mode=\"min\", factor=cfg[\"lr_decay\"], patience=cfg[\"patience\"],\n",
    "        verbose=True, cooldown=cfg[\"cooldown\"], threshold=cfg[\"threshold\"],\n",
    "        min_lr=cfg[\"min_lr\"])\n",
    "\n",
    "    # Prepare a few auxiliary variables\n",
    "    iter_i = 0\n",
    "    train_loss = 0.\n",
    "    print_num = 0\n",
    "    losses = []\n",
    "    accuracies = []\n",
    "    best_eval = 1.0e9\n",
    "    best_iter = 0\n",
    "\n",
    "    model = model.to(device)\n",
    "\n",
    "    # Some debugging info\n",
    "    print(model)\n",
    "    print_parameters(model)\n",
    "\n",
    "    batch_size = cfg['batch_size']\n",
    "    eval_batch_size = cfg['eval_batch_size']\n",
    "    print_every = cfg['print_every']\n",
    "\n",
    "    # Parameters of tricks to better optimise the ELBO \n",
    "    kl_inc = cfg['kl_inc']\n",
    "    kl_weight = cfg['kl_weight']\n",
    "    min_kl = cfg['min_kl']\n",
    "    # Running estimates for baselines\n",
    "    ll_moving_stats = MovingStats(cfg['baseline_memory'])\n",
    "\n",
    "    while True:  # when we run out of examples, shuffle and continue\n",
    "        epoch = iter_i // iters_per_epoch\n",
    "        if epoch > cfg['num_epochs']:\n",
    "            break\n",
    "        \n",
    "        for batch in get_minibatch(train_data, batch_size=batch_size, shuffle=True):\n",
    "\n",
    "            epoch = iter_i // iters_per_epoch\n",
    "            if epoch > cfg['num_epochs']:\n",
    "                break\n",
    "\n",
    "            # forward pass\n",
    "            model.train()\n",
    "            x, y, _ = prepare_minibatch(batch, model.vocab, device=device)\n",
    "            \n",
    "            mask = (x != 1)\n",
    "            py, qz, z = model(x)\n",
    "\n",
    "            # \"KL annealing\"\n",
    "            kl_weight += kl_inc\n",
    "            if kl_weight > 1.:\n",
    "                kl_weight = 1.0\n",
    "                \n",
    "            loss, terms = model.get_loss(\n",
    "                y,\n",
    "                py=py, \n",
    "                qz=qz,\n",
    "                z=z,\n",
    "                mask=mask, \n",
    "                kl_weight=kl_weight,\n",
    "                min_kl=min_kl,\n",
    "                ll_mean=ll_moving_stats.mean(),\n",
    "                ll_std=ll_moving_stats.std(),\n",
    "                iter_i=iter_i)\n",
    "\n",
    "            train_loss += loss.item()\n",
    "            \n",
    "            # keep an running estimate of the reward (log P(y|x,z))\n",
    "            ll_moving_stats.append(terms['ll'])\n",
    "\n",
    "            # backward pass\n",
    "            model.zero_grad()  # erase previous gradients\n",
    "\n",
    "            loss.backward()  # compute new gradients\n",
    "\n",
    "            # gradient clipping generally helps\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=cfg['max_grad_norm'])\n",
    "\n",
    "            # update weights\n",
    "            optimizer.step()\n",
    "\n",
    "            print_num += 1\n",
    "            iter_i += 1\n",
    "\n",
    "            # print info\n",
    "            if iter_i % print_every == 0:\n",
    "\n",
    "                train_loss = train_loss / print_every\n",
    "\n",
    "                print_str = make_kv_string(terms)\n",
    "                print(\"Epoch %r Iter %r loss=%.4f %s\" %\n",
    "                      (epoch, iter_i, train_loss, print_str))\n",
    "                losses.append(train_loss)\n",
    "                print_num = 0\n",
    "                train_loss = 0.\n",
    "\n",
    "            # evaluate\n",
    "            if iter_i % eval_every == 0:\n",
    "\n",
    "                dev_eval, rationales = evaluate(\n",
    "                    model, dev_data, \n",
    "                    batch_size=eval_batch_size, \n",
    "                    device=device,\n",
    "                    cfg=cfg, iter_i=iter_i\n",
    "                )\n",
    "                accuracies.append(dev_eval[\"acc\"])\n",
    "\n",
    "                print(\"\\n# epoch %r iter %r: dev %s\" % (\n",
    "                    epoch, iter_i, make_kv_string(dev_eval)))\n",
    "                \n",
    "                for exid in range(3):\n",
    "                    print(' dev%d [gold=%d,pred=%d]:' % (exid, dev_data[exid].label, rationales[exid][1]),  \n",
    "                          ' '.join(rationales[exid][0]))\n",
    "                print()\n",
    "\n",
    "                # adjust learning rate\n",
    "                scheduler.step(dev_eval[\"loss\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "A5uYKcw-WvZl",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "# Constructing model\n",
      "Classifier #params: 1505\n",
      "ProductOfBernoullis #params: 301\n",
      "ProductOfBernoullis #params: 301\n",
      "\n",
      "# Loading embeddings\n",
      "fixed word embeddings\n",
      "Model(\n",
      "  (embed): Embedding(20727, 300, padding_idx=1)\n",
      "  (cls_net): Classifier(\n",
      "    (embed_layer): Sequential(\n",
      "      (0): Embedding(20727, 300, padding_idx=1)\n",
      "    )\n",
      "    (enc_layer): BagOfWordsEncoder()\n",
      "    (output_layer): Sequential(\n",
      "      (0): Dropout(p=0.5)\n",
      "      (1): Linear(in_features=300, out_features=5, bias=True)\n",
      "      (2): LogSoftmax()\n",
      "    )\n",
      "  )\n",
      "  (inference_net): ProductOfBernoullis(\n",
      "    (embed_layer): Sequential(\n",
      "      (0): Embedding(20727, 300, padding_idx=1)\n",
      "    )\n",
      "    (enc_layer): BagOfWordsEncoder()\n",
      "    (logit_layer): Linear(in_features=300, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "embed.weight             [20727, 300] requires_grad=False\n",
      "cls_net.output_layer.1.weight [5, 300]     requires_grad=True\n",
      "cls_net.output_layer.1.bias [5]          requires_grad=True\n",
      "inference_net.logit_layer.weight [1, 300]     requires_grad=True\n",
      "inference_net.logit_layer.bias [1]          requires_grad=True\n",
      "\n",
      "Total parameters: 6219906\n",
      "\n",
      "Shuffling training data\n",
      "Epoch 0 Iter 100 loss=1774003.7383 elbo -36053.2734 sf -1298917.0000 selected 0.4729 kl 77.3730 ll -40.1399\n",
      "Epoch 0 Iter 200 loss=1346431.3594 elbo -42956.1797 sf -1149918.0000 selected 0.5040 kl 93.2022 ll -40.6262\n",
      "Epoch 0 Iter 300 loss=1022197.7831 elbo -43686.7656 sf -944562.6250 selected 0.5200 kl 97.5365 ll -40.2423\n",
      "\n",
      "# epoch 0 iter 341: dev loss 13577.8375 elbo -39139.7756 sf 25561.9285 selected 0.5041 kl 87.7933 ll -39.1692 acc 0.2779\n",
      " dev0 [gold=3,pred=3]: it **'s** a **lovely** film with **lovely** **performances** **by** buy **and** accorsi **.**\n",
      " dev1 [gold=2,pred=3]: no one goes unindicted **here** **,** which **is** probably for **the** best **.**\n",
      " dev2 [gold=3,pred=3]: and if **you** **'re** **not** nearly **moved** to tears **by** a couple **of** **scenes** **,** you 've **got** ice water **in** **your** **veins** .\n",
      "\n",
      "Shuffling training data\n",
      "Epoch 1 Iter 400 loss=793571.4456 elbo -34863.0000 sf -580969.2500 selected 0.5165 kl 77.1066 ll -38.7475\n",
      "Epoch 1 Iter 500 loss=686402.6725 elbo -45962.0391 sf -732177.2500 selected 0.5164 kl 103.7603 ll -38.5494\n",
      "Epoch 1 Iter 600 loss=647415.2675 elbo -43895.9805 sf -669727.3750 selected 0.5116 kl 98.7648 ll -38.5366\n",
      "\n",
      "# epoch 1 iter 682: dev loss 13458.5213 elbo -38668.5242 sf 25210.0053 selected 0.5033 kl 87.1289 ll -38.6905 acc 0.3197\n",
      " dev0 [gold=3,pred=3]: it 's a **lovely** film **with** lovely **performances** **by** **buy** and accorsi **.**\n",
      " dev1 [gold=2,pred=3]: **no** **one** **goes** **unindicted** here , **which** **is** probably **for** **the** best **.**\n",
      " dev2 [gold=3,pred=3]: **and** **if** you 're not **nearly** moved **to** **tears** by **a** couple of scenes **,** **you** **'ve** got ice water **in** your veins .\n",
      "\n",
      "Shuffling training data\n",
      "Epoch 2 Iter 700 loss=622898.9441 elbo -33320.2461 sf -453803.9062 selected 0.4900 kl 73.8427 ll -39.3428\n",
      "Epoch 2 Iter 800 loss=596569.9453 elbo -44205.0469 sf -630754.4375 selected 0.4942 kl 102.3349 ll -37.0192\n",
      "Epoch 2 Iter 900 loss=578249.2366 elbo -36211.7656 sf -493446.1562 selected 0.5067 kl 84.2938 ll -37.8938\n",
      "Epoch 2 Iter 1000 loss=559469.7462 elbo -32850.7227 sf -425363.3750 selected 0.4775 kl 72.4461 ll -38.7994\n",
      "\n",
      "# epoch 2 iter 1023: dev loss 13398.2752 elbo -38431.2787 sf 25032.9998 selected 0.5057 kl 88.2885 ll -38.4013 acc 0.3252\n",
      " dev0 [gold=3,pred=3]: it **'s** a lovely **film** with **lovely** performances by **buy** and accorsi .\n",
      " dev1 [gold=2,pred=1]: no one goes **unindicted** here **,** **which** **is** **probably** **for** **the** best **.**\n",
      " dev2 [gold=3,pred=1]: **and** **if** you 're not nearly **moved** **to** tears by **a** **couple** of **scenes** **,** you 've **got** **ice** water in your veins **.**\n",
      "\n",
      "Shuffling training data\n",
      "Epoch 3 Iter 1100 loss=545776.9506 elbo -39017.9688 sf -468581.1875 selected 0.5168 kl 88.6856 ll -39.7847\n",
      "Epoch 3 Iter 1200 loss=532194.4087 elbo -41724.7109 sf -498742.5000 selected 0.5054 kl 95.9387 ll -38.3670\n",
      "Epoch 3 Iter 1300 loss=502415.6709 elbo -35643.2812 sf -449288.0625 selected 0.4886 kl 79.5838 ll -36.3824\n",
      "\n",
      "# epoch 3 iter 1364: dev loss 13155.9309 elbo -38029.8975 sf 24873.9684 selected 0.4966 kl 80.9559 ll -38.1834 acc 0.3324\n",
      " dev0 [gold=3,pred=1]: **it** **'s** a **lovely** **film** with lovely performances by **buy** **and** accorsi **.**\n",
      " dev1 [gold=2,pred=3]: no one **goes** **unindicted** **here** , **which** is **probably** for **the** **best** **.**\n",
      " dev2 [gold=3,pred=3]: **and** **if** **you** 're **not** nearly **moved** **to** tears by **a** couple **of** **scenes** **,** you 've got **ice** water in your veins **.**\n",
      "\n",
      "Shuffling training data\n",
      "Epoch 4 Iter 1400 loss=502940.3678 elbo -41489.1445 sf -457112.6562 selected 0.4738 kl 84.1959 ll -40.3941\n",
      "Epoch 4 Iter 1500 loss=493915.2022 elbo -35442.5039 sf -413039.0312 selected 0.5278 kl 78.5361 ll -37.1991\n",
      "Epoch 4 Iter 1600 loss=475430.9872 elbo -39656.7188 sf -443240.2812 selected 0.4933 kl 84.0265 ll -38.5190\n",
      "Epoch 4 Iter 1700 loss=476189.6616 elbo -38178.1797 sf -442137.4375 selected 0.4964 kl 81.9466 ll -37.0560\n",
      "\n",
      "# epoch 4 iter 1705: dev loss 13076.9699 elbo -37918.0325 sf 24841.0614 selected 0.4908 kl 79.0405 ll -38.1223 acc 0.3324\n",
      " dev0 [gold=3,pred=1]: it **'s** **a** **lovely** film with lovely performances **by** **buy** and accorsi .\n",
      " dev1 [gold=2,pred=3]: no **one** goes **unindicted** here **,** **which** **is** probably for **the** **best** **.**\n",
      " dev2 [gold=3,pred=3]: and if **you** **'re** not nearly **moved** to tears **by** a **couple** **of** **scenes** **,** you 've **got** **ice** **water** in **your** **veins** .\n",
      "\n",
      "Shuffling training data\n",
      "Epoch 5 Iter 1800 loss=466226.6944 elbo -33987.3828 sf -401104.0000 selected 0.4789 kl 73.4449 ll -35.7237\n",
      "Epoch 5 Iter 1900 loss=478040.8809 elbo -38144.5781 sf -400838.5000 selected 0.4951 kl 77.3173 ll -39.1478\n",
      "Epoch 5 Iter 2000 loss=459751.5916 elbo -38426.3125 sf -434469.5625 selected 0.4690 kl 82.4731 ll -36.3645\n",
      "\n",
      "# epoch 5 iter 2046: dev loss 12994.8818 elbo -37675.8358 sf 24680.9634 selected 0.4882 kl 75.7555 ll -37.9210 acc 0.3406\n",
      " dev0 [gold=3,pred=1]: **it** 's a **lovely** **film** **with** lovely **performances** **by** buy and accorsi **.**\n",
      " dev1 [gold=2,pred=3]: no one goes unindicted here **,** which **is** **probably** **for** the **best** .\n",
      " dev2 [gold=3,pred=3]: and if **you** 're **not** nearly **moved** to tears **by** **a** couple **of** scenes **,** **you** **'ve** got ice water in your **veins** .\n",
      "\n",
      "Shuffling training data\n",
      "Epoch 6 Iter 2100 loss=452944.9844 elbo -33006.0078 sf -362090.3438 selected 0.5024 kl 67.3017 ll -36.8511\n",
      "Epoch 6 Iter 2200 loss=450431.8787 elbo -44814.0664 sf -460636.9375 selected 0.5088 kl 87.4974 ll -39.6526\n",
      "Epoch 6 Iter 2300 loss=449474.2547 elbo -44833.2812 sf -493471.8125 selected 0.4826 kl 93.7407 ll -36.9475\n",
      "\n",
      "# epoch 6 iter 2387: dev loss 13002.6403 elbo -37562.8115 sf 24560.1702 selected 0.4830 kl 75.4309 ll -37.8284 acc 0.3433\n",
      " dev0 [gold=3,pred=1]: **it** **'s** **a** **lovely** film with **lovely** performances by buy **and** accorsi .\n",
      " dev1 [gold=2,pred=3]: **no** **one** **goes** unindicted **here** **,** which is **probably** **for** the **best** **.**\n",
      " dev2 [gold=3,pred=3]: **and** if **you** **'re** not nearly moved to **tears** **by** a couple of **scenes** **,** **you** 've **got** **ice** water **in** **your** **veins** .\n",
      "\n",
      "Shuffling training data\n",
      "Epoch 7 Iter 2400 loss=450643.8303 elbo -30755.7637 sf -321815.7500 selected 0.4594 kl 61.5428 ll -37.6996\n",
      "Epoch 7 Iter 2500 loss=450505.2941 elbo -35178.3477 sf -356968.0938 selected 0.5024 kl 68.5949 ll -39.3689\n",
      "Epoch 7 Iter 2600 loss=443446.4400 elbo -35410.0156 sf -362467.2188 selected 0.5109 kl 74.0354 ll -38.3533\n",
      "Epoch 7 Iter 2700 loss=436943.5037 elbo -32132.2773 sf -331657.5000 selected 0.4825 kl 66.8187 ll -38.0772\n",
      "\n",
      "# epoch 7 iter 2728: dev loss 13042.1190 elbo -37467.8289 sf 24425.7082 selected 0.4949 kl 79.7986 ll -37.6265 acc 0.3351\n",
      " dev0 [gold=3,pred=3]: **it** **'s** **a** **lovely** **film** with lovely performances **by** buy **and** **accorsi** **.**\n",
      " dev1 [gold=2,pred=3]: no one **goes** unindicted **here** , **which** is **probably** for the **best** **.**\n",
      " dev2 [gold=3,pred=1]: and if **you** **'re** not **nearly** moved **to** tears **by** **a** couple **of** scenes **,** you **'ve** **got** ice water in **your** **veins** .\n",
      "\n",
      "Shuffling training data\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8 Iter 2800 loss=447523.2259 elbo -37937.5547 sf -412331.5938 selected 0.5010 kl 82.3103 ll -35.8798\n",
      "Epoch 8 Iter 2900 loss=447091.3397 elbo -40252.3242 sf -433915.5625 selected 0.5019 kl 88.0343 ll -36.2394\n",
      "Epoch 8 Iter 3000 loss=435886.8762 elbo -28181.7910 sf -310263.4688 selected 0.5080 kl 63.3258 ll -35.4649\n",
      "\n",
      "# epoch 8 iter 3069: dev loss 12993.5892 elbo -37392.5725 sf 24398.9733 selected 0.4925 kl 79.5692 ll -37.5388 acc 0.3388\n",
      " dev0 [gold=3,pred=1]: it 's **a** **lovely** film with **lovely** performances by **buy** and accorsi .\n",
      " dev1 [gold=2,pred=3]: **no** **one** **goes** **unindicted** **here** , which **is** **probably** for the best **.**\n",
      " dev2 [gold=3,pred=1]: **and** **if** you 're not **nearly** moved **to** **tears** by **a** **couple** of **scenes** **,** you 've **got** ice water in your **veins** **.**\n",
      "\n",
      "Shuffling training data\n",
      "Epoch 9 Iter 3100 loss=419992.1156 elbo -42497.2695 sf -462092.7188 selected 0.4969 kl 97.3479 ll -35.6121\n",
      "Epoch 9 Iter 3200 loss=434158.3531 elbo -46545.7383 sf -443219.0000 selected 0.5182 kl 95.6391 ll -40.1408\n",
      "Epoch 9 Iter 3300 loss=434116.4503 elbo -36502.0898 sf -367565.5625 selected 0.4962 kl 78.6127 ll -37.3370\n",
      "Epoch 9 Iter 3400 loss=425233.2166 elbo -46262.1680 sf -480209.5000 selected 0.5008 kl 98.3544 ll -36.5028\n",
      "\n",
      "# epoch 9 iter 3410: dev loss 13034.5132 elbo -37487.0698 sf 24452.5590 selected 0.4888 kl 77.8809 ll -37.6752 acc 0.3397\n",
      " dev0 [gold=3,pred=1]: it **'s** **a** **lovely** **film** with **lovely** performances by **buy** **and** **accorsi** **.**\n",
      " dev1 [gold=2,pred=1]: no **one** **goes** unindicted **here** **,** which **is** probably **for** the **best** **.**\n",
      " dev2 [gold=3,pred=1]: **and** **if** **you** **'re** not nearly **moved** **to** tears **by** a couple **of** scenes **,** you **'ve** got ice **water** in **your** veins **.**\n",
      "\n",
      "Shuffling training data\n",
      "Epoch 10 Iter 3500 loss=416988.1309 elbo -51539.7500 sf -497923.1875 selected 0.4831 kl 108.5501 ll -38.2949\n",
      "Epoch 10 Iter 3600 loss=428398.0981 elbo -35778.1406 sf -350470.7500 selected 0.4756 kl 76.3456 ll -37.6328\n",
      "Epoch 10 Iter 3700 loss=416924.3541 elbo -36081.4570 sf -340746.6562 selected 0.5029 kl 71.8100 ll -39.1842\n",
      "\n",
      "# epoch 10 iter 3751: dev loss 12942.4970 elbo -37210.6389 sf 24268.1444 selected 0.4902 kl 78.8631 ll -37.3396 acc 0.3515\n",
      " dev0 [gold=3,pred=1]: it 's a **lovely** **film** with **lovely** performances **by** buy and **accorsi** .\n",
      " dev1 [gold=2,pred=1]: no **one** goes unindicted here **,** which is probably **for** the best **.**\n",
      " dev2 [gold=3,pred=1]: and if you 're not **nearly** **moved** to tears by **a** **couple** of **scenes** **,** you 've got **ice** water **in** **your** veins **.**\n",
      "\n",
      "Shuffling training data\n",
      "Epoch 11 Iter 3800 loss=415020.2692 elbo -46054.0586 sf -461543.2812 selected 0.4767 kl 101.2358 ll -36.2693\n",
      "Epoch 11 Iter 3900 loss=405155.1691 elbo -40270.8828 sf -368025.9688 selected 0.5149 kl 80.9935 ll -39.2266\n",
      "Epoch 11 Iter 4000 loss=395858.0981 elbo -37310.3281 sf -362450.8750 selected 0.4626 kl 83.8829 ll -36.1161\n",
      "\n",
      "# epoch 11 iter 4092: dev loss 12906.5341 elbo -37029.6371 sf 24123.1275 selected 0.4912 kl 79.0036 ll -37.1615 acc 0.3460\n",
      " dev0 [gold=3,pred=1]: **it** 's a **lovely** **film** with **lovely** performances by **buy** and **accorsi** .\n",
      " dev1 [gold=2,pred=3]: no **one** **goes** **unindicted** **here** **,** **which** is probably for the **best** **.**\n",
      " dev2 [gold=3,pred=1]: and if you **'re** **not** **nearly** moved to **tears** by **a** couple **of** scenes **,** you 've got **ice** **water** **in** **your** **veins** **.**\n",
      "\n",
      "Epoch 12 Iter 4100 loss=397631.1731 elbo -42927.0859 sf -397666.9688 selected 0.4735 kl 89.8662 ll -37.8422\n",
      "Shuffling training data\n",
      "Epoch 12 Iter 4200 loss=390927.4059 elbo -44799.1523 sf -425737.0000 selected 0.5089 kl 95.5228 ll -36.0946\n",
      "Epoch 12 Iter 4300 loss=395948.3761 elbo -37035.0742 sf -330962.1875 selected 0.4865 kl 73.6781 ll -38.0466\n",
      "Epoch 12 Iter 4400 loss=389182.9028 elbo -40256.5586 sf -389071.7812 selected 0.4837 kl 87.7956 ll -35.4062\n",
      "\n",
      "# epoch 12 iter 4433: dev loss 12870.4719 elbo -36983.6305 sf 24113.1504 selected 0.4825 kl 76.1405 ll -37.1814 acc 0.3433\n",
      " dev0 [gold=3,pred=1]: **it** 's a **lovely** **film** **with** lovely **performances** **by** **buy** **and** accorsi **.**\n",
      " dev1 [gold=2,pred=1]: no **one** goes unindicted **here** **,** which **is** probably **for** **the** **best** **.**\n",
      " dev2 [gold=3,pred=1]: **and** if you 're not **nearly** **moved** to **tears** by a couple **of** **scenes** **,** **you** **'ve** got ice **water** in your veins **.**\n",
      "\n",
      "Shuffling training data\n",
      "Epoch 13 Iter 4500 loss=381062.8255 elbo -34849.0781 sf -299438.5000 selected 0.4729 kl 62.5294 ll -39.1598\n",
      "Epoch 13 Iter 4600 loss=384459.1381 elbo -38476.7500 sf -349042.8438 selected 0.4646 kl 78.4605 ll -37.4515\n",
      "Epoch 13 Iter 4700 loss=383737.1931 elbo -38942.2617 sf -369698.0000 selected 0.4907 kl 79.6834 ll -36.0489\n",
      "\n",
      "# epoch 13 iter 4774: dev loss 12821.1361 elbo -36766.2036 sf 23945.0648 selected 0.4848 kl 75.1848 ll -36.9899 acc 0.3642\n",
      " dev0 [gold=3,pred=1]: it 's **a** **lovely** film with lovely **performances** by **buy** **and** accorsi .\n",
      " dev1 [gold=2,pred=1]: **no** one goes **unindicted** **here** **,** which **is** probably **for** the **best** **.**\n",
      " dev2 [gold=3,pred=1]: **and** if you 're **not** **nearly** moved **to** tears **by** a couple of scenes **,** you **'ve** got **ice** water **in** your veins **.**\n",
      "\n",
      "Shuffling training data\n",
      "Epoch 14 Iter 4800 loss=388278.0097 elbo -44787.8047 sf -411765.0000 selected 0.4896 kl 93.0358 ll -36.9233\n",
      "Epoch 14 Iter 4900 loss=379153.6191 elbo -41313.5547 sf -378280.7500 selected 0.4943 kl 81.5748 ll -37.4040\n",
      "Epoch 14 Iter 5000 loss=401063.2678 elbo -34018.0820 sf -325619.0312 selected 0.4823 kl 67.9222 ll -36.9371\n",
      "Epoch 14 Iter 5100 loss=397183.5237 elbo -33252.7266 sf -313938.3125 selected 0.5047 kl 62.9371 ll -37.2698\n",
      "\n",
      "# epoch 14 iter 5115: dev loss 12819.2049 elbo -36801.0547 sf 23981.8528 selected 0.4761 kl 69.8620 ll -37.1762 acc 0.3451\n",
      " dev0 [gold=3,pred=1]: it 's **a** **lovely** film **with** lovely **performances** by buy and **accorsi** **.**\n",
      " dev1 [gold=2,pred=3]: **no** one **goes** unindicted **here** **,** **which** **is** probably **for** the best .\n",
      " dev2 [gold=3,pred=3]: and **if** you **'re** not nearly moved **to** tears **by** a **couple** of **scenes** **,** you 've got **ice** **water** in your veins .\n",
      "\n",
      "Shuffling training data\n",
      "Epoch 15 Iter 5200 loss=406145.9034 elbo -29275.7617 sf -287994.8438 selected 0.4555 kl 56.5511 ll -35.9510\n",
      "Epoch 15 Iter 5300 loss=397731.4706 elbo -31352.7598 sf -301397.5625 selected 0.4788 kl 57.6959 ll -37.3879\n",
      "Epoch 15 Iter 5400 loss=403609.9984 elbo -36409.9883 sf -354185.8438 selected 0.4726 kl 67.4225 ll -36.5520\n",
      "\n",
      "# epoch 15 iter 5456: dev loss 12746.5035 elbo -36655.9326 sf 23909.4324 selected 0.4704 kl 67.3307 ll -37.0579 acc 0.3497\n",
      " dev0 [gold=3,pred=1]: it 's a **lovely** **film** with lovely **performances** by buy and **accorsi** **.**\n",
      " dev1 [gold=2,pred=3]: **no** **one** goes unindicted here , which is **probably** for the **best** .\n",
      " dev2 [gold=3,pred=1]: and **if** you **'re** **not** nearly **moved** to tears **by** a **couple** **of** scenes **,** **you** 've **got** **ice** water in your veins **.**\n",
      "\n",
      "Shuffling training data\n",
      "Epoch 16 Iter 5500 loss=397765.3095 elbo -39816.0742 sf -378013.7500 selected 0.4429 kl 76.7402 ll -36.9732\n",
      "Epoch 16 Iter 5600 loss=399242.0153 elbo -31364.6602 sf -285790.4688 selected 0.4658 kl 52.5394 ll -38.7757\n",
      "Epoch 16 Iter 5700 loss=389957.2725 elbo -45435.8594 sf -420870.2812 selected 0.4687 kl 83.6203 ll -37.6916\n",
      "\n",
      "# epoch 16 iter 5797: dev loss 12832.1589 elbo -36608.2101 sf 23776.0590 selected 0.4682 kl 69.7918 ll -36.9396 acc 0.3551\n",
      " dev0 [gold=3,pred=3]: it 's a **lovely** **film** **with** **lovely** **performances** by buy **and** **accorsi** **.**\n",
      " dev1 [gold=2,pred=3]: **no** one goes unindicted **here** **,** **which** **is** probably for the **best** **.**\n",
      " dev2 [gold=3,pred=1]: **and** if you 're not **nearly** moved **to** tears by a couple of scenes **,** **you** **'ve** **got** ice water **in** your veins **.**\n",
      "\n",
      "Epoch 17 Iter 5800 loss=393023.6342 elbo -44550.9961 sf -427717.5312 selected 0.4530 kl 85.2510 ll -36.8867\n",
      "Shuffling training data\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17 Iter 5900 loss=390506.6202 elbo -41875.1914 sf -399914.5625 selected 0.4709 kl 88.9681 ll -36.0464\n",
      "Epoch 17 Iter 6000 loss=392234.3042 elbo -29362.4199 sf -271008.9688 selected 0.4827 kl 57.4631 ll -37.2343\n",
      "Epoch 17 Iter 6100 loss=382492.2767 elbo -42301.7383 sf -377982.6875 selected 0.4800 kl 85.5820 ll -37.3602\n",
      "\n",
      "# epoch 17 iter 6138: dev loss 12761.9700 elbo -36486.8433 sf 23724.8663 selected 0.4777 kl 71.8131 ll -36.7689 acc 0.3533\n",
      " dev0 [gold=3,pred=1]: **it** 's **a** **lovely** **film** with **lovely** performances **by** buy **and** **accorsi** **.**\n",
      " dev1 [gold=2,pred=3]: no one goes unindicted **here** , **which** is probably for the **best** **.**\n",
      " dev2 [gold=3,pred=1]: **and** **if** **you** **'re** **not** nearly moved to tears by **a** couple **of** scenes , you 've **got** ice **water** in your veins **.**\n",
      "\n",
      "Shuffling training data\n",
      "Epoch 18 Iter 6200 loss=380050.2177 elbo -42703.4141 sf -429156.6562 selected 0.5004 kl 96.3800 ll -32.8930\n",
      "Epoch 18 Iter 6300 loss=377558.8237 elbo -45519.6484 sf -384175.8125 selected 0.4898 kl 87.6947 ll -38.5132\n",
      "Epoch 18 Iter 6400 loss=364088.1322 elbo -47086.9453 sf -388792.0625 selected 0.4930 kl 88.7816 ll -39.0152\n",
      "\n",
      "# epoch 18 iter 6479: dev loss 12767.6606 elbo -36575.9985 sf 23808.3299 selected 0.4723 kl 71.9732 ll -36.8642 acc 0.3424\n",
      " dev0 [gold=3,pred=1]: it 's **a** **lovely** film with **lovely** performances **by** **buy** and accorsi .\n",
      " dev1 [gold=2,pred=3]: no one **goes** **unindicted** **here** , **which** is probably for **the** **best** .\n",
      " dev2 [gold=3,pred=1]: and **if** you **'re** **not** **nearly** moved to tears **by** **a** couple of scenes **,** **you** **'ve** got **ice** water in **your** veins .\n",
      "\n",
      "Shuffling training data\n",
      "Epoch 19 Iter 6500 loss=357814.9855 elbo -44741.9727 sf -377795.0938 selected 0.4693 kl 90.5398 ll -37.7586\n",
      "Epoch 19 Iter 6600 loss=362808.7000 elbo -32774.4883 sf -295433.4375 selected 0.4789 kl 65.9962 ll -35.5709\n",
      "Epoch 19 Iter 6700 loss=363617.7323 elbo -37309.8047 sf -295866.5625 selected 0.4656 kl 65.5688 ll -39.6340\n",
      "Epoch 19 Iter 6800 loss=365766.5097 elbo -32514.8555 sf -276372.7500 selected 0.4691 kl 58.0278 ll -37.6536\n",
      "\n",
      "# epoch 19 iter 6820: dev loss 12752.1274 elbo -36448.1205 sf 23695.9990 selected 0.4681 kl 65.6815 ll -36.8778 acc 0.3515\n",
      " dev0 [gold=3,pred=1]: it **'s** **a** lovely **film** with **lovely** **performances** by buy and **accorsi** **.**\n",
      " dev1 [gold=2,pred=1]: no **one** **goes** unindicted here **,** which **is** probably for **the** best .\n",
      " dev2 [gold=3,pred=1]: **and** if you **'re** not nearly **moved** to tears **by** a **couple** of scenes , **you** 've got ice water in your **veins** **.**\n",
      "\n",
      "Shuffling training data\n",
      "Epoch 20 Iter 6900 loss=363047.4914 elbo -30667.0332 sf -262908.1562 selected 0.4787 kl 55.2554 ll -37.7879\n",
      "Epoch 20 Iter 7000 loss=365143.6344 elbo -31768.6172 sf -264525.5938 selected 0.4400 kl 53.7886 ll -38.0298\n",
      "Epoch 20 Iter 7100 loss=365104.7273 elbo -34488.7695 sf -293658.5625 selected 0.4697 kl 58.0447 ll -37.7573\n",
      "\n",
      "# epoch 20 iter 7161: dev loss 12773.4460 elbo -36463.2967 sf 23689.8503 selected 0.4634 kl 65.3203 ll -36.9163 acc 0.3497\n",
      " dev0 [gold=3,pred=1]: it 's a **lovely** **film** with lovely performances by buy and **accorsi** **.**\n",
      " dev1 [gold=2,pred=1]: no one goes unindicted here **,** which is probably for the **best** **.**\n",
      " dev2 [gold=3,pred=1]: and if you **'re** not nearly **moved** **to** **tears** **by** **a** couple **of** scenes **,** you 've **got** ice **water** **in** your **veins** **.**\n",
      "\n",
      "Shuffling training data\n",
      "Epoch 21 Iter 7200 loss=359966.4813 elbo -43554.5508 sf -370640.8750 selected 0.4855 kl 77.9677 ll -37.8230\n",
      "Epoch 21 Iter 7300 loss=355739.2634 elbo -33306.5312 sf -286986.2500 selected 0.4753 kl 55.5218 ll -37.5512\n",
      "Epoch 21 Iter 7400 loss=365435.2852 elbo -35871.3008 sf -308247.3438 selected 0.4649 kl 61.2602 ll -37.1240\n",
      "Epoch 21 Iter 7500 loss=361440.1056 elbo -46848.2188 sf -380473.9062 selected 0.4591 kl 77.8248 ll -39.0457\n",
      "\n",
      "# epoch 21 iter 7502: dev loss 12524.8732 elbo -36031.8393 sf 23506.9714 selected 0.4675 kl 64.7509 ll -36.4881 acc 0.3606\n",
      " dev0 [gold=3,pred=1]: **it** 's a **lovely** **film** **with** lovely performances **by** buy and accorsi **.**\n",
      " dev1 [gold=2,pred=1]: **no** **one** **goes** **unindicted** here , **which** is **probably** for **the** best .\n",
      " dev2 [gold=3,pred=1]: and if **you** **'re** not **nearly** **moved** **to** tears by a couple of **scenes** **,** **you** 've **got** ice water **in** your veins **.**\n",
      "\n",
      "Shuffling training data\n",
      "Epoch 22 Iter 7600 loss=353854.8048 elbo -31093.3555 sf -252135.9219 selected 0.4512 kl 53.1780 ll -37.2049\n",
      "Epoch 22 Iter 7700 loss=351188.8727 elbo -38312.0156 sf -331549.9375 selected 0.4712 kl 76.9061 ll -35.5017\n",
      "Epoch 22 Iter 7800 loss=357655.3950 elbo -30283.1816 sf -244325.3438 selected 0.4581 kl 52.4462 ll -37.3833\n",
      "\n",
      "# epoch 22 iter 7843: dev loss 12622.8622 elbo -36143.1904 sf 23520.3419 selected 0.4645 kl 65.2305 ll -36.5631 acc 0.3506\n",
      " dev0 [gold=3,pred=3]: it 's **a** lovely film **with** lovely performances **by** buy **and** **accorsi** .\n",
      " dev1 [gold=2,pred=3]: **no** **one** **goes** unindicted here , which is probably for the **best** .\n",
      " dev2 [gold=3,pred=1]: **and** **if** you **'re** **not** nearly moved to tears by a **couple** **of** scenes , you 've **got** ice **water** **in** **your** **veins** **.**\n",
      "\n",
      "Shuffling training data\n",
      "Epoch 23 Iter 7900 loss=346783.9909 elbo -34112.0352 sf -279957.1562 selected 0.4789 kl 58.4536 ll -37.3150\n",
      "Epoch 23 Iter 8000 loss=351657.1206 elbo -33476.4531 sf -286166.3125 selected 0.4629 kl 58.7456 ll -36.5804\n",
      "Epoch 23 Iter 8100 loss=348544.5739 elbo -36457.8867 sf -279483.7188 selected 0.4514 kl 53.8099 ll -40.1287\n",
      "\n",
      "# epoch 23 iter 8184: dev loss 12488.2523 elbo -35986.2724 sf 23498.0200 selected 0.4643 kl 61.2706 ll -36.5351 acc 0.3560\n",
      " dev0 [gold=3,pred=1]: **it** 's a **lovely** **film** with lovely performances by buy **and** **accorsi** **.**\n",
      " dev1 [gold=2,pred=1]: **no** one goes **unindicted** here , **which** is probably for **the** best .\n",
      " dev2 [gold=3,pred=1]: and **if** you **'re** not nearly moved **to** **tears** by a **couple** **of** **scenes** **,** you **'ve** **got** ice **water** in your veins .\n",
      "\n",
      "Epoch 24 Iter 8200 loss=350629.9356 elbo -33528.8008 sf -273866.5938 selected 0.4400 kl 54.9306 ll -37.8301\n",
      "Shuffling training data\n",
      "Epoch 24 Iter 8300 loss=352643.7167 elbo -38499.7188 sf -331568.4375 selected 0.4566 kl 63.5604 ll -36.0105\n",
      "Epoch 24 Iter 8400 loss=354458.9709 elbo -34505.9258 sf -278426.1562 selected 0.4388 kl 51.3654 ll -39.0844\n",
      "Epoch 24 Iter 8500 loss=358161.9716 elbo -29164.7617 sf -271777.6562 selected 0.4575 kl 48.4296 ll -34.9425\n",
      "\n",
      "# epoch 24 iter 8525: dev loss 12567.3318 elbo -36008.4327 sf 23441.0980 selected 0.4557 kl 60.1929 ll -36.5618 acc 0.3551\n",
      " dev0 [gold=3,pred=1]: it 's **a** **lovely** **film** with lovely performances by buy and **accorsi** **.**\n",
      " dev1 [gold=2,pred=3]: no **one** goes **unindicted** here **,** which is probably for the best .\n",
      " dev2 [gold=3,pred=1]: **and** **if** you **'re** not nearly moved to **tears** by a **couple** **of** **scenes** **,** **you** 've got **ice** water in your **veins** .\n",
      "\n",
      "Shuffling training data\n",
      "Epoch 25 Iter 8600 loss=368894.2569 elbo -36658.7734 sf -312065.0938 selected 0.4605 kl 55.1184 ll -38.1413\n",
      "Epoch 25 Iter 8700 loss=367699.7855 elbo -36076.2031 sf -310090.3438 selected 0.4411 kl 60.2065 ll -38.4123\n",
      "Epoch 25 Iter 8800 loss=360920.2497 elbo -43970.9727 sf -383470.8750 selected 0.4542 kl 69.1870 ll -37.5478\n",
      "\n",
      "# epoch 25 iter 8866: dev loss 12579.6351 elbo -36080.1848 sf 23500.5426 selected 0.4525 kl 60.0803 ll -36.6441 acc 0.3497\n",
      " dev0 [gold=3,pred=1]: it **'s** **a** **lovely** **film** **with** **lovely** **performances** by **buy** **and** **accorsi** **.**\n",
      " dev1 [gold=2,pred=3]: no one goes unindicted **here** **,** which is probably for the best .\n",
      " dev2 [gold=3,pred=1]: and if you 're not **nearly** moved to tears by a couple **of** scenes **,** you **'ve** **got** ice water in your veins **.**\n",
      "\n",
      "Shuffling training data\n",
      "Epoch 26 Iter 8900 loss=377942.1559 elbo -37434.1484 sf -337670.9062 selected 0.4831 kl 64.6379 ll -36.7366\n",
      "Epoch 26 Iter 9000 loss=368480.9080 elbo -31789.8184 sf -287487.9688 selected 0.4565 kl 49.4407 ll -35.9456\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26 Iter 9100 loss=372285.1556 elbo -40172.2148 sf -362563.2188 selected 0.4800 kl 65.7008 ll -36.6949\n",
      "Epoch 26 Iter 9200 loss=366821.5009 elbo -43449.9453 sf -433252.3750 selected 0.4640 kl 83.1483 ll -33.0971\n",
      "\n",
      "# epoch 26 iter 9207: dev loss 12657.7738 elbo -36137.3946 sf 23479.6068 selected 0.4504 kl 58.2238 ll -36.7908 acc 0.3678\n",
      " dev0 [gold=3,pred=3]: it 's a **lovely** **film** **with** **lovely** **performances** by **buy** **and** **accorsi** **.**\n",
      " dev1 [gold=2,pred=3]: no one goes unindicted **here** **,** which is probably for the **best** .\n",
      " dev2 [gold=3,pred=1]: **and** if **you** **'re** not **nearly** moved to **tears** **by** a **couple** of **scenes** , you 've got ice **water** **in** your veins **.**\n",
      "\n",
      "Shuffling training data\n",
      "Epoch 27 Iter 9300 loss=370943.5409 elbo -32430.5449 sf -281970.1250 selected 0.4353 kl 46.3566 ll -36.7901\n",
      "Epoch 27 Iter 9400 loss=361231.7069 elbo -34649.8125 sf -310953.4688 selected 0.4724 kl 61.3705 ll -35.8006\n"
     ]
    }
   ],
   "source": [
    "train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7w_Ko657vRGo"
   },
   "source": [
    "# Variance reduction\n",
    "\n",
    "**This is an extra**\n",
    "\n",
    "We can use a *control variate* to reduce the variance of our gradient estimates.\n",
    "\n",
    "Let's recap the idea in general terms. We are looking to solve some expectation\n",
    "\\begin{align}\n",
    "\\mu_f = \\mathbb E[f(Z)]\n",
    "\\end{align}\n",
    "but unfortunatelly, realising the full sum (or integral for continuous variables) is intractable. Thus we employ MC estimation\n",
    "\\begin{align}\n",
    "\\hat \\mu_f &\\overset{\\text{MC}}{\\approx} \\frac{1}{S} \\sum_{s=1}^S f(z_s) & \\text{where }z_s \\sim Q(z|x)\n",
    "\\end{align}\n",
    "Note that the variance of this estimate is\n",
    "\\begin{align}\n",
    "\\text{Var}(\\hat \\mu_f) &=  \\frac{1}{S}\\text{Var}(f(Z)) \\\\\n",
    "&= \\frac{1}{S} \\mathbb E[( f(Z) - \\mathbb E[f(Z)])^2]\n",
    "\\end{align}\n",
    "Note that this variance is such that it goes down as we sample more, in a rate $\\mathcal O(S^{-1})$.\n",
    "See that if we sample $10$ times more, we will only obtain an decrease in variance in the order of $10^{-1}$. This means that sampling more is generally not the most convenient way to decrease variance.\n",
    "\n",
    "*Digression* we can estimate the variance itself via MC, an unbiased estimate looks like\n",
    "\\begin{align}\n",
    "\\hat \\sigma^2_f = \\frac{1}{S(S-1)} \\sum_{s=1}^S (f(z_s) - \\hat \\mu_f)^2\n",
    "\\end{align}\n",
    "but not that this estimate is even hard to improve since it decreases with $\\mathcal O(S^{-2})$.\n",
    "\n",
    "Back to out main problem: let's try and improve the variance of our estimator to $\\mu_f$.\n",
    "\n",
    "It's a fact, and it can be shown trivially, that\n",
    "\\begin{align}\n",
    "\\mu_f &=  \\mathbb E[f(Z) - \\psi(Z)] + \\underbrace{\\mathbb E[\\psi(Z)]}_{\\mu_\\psi} \\\\\n",
    " &\\overset{\\text{MC}}{\\approx} \\underbrace{\\left(\\frac{1}{S} \\sum_{s=1}^S f(z_s) - \\psi(z_s) \\right) + \\mu_\\psi}_{\\hat c}\n",
    "\\end{align}\n",
    "where we assume the existence of some function $\\psi(z)$ for which the expected value $\\mu_\\psi$ is known and we estimate the expected difference $\\mathbb E[f(Z) - \\psi(Z)]$ via MC. We used this axuxiliary function, also known as a *control variate*, to derive a new estimator, which we will denote by $\\hat c$.\n",
    "\n",
    "The variance of this new estimator is show below:\n",
    "\n",
    "\\begin{align}\n",
    "\\text{Var}( \\hat c ) &= \\text{Var}(\\hat \\mu_{f-\\psi}) + 2\\underbrace{\\text{Cov}(\\hat \\mu_{f-\\psi}, \\mu_\\psi)}_{\\mathbb E[\\hat \\mu_{f-\\psi}  \\mu_\\psi] - \\mathbb E[\\hat \\mu_{f-\\psi}] \\mathbb E[\\mu_\\psi]} + \\underbrace{\\text{Var}(\\mu_\\psi)}_{\\color{blue}{0} } \\\\\n",
    "&= \\frac{1}{S}\\text{Var}(f- \\psi)  + 2 \\underbrace{\\left( \\mu_\\psi \\mu_{f-\\psi} - \\mu_{f-\\psi} \\mu_\\psi \\right)}_{\\color{blue}{0}} \n",
    "\\end{align}\n",
    "where the variance of $\\mu_\\psi$ is 0 because we know it in closed form (no need for MC estimation), and the covariance is $0$ as shown in the second row.\n",
    "\n",
    "That is, the variance of $\\hat c$ is essentially the variance of estimating $\\mathbb E[f(Z) - \\psi(Z)]$, which in turn depends on the variance \n",
    "\n",
    "\\begin{align}\n",
    "\\text{Var}(f-\\psi) &= \\text{Var}(f) - 2\\text{Cov}(f, \\psi) + \\text{Var}(\\psi)\n",
    "\\end{align}\n",
    "where we can see that if $\\text{Cov}(f, \\psi) > \\frac{\\text{Var}(\\psi)}{2}$ we achieve variance reduction as then $\\text{Var}(f-\\psi)$ would be smaller than $\\text{Var(f)}$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ovKcRnqH_PGp"
   },
   "source": [
    "\n",
    "## Baselines\n",
    "\n",
    "Baslines are control variates of a very simple form:\n",
    "\\begin{align}\n",
    "\\mathbb E[f(Z)] = \\mathbb E[f(Z) - C] + \\mathbb E[C]\n",
    "\\end{align}\n",
    "where $C$ is a constant with respect to $z$.\n",
    "\n",
    "In the context of the score function estimator, a baseline looks like a quantity $C(x; \\omega)$, this may be\n",
    "* just a constant;\n",
    "* or a function of the input (but not of the latent variable), which could be itself implemented as a neural network;\n",
    "* a combination of the two.\n",
    " \n",
    "\n",
    "Let's focus on the first term of the ELBO (so I'm omitting the KL term here). The gradient with respect to parameters of the inference model becomes:\n",
    "\n",
    "\\begin{align}\n",
    "&\\mathbb E_{Q(z|x, \\lambda)}\\left[ \\log P(x|z, \\theta) \\nabla_\\lambda \\log Q(z|x, \\lambda)\\right]\\\\\n",
    "&=\\mathbb E_{Q(z|x, \\lambda)}\\left[\\log P(x|z, \\theta) \\nabla_\\lambda \\log Q(z|x, \\lambda) - \\color{red}{C(x; \\omega)\\nabla_\\lambda \\log Q(z|x, \\lambda) }  \\right] + \\underbrace{\\mathbb E_{Q(z|x, \\lambda)}\\left[\\color{red}{C(x; \\omega)\\nabla_\\lambda \\log Q(z|x, \\lambda) }  \\right] }_{=0} \\\\\n",
    "&= \\mathbb E_{Q(z|x, \\lambda)}\\left[ \\color{blue}{\\left(\\log P(x|z, \\theta) - C(x; \\omega) \\right)}\\nabla_\\lambda \\log Q(z|x, \\lambda)\\right] \\\\\n",
    "&\n",
    "\\end{align}\n",
    "We can show that the last term is $0$\n",
    "\n",
    "\\begin{align}\n",
    "&\\mathbb E_{Q(z|x, \\lambda)}\\left[C(x; \\omega)\\nabla_\\lambda \\log Q(z|x, \\lambda)   \\right]  \\\\&= C(x; \\omega) \\mathbb E_{Q(z|x, \\lambda)}\\left[\\nabla_\\lambda \\log Q(z|x, \\lambda)   \\right]\\\\\n",
    "&= C(x; \\omega) \\mathbb E_{Q(z|x, \\lambda)}\\left[\\frac{1}{Q(z|x, \\lambda)} \\nabla_\\lambda Q(z|x, \\lambda)   \\right] \\\\\n",
    "&= C(x; \\omega) \\sum_z Q(z|x, \\lambda) \\frac{1}{Q(z|x, \\lambda)} \\nabla_\\lambda Q(z|x, \\lambda)   \\\\\n",
    "&= C(x; \\omega) \\sum_z\\nabla_\\lambda Q(z|x, \\lambda)  \\\\\n",
    "&= C(x; \\omega) \\nabla_\\lambda \\underbrace{\\sum_z Q(z|x, \\lambda)  }_{=1}\\\\\n",
    "&=0\n",
    "\\end{align}\n",
    "\n",
    "Examples of useful baselines:\n",
    "\n",
    "* a running average of the learning signal: at some iteration $t$ we can use a running average of $\\log P(x|z, \\theta)$ using parameter estimates $\\theta$ from iterations $i < t$, this is a baseline that likely leads to high correlation between control variate and learning signal and can lead to variance reduction;\n",
    "* another technique is to have an MLP with parameters $\\omega$ predict a scalar and train this MLP to approximate the learning signal $\\log P(x|z, \\theta)$ via regression:\n",
    "\\begin{align}\n",
    "\\arg\\max_\\omega \\left( C(x; \\omega) - \\log P(x|z, \\theta) \\right)^2\n",
    "\\end{align}\n",
    "its left as an extra to implement these ideas.\n",
    "\n",
    "One more note: we can also use something called a *multiplicative baseline* in the literature of reinforcement learning, whereby we incorporate a running estimate of the standard deviation of the learning signal computed based on the values attained on previous iterations:\n",
    "\\begin{align}\n",
    "\\mathbb E_{Q(z|x, \\lambda)}\\left[ \\frac{1}{\\hat\\sigma_{\\text{past}}}\\left(\\log P(x|z, \\theta) - \\hat \\mu_{\\text{past}}\\right)\\nabla_\\lambda \\log Q(z|x, \\lambda)\\right]\n",
    "\\end{align}\n",
    "this form of contorl variate aim at promoting the learning signal (or reward in reinforcement learning literature) to be distributed by $\\mathcal N(0, 1)$. Note that multiplying the reward by a constant does not bias the estimator, and in this case, may lead to variance reduction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SVsWgmlIWvZq"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "T2VntYV3WvZt"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rTxG1AvPWvZv"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "SST.ipynb",
   "provenance": [],
   "toc_visible": true,
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
