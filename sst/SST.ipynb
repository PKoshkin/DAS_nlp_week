{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/probabll/dgm4nlp/blob/master/notebooks/sst/SST.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Udt3kHMdWvYe"
   },
   "source": [
    "We will need to import some helper code, so we need to run this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "U8eXUCRiWvYi"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "nb_dir = os.path.split(os.getcwd())[0]\n",
    "if nb_dir not in sys.path:\n",
    "    sys.path.append(nb_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rYhItDYMZi6a"
   },
   "source": [
    "# Colab\n",
    "\n",
    "We will need to download some data for this notebook, so if you are using [colab](https://colab.research.google.com), set the `using_colab` flag below to `True` in order to clone our [github repo](https://github.com/probabll/dgm4nlp)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_shCMftIx1rW"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "README.md                     kolka_3_SST.ipynb\r\n",
      "SST.ipynb                     kolka_working_SST-Copy1.ipynb\r\n",
      "__init__.py                   kolka_working_SST.ipynb\r\n",
      "\u001b[1m\u001b[36m__pycache__\u001b[m\u001b[m                   my_first_SST.ipynb\r\n",
      "\u001b[1m\u001b[36mdata\u001b[m\u001b[m                          \u001b[1m\u001b[36mnn\u001b[m\u001b[m\r\n",
      "evaluate.py                   \u001b[31mplotting.py\u001b[m\u001b[m\r\n",
      "\u001b[1m\u001b[36mimg\u001b[m\u001b[m                           sstutil.py\r\n",
      "kolka_2_SST.ipynb             util.py\r\n"
     ]
    }
   ],
   "source": [
    "using_colab = not True\n",
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "N-fFME2OW22i"
   },
   "outputs": [],
   "source": [
    "if using_colab:\n",
    "  !rm -fr dgm4nlp sst\n",
    "  !git clone https://github.com/probabll/dgm4nlp.git\n",
    "  !cp -R dgm4nlp/notebooks/sst ./  \n",
    "  !ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "l_7NCZlZacNu"
   },
   "source": [
    "Now we can start our lab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "s9mH-rUhWvYq"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "# CPU should be fine for this lab\n",
    "device = torch.device('cpu')  \n",
    "#device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')  \n",
    "from collections import OrderedDict\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "okMoxTJ9bWjc"
   },
   "source": [
    "# Sentiment Classification \n",
    "\n",
    "\n",
    "We are going to augment a sentiment classifier with a layer of discrete latent variables which will help us improve the model's interpretability. But first, let's quickly review the baseline task.\n",
    "\n",
    "\n",
    "In sentiment classification, we have some text input $x = \\langle x_1, \\ldots, x_n \\rangle$, e.g. a sentence or short paragraph, which expresses a certain sentiment $y$, i.e. one of $K$ classes, towards a subject (e.g. a film or a product). \n",
    "\n",
    "\n",
    "\n",
    "We can learn a sentiment classifier by learning a categorical distribution over classes for a given input:\n",
    "\n",
    "\\begin{align}\n",
    "Y|x &\\sim \\text{Cat}(f(x; \\theta))\n",
    "\\end{align}\n",
    "\n",
    "where the Categorical pmf is $\\text{Cat}(y|\\pi) = \\pi_y$.\n",
    "\n",
    "A categorical distribution over $K$ classes is parameterised by a $K$-dimensional probability vector, here we use a neural network $f$ to map from the input to this probability vector. Technically we say *a neural network parameterise our model*, that is, it computes the parameters of our categorical observation model. The figure below is a graphical depiction of the model: circled nodes are random variables (a shaded node is an observed variable), uncircled nodes are deterministic, a plate indicates multiple draws.\n",
    "\n",
    "<img src=\"https://github.com/probabll/dgm4nlp/raw/master/notebooks/sst/img/classifier.png\"  height=\"100\">\n",
    "\n",
    "The neural network (NN) $f(\\cdot; \\theta)$ has parameters of its own, i.e. the weights of the various architecture blocks used, which we denoted generically by $\\theta$.\n",
    "\n",
    "Suppose we have a dataset $\\mathcal D = \\{(x^{(1)}, y^{(1)}), \\ldots, (x^{(N)}, y^{(N)})\\}$ containing $N$ i.i.d. observations. Then we can use the log-likelihood function \n",
    "\\begin{align}\n",
    "\\mathcal L(\\theta|\\mathcal D) &= \\sum_{k=1}^{N} \\log P(y^{(k)}|x^{(k)}, \\theta) \\\\\n",
    "&= \\sum_{k=1}^{N} \\log \\text{Cat}(y^{(k)}|f(x^{(k)}; \\theta))\n",
    "\\end{align}\n",
    " to estimate $\\theta$ by maximisation:\n",
    " \\begin{align}\n",
    " \\theta^\\star = \\arg\\max_{\\theta \\in \\Theta} \\mathcal L(\\theta|\\mathcal D) ~ .\n",
    " \\end{align}\n",
    " \n",
    "\n",
    "We can use stochastic gradient-ascent to find a local optimum of $\\mathcal L(\\theta|\\mathcal D)$, which only requires a gradient estimate:\n",
    "\n",
    "\\begin{align}\n",
    "\\nabla_\\theta \\mathcal L(\\theta|\\mathcal D) &= \\sum_{k=1}^{|\\mathcal D|} \\nabla_\\theta  \\log P(y^{(k)}|x^{(k)}, \\theta) \\\\ \n",
    "&= \\sum_{k=1}^{|\\mathcal D|} \\frac{1}{N} N \\nabla_\\theta  \\log P(y^{(k)}|x^{(k)}, \\theta)  \\\\\n",
    "&= \\mathbb E_{\\mathcal U(1/N)} \\left[ N \\nabla_\\theta  \\log P(y^{(K)}|x^{(K)}, \\theta) \\right]  \\\\\n",
    "&\\overset{\\text{MC}}{\\approx} \\frac{N}{M} \\sum_{m=1}^M \\nabla_\\theta  \\log P(y^{(k_m)}|x^{(k_m)}, \\theta) \\\\\n",
    "&\\text{where }K_m \\sim \\mathcal U(1/N)\n",
    "\\end{align}\n",
    "\n",
    "This is a Monte Carlo (MC) estimate of the gradient computed on $M$ data points selected uniformly at random from $\\mathcal D$.\n",
    "\n",
    "For as long as $f$ remains differentiable wrt to its inputs and parameters, we can rely on automatic differentiation to obtain gradient estimates.\n",
    "\n",
    "In what follows we show how to design $f$ and how to extend this basic model to a latent-variable model.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4LUjyO-39zan"
   },
   "source": [
    "## Data\n",
    "\n",
    "We provide you some code to load the data (see `sst.sstutil.examplereader`). Play with the snippet below and inspect a few training instances:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4z8Bt5no9z6w"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data\n",
      "train 8544\n",
      "dev 1101\n",
      "test 2210\n",
      "\n",
      "# Examples\n",
      "First dev example: Example(tokens=['It', \"'s\", 'a', 'lovely', 'film', 'with', 'lovely', 'performances', 'by', 'Buy', 'and', 'Accorsi', '.'], label=3, transitions=[0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1], token_labels=[2, 2, 2, 3, 2, 2, 3, 2, 2, 2, 2, 2, 2])\n",
      "First dev example tokens: ['It', \"'s\", 'a', 'lovely', 'film', 'with', 'lovely', 'performances', 'by', 'Buy', 'and', 'Accorsi', '.']\n",
      "First dev example label: 3\n"
     ]
    }
   ],
   "source": [
    "from sst.sstutil import examplereader, Vocabulary, load_glove    \n",
    "\n",
    "\n",
    "# Let's load the data into memory.\n",
    "print(\"Loading data\")\n",
    "train_data = list(examplereader('../sst/data/sst/train.txt'))\n",
    "dev_data = list(examplereader('../sst/data/sst/dev.txt'))\n",
    "test_data = list(examplereader('../sst/data/sst/test.txt'))\n",
    "\n",
    "print(\"train\", len(train_data))\n",
    "print(\"dev\", len(dev_data))\n",
    "print(\"test\", len(test_data))\n",
    "\n",
    "print('\\n# Examples')\n",
    "example = dev_data[0]\n",
    "print(\"First dev example:\", example)\n",
    "print(\"First dev example tokens:\", example.tokens)\n",
    "print(\"First dev example label:\", example.label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lB2lEsNuWvYx"
   },
   "source": [
    "## Architecture\n",
    "\n",
    "\n",
    "The function $f$ conditions on a high-dimensional input (i.e. text), so we need to convert it to continuous real vectors. This is the job an *encoder*. \n",
    "\n",
    "**Embedding Layer**\n",
    "\n",
    "The first step is to convert the words in $x$ to vectors, which in this lab we will do with a pre-trained embedding layer (we will use GloVe).\n",
    "\n",
    "We will denote the embedding of the $i$th word of the input by:\n",
    "\n",
    "\\begin{equation}\n",
    "\\mathbf x_i = \\text{glove}(x_i)\n",
    "\\end{equation}\n",
    "\n",
    "**Encoder Layer**\n",
    "\n",
    "In this lab, an encoder takes a sequence of input vectors $\\mathbf x_1^n$, each $I$-dimensional, and produces a sequence of output vectors $\\mathbf t_1^n$, each $O$-dimensional and a summary vector $\\mathbf h \\in \\mathbb R^O$:\n",
    "\n",
    "\\begin{equation}\n",
    "    \\mathbf t_1^n, \\mathbf h = \\text{encoder}(\\mathbf x_1^n; \\theta_{\\text{enc}})\n",
    "\\end{equation}\n",
    "\n",
    "where we use $\\theta_{\\text{enc}}$ to denote the subset of parameters in $\\theta$ that are specific to this encoder block. \n",
    "\n",
    "*Remark:* in practice for a correct batched implementation, our encoders also take a mask matrix and a vector of lengths.\n",
    "\n",
    "Examples of encoding functions can be a feed-forward NN (with an aggregator based on sum or average/max pooling) or a recurrent NN (e.g. an LSTM/GRU). Other architectures are also possible.\n",
    "\n",
    "**Output Layer**\n",
    "\n",
    "From our summary vector $\\mathbf h$, we need to parameterise a categorical distribution over $K$ classes, thus we use\n",
    "\n",
    "\\begin{align}\n",
    "f(x; \\theta) &= \\text{softmax}(\\text{dense}_K(\\mathbf h; \\theta_{\\text{output}}))\n",
    "\\end{align}\n",
    "\n",
    "where $\\text{dense}_K$ is a dense layer with $K=5$ outputs and $\\theta_{\\text{output}}$ corresponds to its parameters (weight matrix and bias vector). Note that we need to use the softmax activation function in order to guarantee that the output of $f$ is a normalised probability vector.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Kc15Nv2i41cq"
   },
   "source": [
    "## Implementation\n",
    "\n",
    "To leave an indication of the shape of tensors in the code, we use the following convention\n",
    "\n",
    "```python\n",
    "[B, T, D]\n",
    "```\n",
    "\n",
    "where `B` stands for `batch_size`, `T` stands for `time` (or rather *maximum sequence length*), and `D` is the size of the representation.\n",
    "\n",
    "\n",
    "Consider the following abstract Encoder class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xwEPXT2MWvYz",
    "tags": [
     "encoders"
    ]
   },
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    \"\"\"\n",
    "    An Encoder for us is a function that\n",
    "      1. transforms a sequence of I-dimensional vectors into a sequence of O-dimensional vectors\n",
    "      2. summarises a sequence of I-dimensional vectors into one O-dimensional vector\n",
    "      \n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    def __init__(self):\n",
    "        super(Encoder, self).__init__()\n",
    "        \n",
    "    def forward(self, inputs, mask, lengths):\n",
    "        \"\"\"\n",
    "        The input is a batch-first tensor of token ids. Here is an example:\n",
    "        \n",
    "        Example of inputs (though rather than words, we have word ids):\n",
    "            INPUTS                     MASK       LENGTHS\n",
    "            [the nice cat -PAD-]    -> [1 1 1 0]  [3]\n",
    "            [the nice dog running]  -> [1 1 1 1]  [4]\n",
    "            \n",
    "        Note that:\n",
    "              mask =  inputs == 1\n",
    "              lengths = mask.sum(dim=-1)\n",
    "        \n",
    "        :param inputs: [B, T, I]\n",
    "        :param mask: [B, T]\n",
    "        :param lengths: [B]\n",
    "        :returns: [B, T, O], [B, O]\n",
    "            where the first tensor is the transformed input\n",
    "            and the second tensor is a summary of all inputs\n",
    "        \"\"\"\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WA5wmkcRg9Am"
   },
   "source": [
    "Let's start easy, implement a *bag of words* encoder:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "U-9hLQ0lF5SG"
   },
   "outputs": [],
   "source": [
    "class BagOfWordsEncoder(Encoder):\n",
    "    \"\"\"\n",
    "    This encoder does not transform the input sequence, \n",
    "     and its summary output is just a sum.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(BagOfWordsEncoder, self).__init__()\n",
    "        \n",
    "    def forward(self, inputs, mask, lengths=None, **kwargs):\n",
    "        reshaped_mask = mask.float().unsqueeze(-1)  # shape: [B, T, 1]\n",
    "        return inputs, (inputs * reshaped_mask).sum(1) / (reshaped_mask.sum(1) + 1e-8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IS7x0hLrUXfN"
   },
   "source": [
    "You can also consider implementing\n",
    "\n",
    "* a feed-forward encoder with average pooling\n",
    "* and a biLSTM encoder\n",
    "\n",
    "but these are certainly optional."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BpOGFpK_Uo0-"
   },
   "outputs": [],
   "source": [
    "class FFEncoder(Encoder):\n",
    "    \"\"\"\n",
    "    A typical feed-forward NN with tanh hidden activations.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self,\n",
    "                 input_size,\n",
    "                 output_size, \n",
    "                 activation=None, \n",
    "                 hidden_sizes=[], \n",
    "                 aggregator='sum',\n",
    "                 dropout=0.5):\n",
    "        \"\"\"\n",
    "        :param input_size: int\n",
    "        :param output_size: int\n",
    "        :param hidden_sizes: list of integers (dimensionality of hidden layers)\n",
    "        :param aggregator: 'sum' or 'avg'\n",
    "        :param dropout: dropout rate\n",
    "        \"\"\"\n",
    "        super(FFEncoder, self).__init__()\n",
    "        layers = []\n",
    "        \n",
    "        def add_droput(name):\n",
    "            if dropout > 0:\n",
    "                layers.append((name, nn.Dropout(p=dropout)))\n",
    "        \n",
    "        if len(hidden_sizes) > 0:                    \n",
    "            for i, size in enumerate(hidden_sizes):\n",
    "                add_droput('dropout_{}'.format(i))\n",
    "                layers.append(('linear_{}'.format(i), nn.Linear(input_size, size)))\n",
    "                layers.append(('tanh_{}'.format(i), nn.Tanh()))\n",
    "                input_size = size\n",
    "\n",
    "        last_output_size = hidden_sizes[-1] if len(hidden_sizes) > 0 else input_size\n",
    "        add_droput('final_dropout')\n",
    "        layers.append(('final_linear', nn.Linear(last_output_size, output_size)))       \n",
    "        self.layer = nn.Sequential(OrderedDict(layers))\n",
    "\n",
    "        self.activation = activation\n",
    "\n",
    "        if not aggregator in ['sum', 'avg']:\n",
    "            raise ValueError(\"I can only aggregate outputs using 'sum' or 'avg'\")\n",
    "        self.aggregator = aggregator\n",
    "        \n",
    "    def forward(self, inputs, mask, lengths):\n",
    "        outputs = self.layer(inputs)  # shape: [B, T, O]\n",
    "        if not self.activation is None:\n",
    "            outputs = self.activation(outputs)  # shape: [B, T, O]\n",
    "        reshaped_mask = mask.float().unsqueeze(-1)  # shape: [B, T, 1]\n",
    "        summary = (outputs * reshaped_mask).sum(dim=1)  # shape: [B, O]\n",
    "        if self.aggregator == 'avg':\n",
    "            reshaped_lens = lengths.float().unsqueeze(-1)  # shape: [B, 1]\n",
    "            summary /= reshaped_lens  # shape: [B, O]\n",
    "        return outputs, summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IxQ5djZ_VAvK"
   },
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "\n",
    "\n",
    "class LSTMEncoder(Encoder):\n",
    "    \"\"\"\n",
    "    This module encodes a sequence into a single vector using an LSTM,\n",
    "     it also returns the hidden states at each time step.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 in_features,\n",
    "                 hidden_size: int=200,\n",
    "                 batch_first: bool=True,\n",
    "                 bidirectional: bool=True):\n",
    "        \"\"\"\n",
    "        :param in_features:\n",
    "        :param hidden_size:\n",
    "        :param batch_first:\n",
    "        :param bidirectional:\n",
    "        \"\"\"\n",
    "        super(LSTMEncoder, self).__init__()\n",
    "        self.lstm = nn.LSTM(\n",
    "            in_features,\n",
    "            hidden_size,\n",
    "            batch_first=batch_first,\n",
    "            bidirectional=bidirectional\n",
    "        )\n",
    "        self.batch_first = batch_first\n",
    "\n",
    "    def forward(self, x, mask, lengths):\n",
    "        \"\"\"\n",
    "        Encode sentence x\n",
    "        :param x: sequence of word embeddings, shape [B, T, E]\n",
    "        :param mask: byte mask that is 0 for invalid positions, shape [B, T]\n",
    "        :param lengths: the lengths of each input sequence [B]\n",
    "        :return:\n",
    "        \"\"\"\n",
    "\n",
    "        packed_sequence = pack_padded_sequence(x, lengths, batch_first=self.batch_first)\n",
    "        outputs, (hx, cx) = self.lstm(packed_sequence)\n",
    "        outputs, _ = pad_packed_sequence(outputs, batch_first=self.batch_first)\n",
    "\n",
    "        # classify from concatenation of final states\n",
    "        if self.lstm.bidirectional:\n",
    "            final = torch.cat([hx[-2], hx[-1]], dim=-1)\n",
    "        else:  # classify from final state\n",
    "            final = hx[-1]\n",
    "\n",
    "        return outputs, final"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "s_zz5zIyVkSh"
   },
   "source": [
    "Here is some helper code to select and return an encoder:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "59ZU6JddVjMV"
   },
   "outputs": [],
   "source": [
    "def get_encoder(layer, in_features, hidden_size, bidirectional=True):\n",
    "    \"\"\"Returns the requested layer.\"\"\"\n",
    "\n",
    "    # TODO: make pass and average layers\n",
    "    if layer == \"bow\":\n",
    "        return BagOfWordsEncoder()\n",
    "    elif layer == 'ff':\n",
    "        return FFEncoder(\n",
    "            in_features, \n",
    "            2 * hidden_size,   # for convenience\n",
    "            hidden_sizes=[hidden_size], \n",
    "            aggregator='avg'\n",
    "        )\n",
    "    elif layer == \"lstm\":\n",
    "        return LSTMEncoder(\n",
    "            in_features, \n",
    "            hidden_size,\n",
    "            bidirectional=bidirectional\n",
    "        )\n",
    "    else:\n",
    "        raise ValueError(\"Unknown layer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kY8LZiMN5CHW"
   },
   "source": [
    "# Sentiment Classification with Latent Rationale\n",
    "\n",
    "A latent rationale is a compact and informative fragment of the input based on which a NN classifier makes its decisions. [Lei et al (2016)](http://aclweb.org/anthology/D16-1011) proposed to induce such rationales along with a regression model for multi-aspect sentiment analsysis, their model is trained via REINFORCE on a dataset of beer reviews.\n",
    "\n",
    "*Remark:* the model we will develop here can be seen as a probabilistic version of their model. The rest of this notebook focus on our own probabilitisc view of the model.\n",
    "\n",
    "The picture below depicts our latent-variable model for rationale extraction:\n",
    "\n",
    "<img src=\"https://github.com/probabll/dgm4nlp/raw/master/notebooks/sst/img/rationale.png\"  height=\"200\">\n",
    "\n",
    "where we augment the model with a collection of latent variables $z = \\langle z_1, \\ldots, z_n\\rangle$ where $z_i$ is a binary latent variable. Each latent variable $z_i$ regulates whether or not the input $x_i$ is available to the classifier.  We use $x \\odot z$ to denote the selected words, which, in the terminology of Lei et al, is a latent rationale.\n",
    "\n",
    "Again the classifier parameterises a Categorical distribution over $K=5$ outcomes, though this time it can encode only a selection of the input:\n",
    "\n",
    "\\begin{align}\n",
    "    Z_i & \\sim \\text{Bern}(p_1) \\\\\n",
    "    Y|z,x &\\sim \\text{Cat}(f(x \\odot z; \\theta))\n",
    "\\end{align}\n",
    "\n",
    "where we have a shared and fixed Bernoulli prior (with parameter $p_1$) for all $n$ latent variables.\n",
    "\n",
    "\n",
    "Here is an example design for $f$:\n",
    "\n",
    "\\begin{align}\n",
    "\\mathbf x_i &= z_i \\, \\text{glove}(x_i) \\\\\n",
    "\\mathbf t_1^n, \\mathbf h &= \\text{encoder}(\\mathbf x_1^n; \\theta_{\\text{enc}}) \\\\\n",
    "f(x \\odot z; \\theta) &= \\text{softmax}(\\text{dense}_K(\\mathbf h; \\theta_{\\text{output}}))\n",
    "\\end{align}\n",
    "\n",
    "where:\n",
    "* $z_i$ either leaves $\\mathbf x_i$ unchanged or turns it into a vector of zeros;\n",
    "* the encoder only sees features from selected inputs, i.e. $x_i$ for which $z_i = 1$;\n",
    "* $\\text{dense}_K$ is a linear layer with $K=5$ outputs.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hDHNxLHMWvY-"
   },
   "source": [
    "## Prior\n",
    "\n",
    "\n",
    "Our prior is a Bernoulli with fixed parameter $0 < p_1 < 1$:\n",
    "\n",
    "\\begin{align}\n",
    "Z_i & \\sim \\text{Bern}(p_1)\n",
    "\\end{align}\n",
    "\n",
    "whose pmf is $\\text{Bern}(z_i|p_1) = p_1^{z_i}\\times (1-p_1)^{1-z_i}$.\n",
    "\n",
    "As we will be using Bernoulli priors and posteriors, it is a good idea to implement a Bernoulli class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iCBcHnTsOuDr"
   },
   "outputs": [],
   "source": [
    "class Bernoulli:\n",
    "    \"\"\"\n",
    "    This class encapsulates a collection of Bernoulli distributions. \n",
    "    Each Bernoulli is uniquely specified by p_1, where\n",
    "        Bernoulli(X=x|p_1) = pow(p_1, x) + pow(1 - p_1, 1 - x)\n",
    "    is the Bernoulli probability mass function (pmf).    \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, logits=None, probs=None):\n",
    "        \"\"\"\n",
    "        We can specify a Bernoulli distribution via a logit or a probability. \n",
    "         You need to specify at least one, and if you specify both, beware that\n",
    "         in this implementation logits will be used.\n",
    "         \n",
    "        Recall that: probs = sigmoid(logits).\n",
    "         \n",
    "        :param logits: a tensor of logits (a logit is defined as log (p_1 / p_0))\n",
    "            where p_0 = 1 - p_1\n",
    "        :param probs: a tensor of probabilities, each in (0, 1)\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        #print(\"probs:\", type(probs))\n",
    "        #print(\"logits:\", type(logits))\n",
    "                \n",
    "        if probs is None and logits is None:\n",
    "            raise ValueError('I need probabilities or logits')   \n",
    "        \n",
    "        self.probs = probs if logits is None else torch.sigmoid(logits)\n",
    "    \n",
    "    def sample(self):\n",
    "        \"\"\"Returns a sample with the same shape as the parameters\"\"\"\n",
    "        return torch.bernoulli(self.probs)\n",
    "    \n",
    "    def log_pmf(self, x):\n",
    "        \"\"\"\n",
    "        Assess the log probability of a sample. \n",
    "        :param x: either a single sample (0 or 1) or a tensor of samples with the same shape as the parameters.\n",
    "        :returns: tensor with log probabilities with the same shape as parameters\n",
    "            (if the input is a single sample we broadcast it to the shape of the parameters)\n",
    "        \"\"\"\n",
    "        return x * torch.log(self.probs) + (1.0 - x) * torch.log(1.0 - self.probs)\n",
    "    \n",
    "    def kl(self, other: 'Bernoulli'):\n",
    "        \"\"\"\n",
    "        Compute the KL divergence between two Bernoulli distributions (from self to other).\n",
    "        \n",
    "        :return: KL[self||other] with same shape parameters\n",
    "        \"\"\"\n",
    "        return self.probs * (torch.log(self.probs) - torch.log(other.probs)) \\\n",
    "            + (1 - self.probs) * (torch.log(1 - self.probs) - torch.log(1 - other.probs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "u0yfkCZlWvZP"
   },
   "source": [
    "## Classifier\n",
    "\n",
    "The classifier encodes only a selection of the input, which we denote $x \\odot z$, and parameterises a Categorical distribution over $5$ outcomes (sentiment levels).\n",
    "\n",
    "Thus let's implement a Categorical distribution (we will only need to be able to assess its lgo pmf):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "F-6JLDnBQcdg"
   },
   "outputs": [],
   "source": [
    "class Categorical:\n",
    "    \n",
    "    def __init__(self, log_probs):\n",
    "        # [B, K]: class probs\n",
    "        self.log_probs = log_probs\n",
    "        \n",
    "    def log_pmf(self, x):\n",
    "        \"\"\"\n",
    "        :param x: [B] integers (targets)\n",
    "        :returns: [B] scalars (log probabilities)\n",
    "        \"\"\"\n",
    "        return self.log_probs[np.arange(len(x)), x]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CdrM_YRI8xBF"
   },
   "source": [
    "and a classifier architecture:\n",
    "\n",
    "* implement the forward method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sz7GaKbgRCd8"
   },
   "outputs": [],
   "source": [
    "class Classifier(nn.Module):\n",
    "    \"\"\"\n",
    "    The Encoder takes an input text (and rationale z) and computes p(y|x,z)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 embed: nn.Embedding=None,\n",
    "                 hidden_size: int=200,\n",
    "                 output_size: int=1,\n",
    "                 dropout: float=0.1,\n",
    "                 layer: str=\"pass\"):\n",
    "\n",
    "        super(Classifier, self).__init__()\n",
    "\n",
    "        emb_size = embed.weight.shape[1]\n",
    "        enc_size = hidden_size * 2\n",
    "        # Here we embed the words\n",
    "        self.embed_layer = nn.Sequential(embed)\n",
    "\n",
    "        self.enc_layer = get_encoder(layer, emb_size, hidden_size)\n",
    "\n",
    "        # and here we predict categorical parameters\n",
    "        self.output_layer = nn.Sequential(\n",
    "            nn.Dropout(p=dropout),\n",
    "            nn.Linear(enc_size, output_size),\n",
    "            nn.LogSoftmax(dim=-1)\n",
    "        )\n",
    "\n",
    "        self.report_params()\n",
    "\n",
    "    def report_params(self):\n",
    "        count = 0\n",
    "        for name, p in self.named_parameters():\n",
    "            if p.requires_grad and \"embed\" not in name:\n",
    "                count += np.prod(list(p.shape))\n",
    "        print(\"{} #params: {}\".format(self.__class__.__name__, count))\n",
    "\n",
    "    def forward(self, x, mask, z) -> Categorical:\n",
    "        \"\"\"\n",
    "        :params x: [B, T, I] word representations\n",
    "        :params mask: [B, T] indicates valid positions\n",
    "        :params z: [B, T] binary selectors\n",
    "        :returns: one Categorical distribution per instance in the batch\n",
    "          each conditioning only on x_i for which z_i = 1\n",
    "        \"\"\"\n",
    "        embeddings = self.embed_layer(x)  # [B, T, E]\n",
    "        embedding_mask = z.float().unsqueeze(-1)  # [B, T, 1]\n",
    "        masked_embeddings = embeddings * embedding_mask  # [B, T, E]\n",
    "        lengths = mask.long().sum(1)  # [B]\n",
    "\n",
    "        # encode the sentence\n",
    "        _, final = self.enc_layer(masked_embeddings, mask, lengths)\n",
    "\n",
    "        # predict sentiment from final state(s)\n",
    "        log_probs = self.output_layer(final)  # [B, T, O]\n",
    "        return Categorical(log_probs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "p2waCCBF9MaH"
   },
   "source": [
    "## Inference\n",
    "\n",
    "\n",
    "Computing the log-likelihood of an observation requires marginalising over assignments of $z$:\n",
    "\n",
    "\\begin{align}\n",
    "P(y|x,\\theta,p_1) &= \\sum_{z_1 = 0}^1 \\cdots \\sum_{z_n=0}^1 P(z|p_1)\\times P(y|x,z, \\theta) \\\\\n",
    "&= \\sum_{z_1 = 0}^1 \\cdots \\sum_{z_n=0}^1 \\left( \\prod_{i=1}^n \\text{Bern}(z_i|p_1)\\right) \\times \\text{Cat}(y|f(x \\odot z; \\theta)) \n",
    "\\end{align}\n",
    "\n",
    "This is clearly intractable: there are $2^n$ possible assignments to $z$ and because the classifier conditions on all latent selectors, there's no way to simplify the expression.\n",
    "\n",
    "We will avoid computing this intractable marginal by instead employing an independently parameterised inference model.\n",
    "This inference model $Q(z|x, y, \\lambda)$ is an approximation to the true postrerior $P(z|x, y, \\theta, p_1)$, and we use $\\lambda$ to denote its parameters.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0jcVdYTg8Wun"
   },
   "source": [
    "We make a *mean field* assumption, whereby we model latent variables independently given the input:\n",
    "\\begin{align}\n",
    "Q(z|x, y, \\lambda) \n",
    "    &= \\prod_{i=1}^{n} Q(z_i|x; \\lambda) \\\\\n",
    "    &= \\prod_{i=1}^{n} \\text{Bern}(z_i|g_i(x; \\lambda)) \n",
    "\\end{align}\n",
    "\n",
    "where $g(x; \\lambda)$ is a NN that maps from $x = \\langle x_1, \\ldots, x_n\\rangle$ to $n$ Bernoulli parameters, each of which, is a probability value (thus $0 < g_i(x; \\lambda) < 1$).\n",
    "\n",
    "Note that though we could condition on $y$ for approximate posterior inference, we are opportunistically leaving it out. This way, $Q$ is directly available at test time for making predictions. The figure below is a graphical depiction of the inference model (we show a dashed arrow from $y$ to $z$ to remind you that in principle the label is also available).\n",
    "\n",
    "<img src=\"https://github.com/probabll/dgm4nlp/raw/master/notebooks/sst/img/inference.png\"  height=\"200\">\n",
    "\n",
    "Here is an example design for $g$:\n",
    "\\begin{align}\n",
    "\\mathbf x_i &= \\text{glove}(x_i) \\\\\n",
    "\\mathbf t_1^n, \\mathbf h &= \\text{encoder}(\\mathbf x_1^n; \\lambda_{\\text{enc}}) \\\\\n",
    "g_i(x; \\lambda) &= \\sigma(\\text{dense}_1(\\mathbf t_i; \\lambda_{\\text{output}}))\n",
    "\\end{align}\n",
    "where\n",
    "* $\\text{glove}$ is a pre-trained embedding function;\n",
    "* $\\text{dense}_1$ is a dense layer with a single output;\n",
    "* and $\\sigma(\\cdot)$ is the sigmoid function, necessary to parameterise a Bernoulli distribution.\n",
    "\n",
    "From now on we will write $Q(z|x, \\lambda)$, that is, without $y.\n",
    "\n",
    "Here we implement this product of Bernoulli distributions:\n",
    "\n",
    "* implement $g$ in the constructor \n",
    "* and the forward pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YLxfcAbuSiFo"
   },
   "outputs": [],
   "source": [
    "class ProductOfBernoullis(nn.Module):\n",
    "    \"\"\"\n",
    "    This is an inference network that parameterises independent Bernoulli distributions.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 embed: nn.Embedding,\n",
    "                 hidden_size: int=200,\n",
    "                 layer: str=\"bow\"):\n",
    "        \"\"\"\n",
    "        :param embed: an embedding layer\n",
    "        :param hidden_suze: hidden size for transformed inputs\n",
    "        :param layer: 'bow' for BoW encoding\n",
    "          you may alternatively implement and 'lstm' option\n",
    "          which uses a biLSTM to transform the inputs         \n",
    "        \"\"\"\n",
    "        super(ProductOfBernoullis, self).__init__()\n",
    "        # 1. we should have an embedding layer \n",
    "        # 2. we may transform the representations\n",
    "        # 3. and we should compute parameters for Bernoulli distributions\n",
    "        \n",
    "        emb_size = embed.weight.shape[1]\n",
    "        # Using twice the units just to make the output as large as that of a biLSTM\n",
    "        enc_size = hidden_size * 2  \n",
    "\n",
    "        self.embed_layer = nn.Sequential(embed)\n",
    "        self.enc_layer = get_encoder(layer, emb_size, hidden_size)\n",
    "        self.logit_layer = nn.Linear(enc_size, 1, bias=True)\n",
    "                \n",
    "        self.report_params()\n",
    "    \n",
    "    def report_params(self):\n",
    "        count = 0\n",
    "        for name, p in self.named_parameters():\n",
    "            if p.requires_grad and \"embed\" not in name:\n",
    "                count += np.prod(list(p.shape))\n",
    "        print(\"{} #params: {}\".format(self.__class__.__name__, count))\n",
    "\n",
    "    def forward(self, x, mask) -> Bernoulli:\n",
    "        \"\"\"\n",
    "        It takes a tensor of tokens (integers)\n",
    "         and predicts a Bernoulli distribution for each position.\n",
    "        \n",
    "        :param x: [B, T]\n",
    "        :param mask: [B, T]\n",
    "        :returns: Bernoulli\n",
    "        \"\"\"\n",
    "\n",
    "        # encode sentence\n",
    "        lengths = mask.long().sum(1)  # [B]\n",
    "        embeddings = self.embed_layer(x)  # [B, T, E]\n",
    "        h, _ = self.enc_layer(embeddings, mask, lengths)  # [B, T, d]\n",
    "\n",
    "        # compute parameters for Bernoulli p(z|x)\n",
    "        # Bernoulli distributions\n",
    "        logits = self.logit_layer(h).squeeze(-1)  # [B, T]\n",
    "\n",
    "        return Bernoulli(logits=logits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fcCu7vkKWvZX"
   },
   "source": [
    "## Parameter Estimation\n",
    "\n",
    "In variational inference, our objective is to maximise the *evidence lowerbound* (ELBO):\n",
    "\n",
    "\\begin{align}\n",
    "\\log P(y|x) &\\ge \\mathbb E_{Q(z|x, y, \\lambda)}\\left[ \\log P(y|x, z, \\theta, p_1) \\right] - \\text{KL}(Q(z|x, y, \\lambda) || P(z|p_1)) \\\\\n",
    "\\text{ELBO}&\\overset{\\text{MF}}{=}\\mathbb E_{Q(z|x, y, \\lambda)}\\left[ \\log P(y|x, z, \\theta, p_1) \\right] - \\sum_{i=1}^n \\text{KL}(Q(z_i|x, \\lambda) || P(z_i|p_1)) \n",
    "\\end{align}\n",
    "\n",
    "where the *mean field* assumption we made implies that the KL term is simply a sum of KL divergences from a Bernoulli posterior to a Bernoulli prior.\n",
    "\n",
    "Note that the ELBO remains intractable, namely, solving the expectation in closed form still requires $2^n$ evaluations of the classifier network. Though unlike the true posterior $P(z|x,y, \\lambda)$, the approximation $Q(z|x,\\lambda)$ is tractable (it does not require an intractable normalisation) and can be used to obtain gradient estimates based on samples.\n",
    "\n",
    "### Gradient of the classifier network\n",
    "\n",
    "For the classifier, we encounter no problem:\n",
    "\n",
    "\\begin{align}\n",
    "\\nabla_\\theta \\text{ELBO} &=\\nabla_\\theta\\sum_{z} Q(z|x, \\lambda)\\log P(y|x,z,\\theta) - \\underbrace{\\nabla_\\theta \\sum_{i=1}^n \\text{KL}(Q(z_i|x, \\lambda) || P(z_i|p_1))}_{\\color{blue}{0}}  \\\\\n",
    "&=\\sum_{z} Q(z|x, \\lambda)\\nabla_\\theta\\log P(y|x,z,\\theta) \\\\\n",
    "&= \\mathbb E_{Q(z|x, \\lambda)}\\left[\\nabla_\\theta\\log P(y|x,z,\\theta) \\right] \\\\\n",
    "&\\overset{\\text{MC}}{\\approx} \\frac{1}{S} \\sum_{s=1}^S \\nabla_\\theta \\log P(y|x, z^{(s)}, \\theta) \n",
    "\\end{align}\n",
    "where $z^{(s)} \\sim Q(z|x,\\lambda)$.\n",
    "\n",
    "\n",
    "### Gradient of the inference network\n",
    "\n",
    "For the inference model, we have to use the *score function estimator* (a.k.a. REINFORCE):\n",
    "\n",
    "\\begin{align}\n",
    "\\nabla_\\lambda \\text{ELBO} &=\\nabla_\\lambda\\sum_{z} Q(z|x, \\lambda)\\log P(y|x,z,\\theta) - \\nabla_\\lambda \\underbrace{\\sum_{i=1}^n \\text{KL}(Q(z_i|x, \\lambda) || P(z_i|p_1))}_{ \\color{blue}{\\text{tractable} }}  \\\\\n",
    "&=\\sum_{z} \\nabla_\\lambda Q(z|x, \\lambda)\\log P(y|x,z,\\theta) - \\sum_{i=1}^n \\nabla_\\lambda \\text{KL}(Q(z_i|x, \\lambda) || P(z_i|p_1))   \\\\\n",
    "&=\\sum_{z}  \\underbrace{Q(z|x, \\lambda) \\nabla_\\lambda \\log Q(z|x, \\lambda)}_{\\nabla_\\lambda Q(z|x, \\lambda)} \\log P(y|x,z,\\theta) - \\sum_{i=1}^n \\nabla_\\lambda \\text{KL}(Q(z_i|x, \\lambda) || P(z_i|p_1))   \\\\\n",
    "&= \\mathbb E_{Q(z|x, \\lambda)}\\left[ \\log P(y|x,z,\\theta) \\nabla_\\lambda \\log Q(z|x, \\lambda) \\right] - \\sum_{i=1}^n \\nabla_\\lambda \\text{KL}(Q(z_i|x, \\lambda) || P(z_i|p_1))   \\\\\n",
    "&\\overset{\\text{MC}}{\\approx} \\left(\\frac{1}{S} \\sum_{s=1}^S  \\log P(y|x, z^{(s)}, \\theta) \\nabla_\\lambda \\log Q(z^{(s)}|x, \\lambda)  \\right) - \\sum_{i=1}^n \\nabla_\\lambda \\text{KL}(Q(z_i|x, \\lambda) || P(z_i|p_1))  \n",
    "\\end{align}\n",
    "\n",
    "where $z^{(s)} \\sim Q(z|x,\\lambda)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6cdfkOYdC0LQ"
   },
   "source": [
    "## Implementation\n",
    "\n",
    "Let's implement the model and the loss (negative ELBO). We work with the notion of a *surrogate loss*, that is, a computation node whose gradients wrt to parameters are equivalent to the gradients we need.\n",
    "\n",
    "For a given sample $z \\sim Q(z|x, \\lambda)$, the following is a single-sample surrogate loss:\n",
    "\n",
    "\\begin{align}\n",
    "\\mathcal S(\\theta, \\lambda|x, y) = \\log P(y|x, z, \\theta) + \\color{red}{\\text{detach}(\\log P(y|x, z, \\theta) )}\\log Q(z|x, \\lambda) - \\sum_{i=1}^n \\text{KL}(Q(z_i|x, \\lambda) || P(z_i|\\phi))\n",
    "\\end{align}\n",
    "where we introduce an auxiliary function such that\n",
    "\\begin{align}\n",
    "\\text{detach}(f(\\alpha))  &= h(\\alpha) \\\\\n",
    "\\nabla_\\beta \\text{detach}(h(\\alpha))  &= 0 \n",
    "\\end{align}\n",
    "or in words, *detach* does not alter the forward call of its argument function $h$, but it alters $h$'s backward call by setting gradients to zero.\n",
    "\n",
    "Show that it's gradients wrt $\\theta$ and $\\lambda$ are exactly what we need:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FednEChaX6WI"
   },
   "source": [
    "\\begin{align}\n",
    "\\nabla_\\theta\\mathcal S(\\theta,\\lambda|x,y)=\\nabla_\\theta\\log P(y|x,z,\\theta)+0=\\nabla_\\theta\\text{ELBO}\n",
    "\\end{align}$$$$\\begin{align}\n",
    "\\nabla_\\lambda \\mathcal S(\\theta, \\lambda|x, y)= 0 + \\underbrace{\\log Q(z|x, \\lambda)\\nabla_\\lambda \\log P(y|x, z, \\theta)  + \\log P(y|x, z, \\theta) \\nabla_\\lambda \\log Q(z|x, \\lambda)}_{\\text{chain rule}}-\\sum_{i=1}^n \\nabla_\\lambda \\text{KL}(Q(z_i|x, \\lambda) || P(z_i|p_1))=\\\\ \n",
    "= 0+ 0 + \\log P(y|x, z, \\theta) \\nabla_\\lambda \\log Q(z|x, \\lambda)-\\sum_{i=1}^n \\nabla_\\lambda \\text{KL}(Q(z_i|x, \\lambda) || P(z_i|p_1))=\\nabla_\\lambda\\text{ELBO}\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OaUMKDShx9T0"
   },
   "source": [
    "Implement the forward pass and loss below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Cnwwk-7tfR02"
   },
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    \"\"\"\n",
    "    \n",
    "    Classifier model:\n",
    "        Z_i ~ Bern(p_1) for i in 1..n\n",
    "        Y|x,z ~ Cat(f([x_i if z_i 1 else 0 for i in 1..n ]))\n",
    "    \n",
    "    Inference model:\n",
    "        Z_i|x ~ Bern(b_i) for i in 1..n\n",
    "            where b_i = g_i(x)\n",
    "    \n",
    "    Objective:\n",
    "        Single-sample MC estimate of ELBO\n",
    "    \n",
    "    Loss: \n",
    "        Surrogate loss\n",
    "\n",
    "    Consists of:\n",
    "        - a product of Bernoulli distributions inference network\n",
    "        - a classifier network\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 vocab: object = None,\n",
    "                 vocab_size: int = 0,\n",
    "                 emb_size: int = 200,\n",
    "                 hidden_size: int = 200,\n",
    "                 num_classes: int = 5,\n",
    "                 prior_p1: float = 0.3,                 \n",
    "                 det_prior: bool = True,\n",
    "                 beta_shape: list = [0.6, 0.6],\n",
    "                 dropout: float = 0.1,\n",
    "                 layer_cls: str = 'bow',\n",
    "                 layer_inf: str = 'bow'):\n",
    "        \"\"\"\n",
    "        :param vocab: Vocabulary\n",
    "        :param vocab_size: necessary for embedding layer\n",
    "        :param emb_size: dimensionality of embedding layer\n",
    "        :param hidden_size: dimensionality of hidden layers\n",
    "        :param num_classes: number of classes\n",
    "        :param prior_p1: (scalar) prior Bernoulli parameter\n",
    "        :param det_prior: (boolean) whether the prior parameter is deterministic\n",
    "        :param beta_shape: (pair of positive scalars) \n",
    "            when the prior parameter is stochastic\n",
    "            it is sampled from a Beta distribution (ignore this at first)\n",
    "        :param dropout: (scalar) dropout rate\n",
    "        :param layer_cls: type of encoder for classification\n",
    "        :param layer_inf: type of encoder for inference\n",
    "        \"\"\"\n",
    "        super(Model, self).__init__()\n",
    "\n",
    "        self.vocab = vocab\n",
    "        self.embed = embed = nn.Embedding(vocab_size, emb_size, padding_idx=1)\n",
    "\n",
    "        self.cls_net = Classifier(\n",
    "            embed=embed, \n",
    "            hidden_size=hidden_size, \n",
    "            output_size=num_classes,\n",
    "            dropout=dropout, \n",
    "            layer=layer_cls\n",
    "        )\n",
    "        \n",
    "        self.inference_net = ProductOfBernoullis(\n",
    "            embed=embed, \n",
    "            hidden_size=hidden_size,\n",
    "            layer=layer_inf\n",
    "        )\n",
    "        \n",
    "        self._prior_p1 = prior_p1\n",
    "        self._det_prior = det_prior\n",
    "        self._beta_shape = beta_shape\n",
    "        \n",
    "    def get_prior_p1(self, p_min=0.001, p_max=0.999):\n",
    "        \"\"\"Return the prior Bernoulli parameter\"\"\"\n",
    "        if self._det_prior:\n",
    "            return torch.tensor(self._prior_p1)\n",
    "        else:\n",
    "            a, b = self._beta_shape\n",
    "            prior_p1 = np.random.beta(a, b)\n",
    "            prior_p1 = max(prior_p1, p_min)\n",
    "            prior_p1 = min(prior_p1, p_max)\n",
    "        return torch.tensor(prior_p1)\n",
    "\n",
    "    def predict(self, py: Categorical, **kwargs):\n",
    "        \"\"\"\n",
    "        Predict deterministically using argmax.\n",
    "        :param py: B Categorical distributions (one per instance in batch)\n",
    "        :return: predictions\n",
    "            [B] sentiment levels\n",
    "        \"\"\"\n",
    "        assert not self.training, \"should be in eval mode for prediction\"\n",
    "        return py.log_probs.argmax(-1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Generate a sequence z with inference model, \n",
    "         then predict with rationale xz, that is, x masked by z.\n",
    "\n",
    "        :param x: [B, T] documents        \n",
    "        :param mask: [B, T] indicates valid positions vs padded positions\n",
    "        :return: \n",
    "            Categorical distributions P(y|x, z)\n",
    "            Bernoulli distributions Q(z|x)\n",
    "            Single sample z ~ Q(z|x) used for the conditional P(y|x, z)\n",
    "        \"\"\"\n",
    "        bern = self.inference_net(x, x != 1)\n",
    "        z = bern.sample()\n",
    "        res = self.cls_net(x, x != 1, z)\n",
    "        return res, bern, z\n",
    "\n",
    "    def get_loss(self,                   \n",
    "                 y, \n",
    "                 py: Categorical,\n",
    "                 qz: Bernoulli, \n",
    "                 z, \n",
    "                 mask,\n",
    "                 iter_i=0, \n",
    "                 # you may ignore the rest of the arguments for the time being\n",
    "                 #  leave them as they are\n",
    "                 kl_weight=1.0,\n",
    "                 min_kl=0.0,\n",
    "                 ll_mean=0.,\n",
    "                 ll_std=1.,\n",
    "                 **kwargs):\n",
    "        \"\"\"\n",
    "        This computes the loss for the whole model.\n",
    "\n",
    "        :param y: target labels [B]\n",
    "        :param py: conditionals P(y|x, z)\n",
    "        :param qz: approximate posteriors Q(z|x)\n",
    "        :param z: sample of binary selectors [B, T]\n",
    "        :param mask: indicates valid positions [B, T]\n",
    "        :param iter_i: indicates the iteration\n",
    "        :param kl_weight: (scalar) multiplies the KL term\n",
    "        :param min_kl: (scalar) sets a minimum for the KL (aka free bits)\n",
    "        :param ll_mean: (scalar) running average of reward\n",
    "        :param ll_std: (scalar) running standard deviation of reward\n",
    "        :return: loss (torch node), terms (dict)\n",
    "        \n",
    "            terms is an OrderedDict that holds the scalar items involved in the loss\n",
    "            e.g. `terms['ll'] = ll.item()` is the log-likelihood term\n",
    "            \n",
    "            Consider tracking the following:\n",
    "            Single-sample ELBO: terms['elbo']\n",
    "            Log-Likelihood log P(y|x,z): terms['ll']\n",
    "            KL: terms['kl']\n",
    "            Score function surrogate log P(y|z, x) log Q(z|x): terms['sf']            \n",
    "            Rate of selected words: terms['selected']\n",
    "        \"\"\"\n",
    "        float_mask = mask.float()\n",
    "        \n",
    "        ll = (py.log_pmf(y).unsqueeze(-1) - ll_mean) * float_mask / ll_std\n",
    "        kl = qz.kl(Bernoulli(probs=self.get_prior_p1())) * float_mask\n",
    "        sf = qz.log_pmf(z.float()) * ll.detach() * float_mask\n",
    "        elbo = ll + sf - kl_weight * kl\n",
    "        \n",
    "        terms = OrderedDict()\n",
    "        terms['elbo'] = elbo.mean().item()\n",
    "        terms['sf'] = sf.mean().item()\n",
    "        terms['selected'] = z.sum().item() * 1.0 / z.shape[0] / z.shape[1]\n",
    "        terms['kl'] = kl.mean().item()\n",
    "        terms['ll'] = ll.mean().item()\n",
    "        \n",
    "        return -elbo.mean(), terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wNQDXTpqWvZa"
   },
   "outputs": [],
   "source": [
    "# This will be used later for maintaining runnin averages of quantites like \n",
    "#  terms in the ELBO\n",
    "from collections import deque\n",
    "\n",
    "class MovingStats:\n",
    "    \n",
    "    def __init__(self, memory=-1):\n",
    "        self.data = deque([])\n",
    "        self.memory = memory\n",
    "        \n",
    "    def append(self, value):\n",
    "        if self.memory != 0:\n",
    "            if self.memory > 0 and len(self.data) == self.memory:\n",
    "                self.data.popleft()\n",
    "            self.data.append(value)\n",
    "        \n",
    "    def mean(self):\n",
    "        if len(self.data):\n",
    "            return np.mean([x for x in self.data])\n",
    "        else:\n",
    "            return 0.\n",
    "    \n",
    "    def std(self):\n",
    "        return np.std(self.data) if len(self.data) > 1 else 1.\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "081YSfU9WvZc"
   },
   "source": [
    "# Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9Pc80gseWvZd"
   },
   "outputs": [],
   "source": [
    "# some helper code for mini batching\n",
    "#  this will take care of annoying things such as \n",
    "#  sorting training instances by length (necessary for pytorch's LSTM, for example)\n",
    "from sst.util import make_kv_string, get_minibatch, prepare_minibatch, print_parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_WVr97kilIRV"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Configuration\n",
      "training_path        : ../sst/data/sst/train.txt\n",
      "dev_path             : ../sst/data/sst/dev.txt\n",
      "test_path            : ../sst/data/sst/test.txt\n",
      "word_vectors         : ../sst/data/sst/glove.840B.300d.filtered.txt\n",
      "prior_p1             :        0.3\n",
      "beta_a               :        0.6\n",
      "beta_b               :        0.6\n",
      "det_prior            :          1\n",
      "num_epochs           :         50\n",
      "print_every          :        100\n",
      "eval_every           :         -1\n",
      "batch_size           :         25\n",
      "eval_batch_size      :         25\n",
      "subphrases           :          0\n",
      "min_phrase_length    :          2\n",
      "lowercase            :          1\n",
      "fix_emb              :          1\n",
      "embed_size           :        300\n",
      "hidden_size          :        150\n",
      "num_layers           :          1\n",
      "dropout              :        0.5\n",
      "layer_inf            : bow       \n",
      "layer_cls            : bow       \n",
      "save_path            : data/results\n",
      "baseline_memory      :       1000\n",
      "min_kl               :        0.0\n",
      "kl_weight            :        0.0\n",
      "kl_inc               :      1e-05\n",
      "lr                   :      0.002\n",
      "weight_decay         :      1e-05\n",
      "lr_decay             :        0.5\n",
      "patience             :          5\n",
      "cooldown             :          5\n",
      "threshold            :     0.0001\n",
      "min_lr               :      1e-05\n",
      "max_grad_norm        :        5.0\n",
      "Set eval_every to 341\n",
      "Loading data\n",
      "train 8544\n",
      "dev 1101\n",
      "test 2210\n",
      "\n",
      "# Example\n",
      "First dev example: Example(tokens=['it', \"'s\", 'a', 'lovely', 'film', 'with', 'lovely', 'performances', 'by', 'buy', 'and', 'accorsi', '.'], label=3, transitions=[0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1], token_labels=[2, 2, 2, 3, 2, 2, 3, 2, 2, 2, 2, 2, 2])\n",
      "First dev example tokens: ['it', \"'s\", 'a', 'lovely', 'film', 'with', 'lovely', 'performances', 'by', 'buy', 'and', 'accorsi', '.']\n",
      "First dev example label: 3\n"
     ]
    }
   ],
   "source": [
    "import torch.optim\n",
    "# We will use Adam\n",
    "from torch.optim import Adam\n",
    "# and a couple of tricks to reduce learning rate on plateau\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "# here is some helper code to evaluate your model\n",
    "from sst.evaluate import evaluate\n",
    "\n",
    "\n",
    "cfg = dict()\n",
    "\n",
    "# Data\n",
    "cfg['training_path'] = \"../sst/data/sst/train.txt\"\n",
    "cfg['dev_path'] = \"../sst/data/sst/dev.txt\"\n",
    "cfg['test_path'] = \"../sst/data/sst/test.txt\"\n",
    "cfg['word_vectors'] = '../sst/data/sst/glove.840B.300d.filtered.txt'\n",
    "# Model\n",
    "cfg['prior_p1'] = 0.3\n",
    "cfg['beta_a'] = 0.6\n",
    "cfg['beta_b'] = 0.6\n",
    "cfg['det_prior'] = True\n",
    "# Architecture\n",
    "cfg['num_epochs'] = 50\n",
    "cfg['print_every'] = 100\n",
    "cfg['eval_every'] = -1\n",
    "cfg['batch_size'] = 25\n",
    "cfg['eval_batch_size'] = 25\n",
    "cfg['subphrases'] = False\n",
    "cfg['min_phrase_length'] = 2\n",
    "cfg['lowercase'] = True\n",
    "cfg['fix_emb'] = True\n",
    "cfg['embed_size'] = 300\n",
    "cfg['hidden_size'] = 150\n",
    "cfg['num_layers'] = 1\n",
    "cfg['dropout'] = 0.5\n",
    "cfg['layer_inf'] = 'bow'\n",
    "cfg['layer_cls'] = 'bow'\n",
    "cfg['save_path'] = 'data/results'\n",
    "cfg['baseline_memory'] = 1000\n",
    "cfg['min_kl'] = 0.  # use more than 0 to enable free bits\n",
    "cfg['kl_weight'] = 0.0  # start from zero to enable annealing\n",
    "cfg['kl_inc'] = 0.00001\n",
    "# Optimiser (leave as is)\n",
    "cfg['lr'] = 0.0002 * 10\n",
    "cfg['weight_decay'] = 1e-5\n",
    "cfg['lr_decay'] = 0.5\n",
    "cfg['patience'] = 5\n",
    "cfg['cooldown'] = 5\n",
    "cfg['threshold'] = 1e-4\n",
    "cfg['min_lr'] = 1e-5\n",
    "cfg['max_grad_norm'] = 5.\n",
    "\n",
    "\n",
    "print('# Configuration')\n",
    "for k, v in cfg.items():\n",
    "    print(\"{:20} : {:10}\".format(k, v))\n",
    "\n",
    "\n",
    "iters_per_epoch = len(train_data) // cfg[\"batch_size\"]\n",
    "\n",
    "if cfg[\"eval_every\"] == -1:\n",
    "    eval_every = iters_per_epoch\n",
    "    print(\"Set eval_every to {}\".format(iters_per_epoch))\n",
    "\n",
    "\n",
    "# Let's load the data into memory.\n",
    "print(\"Loading data\")\n",
    "train_data = list(examplereader(\n",
    "    cfg['training_path'],\n",
    "    lower=cfg['lowercase'], \n",
    "    subphrases=cfg['subphrases'],\n",
    "    min_length=cfg['min_phrase_length']))\n",
    "dev_data = list(examplereader(cfg['dev_path'], lower=cfg['lowercase']))\n",
    "test_data = list(examplereader(cfg['test_path'], lower=cfg['lowercase']))\n",
    "\n",
    "print(\"train\", len(train_data))\n",
    "print(\"dev\", len(dev_data))\n",
    "print(\"test\", len(test_data))\n",
    "\n",
    "print('\\n# Example')\n",
    "example = dev_data[0]\n",
    "print(\"First dev example:\", example)\n",
    "print(\"First dev example tokens:\", example.tokens)\n",
    "print(\"First dev example label:\", example.label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6PMqtVj0WvZf",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def train():\n",
    "\n",
    "    # Create a vocabulary object to map str <-> int\n",
    "    vocab = Vocabulary()  # populated by load_glove\n",
    "    glove_path = cfg[\"word_vectors\"]\n",
    "    vectors = load_glove(glove_path, vocab)\n",
    "\n",
    "    # You may consider using tensorboardX\n",
    "    # writer = SummaryWriter(log_dir=cfg[\"save_path\"])\n",
    "\n",
    "    # Map the sentiment labels 0-4 to a more readable form (and the opposite)\n",
    "    i2t = [\"very negative\", \"negative\", \"neutral\", \"positive\", \"very positive\"]\n",
    "    t2i = OrderedDict({p: i for p, i in zip(i2t, range(len(i2t)))})\n",
    "\n",
    "\n",
    "    print('\\n# Constructing model')\n",
    "    model = Model(\n",
    "        vocab_size=len(vocab.w2i), \n",
    "        emb_size=cfg[\"embed_size\"],\n",
    "        hidden_size=cfg[\"hidden_size\"], \n",
    "        num_classes=len(t2i),\n",
    "        prior_p1=cfg['prior_p1'],\n",
    "        det_prior=cfg['det_prior'],\n",
    "        beta_shape=[cfg['beta_a'], cfg['beta_b']],\n",
    "        vocab=vocab, \n",
    "        dropout=cfg[\"dropout\"], \n",
    "        layer_cls=cfg[\"layer_cls\"],\n",
    "        layer_inf=cfg[\"layer_inf\"]\n",
    "    )\n",
    "\n",
    "    print('\\n# Loading embeddings')\n",
    "    with torch.no_grad():\n",
    "        model.embed.weight.data.copy_(torch.from_numpy(vectors))\n",
    "        if cfg[\"fix_emb\"]:\n",
    "            print(\"fixed word embeddings\")\n",
    "            model.embed.weight.requires_grad = False\n",
    "        model.embed.weight[1] = 0.  # padding zero\n",
    "\n",
    "        \n",
    "    # Congigure optimiser\n",
    "    optimizer = Adam(model.parameters(), lr=cfg[\"lr\"],\n",
    "                     weight_decay=cfg[\"weight_decay\"])\n",
    "    # and learning rate scheduler\n",
    "    scheduler = ReduceLROnPlateau(\n",
    "        optimizer, mode=\"min\", factor=cfg[\"lr_decay\"], patience=cfg[\"patience\"],\n",
    "        verbose=True, cooldown=cfg[\"cooldown\"], threshold=cfg[\"threshold\"],\n",
    "        min_lr=cfg[\"min_lr\"]\n",
    "    )\n",
    "\n",
    "    # Prepare a few auxiliary variables\n",
    "    iter_i = 0\n",
    "    train_loss = 0.\n",
    "    print_num = 0\n",
    "    losses = []\n",
    "    accuracies = []\n",
    "    best_eval = 1.0e9\n",
    "    best_iter = 0\n",
    "\n",
    "    model = model.to(device)\n",
    "\n",
    "    # Some debugging info\n",
    "    print(model)\n",
    "    print_parameters(model)\n",
    "\n",
    "    batch_size = cfg['batch_size']\n",
    "    eval_batch_size = cfg['eval_batch_size']\n",
    "    print_every = cfg['print_every']\n",
    "\n",
    "    # Parameters of tricks to better optimise the ELBO \n",
    "    kl_inc = cfg['kl_inc']\n",
    "    kl_weight = cfg['kl_weight']\n",
    "    min_kl = cfg['min_kl']\n",
    "    # Running estimates for baselines\n",
    "    ll_moving_stats = MovingStats(cfg['baseline_memory'])\n",
    "\n",
    "    while True:  # when we run out of examples, shuffle and continue\n",
    "        epoch = iter_i // iters_per_epoch\n",
    "        if epoch > cfg['num_epochs']:\n",
    "            break\n",
    "        \n",
    "        for batch in get_minibatch(train_data, batch_size=batch_size, shuffle=True):\n",
    "\n",
    "            epoch = iter_i // iters_per_epoch\n",
    "            if epoch > cfg['num_epochs']:\n",
    "                break\n",
    "\n",
    "            # forward pass\n",
    "            model.train()\n",
    "            x, y, _ = prepare_minibatch(batch, model.vocab, device=device)\n",
    "            \n",
    "            mask = (x != 1)\n",
    "            py, qz, z = model(x)\n",
    "\n",
    "            # \"KL annealing\"\n",
    "            kl_weight += kl_inc\n",
    "            if kl_weight > 1.:\n",
    "                kl_weight = 1.0\n",
    "                \n",
    "            loss, terms = model.get_loss(\n",
    "                y,\n",
    "                py=py, \n",
    "                qz=qz,\n",
    "                z=z,\n",
    "                mask=mask, \n",
    "                kl_weight=kl_weight,\n",
    "                min_kl=min_kl,\n",
    "                ll_mean=ll_moving_stats.mean(),\n",
    "                ll_std=ll_moving_stats.std(),\n",
    "                iter_i=iter_i)\n",
    "\n",
    "            train_loss += loss.item()\n",
    "            \n",
    "            # keep an running estimate of the reward (log P(y|x,z))\n",
    "            ll_moving_stats.append(terms['ll'])\n",
    "\n",
    "            # backward pass\n",
    "            model.zero_grad()  # erase previous gradients\n",
    "\n",
    "            loss.backward()  # compute new gradients\n",
    "\n",
    "            # gradient clipping generally helps\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=cfg['max_grad_norm'])\n",
    "\n",
    "            # update weights\n",
    "            optimizer.step()\n",
    "\n",
    "            print_num += 1\n",
    "            iter_i += 1\n",
    "\n",
    "            # print info\n",
    "            if iter_i % print_every == 0:\n",
    "\n",
    "                train_loss = train_loss / print_every\n",
    "\n",
    "                print_str = make_kv_string(terms)\n",
    "                print(\"Epoch %r Iter %r loss=%.4f %s\" %\n",
    "                      (epoch, iter_i, train_loss, print_str))\n",
    "                losses.append(train_loss)\n",
    "                print_num = 0\n",
    "                train_loss = 0.\n",
    "\n",
    "            # evaluate\n",
    "            if iter_i % eval_every == 0:\n",
    "\n",
    "                dev_eval, rationales = evaluate(\n",
    "                    model, dev_data, \n",
    "                    batch_size=eval_batch_size, \n",
    "                    device=device,\n",
    "                    cfg=cfg, iter_i=iter_i\n",
    "                )\n",
    "                accuracies.append(dev_eval[\"acc\"])\n",
    "\n",
    "                print(\"\\n# epoch %r iter %r: dev %s\" % (\n",
    "                    epoch, iter_i, make_kv_string(dev_eval)))\n",
    "                \n",
    "                for exid in range(3):\n",
    "                    print(' dev%d [gold=%d,pred=%d]:' % (exid, dev_data[exid].label, rationales[exid][1]),  \n",
    "                          ' '.join(rationales[exid][0]))\n",
    "                print()\n",
    "\n",
    "                # adjust learning rate\n",
    "                scheduler.step(dev_eval[\"loss\"])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "A5uYKcw-WvZl",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "# Constructing model\n",
      "Classifier #params: 1505\n",
      "ProductOfBernoullis #params: 301\n",
      "\n",
      "# Loading embeddings\n",
      "fixed word embeddings\n",
      "Model(\n",
      "  (embed): Embedding(20727, 300, padding_idx=1)\n",
      "  (cls_net): Classifier(\n",
      "    (embed_layer): Sequential(\n",
      "      (0): Embedding(20727, 300, padding_idx=1)\n",
      "    )\n",
      "    (enc_layer): BagOfWordsEncoder()\n",
      "    (output_layer): Sequential(\n",
      "      (0): Dropout(p=0.5)\n",
      "      (1): Linear(in_features=300, out_features=5, bias=True)\n",
      "      (2): LogSoftmax()\n",
      "    )\n",
      "  )\n",
      "  (inference_net): ProductOfBernoullis(\n",
      "    (embed_layer): Sequential(\n",
      "      (0): Embedding(20727, 300, padding_idx=1)\n",
      "    )\n",
      "    (enc_layer): BagOfWordsEncoder()\n",
      "    (logit_layer): Linear(in_features=300, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "embed.weight             [20727, 300] requires_grad=False\n",
      "cls_net.output_layer.1.weight [5, 300]     requires_grad=True\n",
      "cls_net.output_layer.1.bias [5]          requires_grad=True\n",
      "inference_net.logit_layer.weight [1, 300]     requires_grad=True\n",
      "inference_net.logit_layer.bias [1]          requires_grad=True\n",
      "\n",
      "Total parameters: 6219906\n",
      "\n",
      "Shuffling training data\n",
      "Epoch 0 Iter 100 loss=0.3367 elbo -0.2814 sf 0.5893 selected 0.4800 kl 0.0206 ll -0.8707\n",
      "Epoch 0 Iter 200 loss=0.3544 elbo -0.3783 sf 0.8502 selected 0.4617 kl 0.0158 ll -1.2285\n",
      "Epoch 0 Iter 300 loss=0.3900 elbo -0.3470 sf 0.5633 selected 0.4074 kl 0.0151 ll -0.9102\n",
      "\n",
      "# epoch 0 iter 341: dev loss 0.3025 elbo -0.3025 sf 0.5104 selected 0.4491 kl 0.0278 ll -0.7851 acc 0.3243\n",
      " dev0 [gold=3,pred=1]: it 's a lovely **film** **with** **lovely** **performances** by **buy** and **accorsi** .\n",
      " dev1 [gold=2,pred=3]: no one goes **unindicted** **here** **,** which is **probably** for the best .\n",
      " dev2 [gold=3,pred=1]: and if **you** **'re** **not** nearly moved **to** **tears** by a couple of scenes **,** **you** **'ve** got ice **water** in your veins .\n",
      "\n",
      "Shuffling training data\n",
      "Epoch 1 Iter 400 loss=0.3748 elbo -0.3560 sf 0.7109 selected 0.4925 kl 0.0487 ll -1.0667\n",
      "Epoch 1 Iter 500 loss=0.3468 elbo -0.2262 sf 0.4713 selected 0.5289 kl 0.0706 ll -0.6971\n",
      "Epoch 1 Iter 600 loss=0.3396 elbo -0.2135 sf 0.4110 selected 0.5413 kl 0.1125 ll -0.6239\n",
      "\n",
      "# epoch 1 iter 682: dev loss 0.4225 elbo -0.4225 sf 0.4616 selected 0.5741 kl 0.1363 ll -0.7478 acc 0.3733\n",
      " dev0 [gold=3,pred=1]: it **'s** **a** **lovely** film **with** **lovely** performances **by** **buy** **and** accorsi **.**\n",
      " dev1 [gold=2,pred=3]: **no** one **goes** unindicted **here** , **which** **is** probably for the **best** **.**\n",
      " dev2 [gold=3,pred=1]: **and** if **you** **'re** **not** nearly moved to **tears** by a **couple** of **scenes** **,** **you** **'ve** **got** ice **water** **in** **your** **veins** **.**\n",
      "\n",
      "Shuffling training data\n",
      "Epoch 2 Iter 700 loss=0.3369 elbo -0.3602 sf 0.5799 selected 0.5917 kl 0.1279 ll -0.9392\n",
      "Epoch 2 Iter 800 loss=0.3848 elbo -0.3643 sf 0.5149 selected 0.5758 kl 0.1551 ll -0.8779\n",
      "Epoch 2 Iter 900 loss=0.3647 elbo -0.3353 sf 0.5230 selected 0.5947 kl 0.1683 ll -0.8568\n",
      "Epoch 2 Iter 1000 loss=0.3948 elbo -0.3768 sf 0.3859 selected 0.6217 kl 0.1734 ll -0.7609\n",
      "\n",
      "# epoch 2 iter 1023: dev loss 0.5334 elbo -0.5334 sf 0.4088 selected 0.6254 kl 0.2077 ll -0.7346 acc 0.3806\n",
      " dev0 [gold=3,pred=1]: it **'s** a **lovely** **film** **with** **lovely** **performances** **by** **buy** **and** accorsi **.**\n",
      " dev1 [gold=2,pred=3]: **no** **one** **goes** unindicted here **,** which **is** probably for the **best** **.**\n",
      " dev2 [gold=3,pred=1]: **and** **if** **you** **'re** **not** nearly moved **to** **tears** by a **couple** **of** **scenes** **,** **you** **'ve** **got** **ice** **water** **in** **your** **veins** **.**\n",
      "\n",
      "Shuffling training data\n",
      "Epoch 3 Iter 1100 loss=0.4106 elbo -0.3640 sf 0.4317 selected 0.5920 kl 0.1893 ll -0.7936\n",
      "Epoch 3 Iter 1200 loss=0.4013 elbo -0.4537 sf 0.6220 selected 0.6130 kl 0.1917 ll -1.0733\n",
      "Epoch 3 Iter 1300 loss=0.4209 elbo -0.3268 sf 0.4680 selected 0.5882 kl 0.1448 ll -0.7929\n",
      "\n",
      "# epoch 3 iter 1364: dev loss 0.5782 elbo -0.5782 sf 0.3730 selected 0.6362 kl 0.2334 ll -0.7178 acc 0.4033\n",
      " dev0 [gold=3,pred=1]: **it** **'s** **a** **lovely** **film** **with** **lovely** performances by **buy** and **accorsi** **.**\n",
      " dev1 [gold=2,pred=3]: **no** **one** **goes** unindicted here **,** which **is** **probably** for the best .\n",
      " dev2 [gold=3,pred=1]: **and** **if** **you** **'re** **not** **nearly** moved **to** **tears** by a **couple** of **scenes** **,** **you** **'ve** got ice **water** **in** **your** veins .\n",
      "\n",
      "Shuffling training data\n",
      "Epoch 4 Iter 1400 loss=0.4323 elbo -0.6760 sf 0.5839 selected 0.6541 kl 0.2292 ll -1.2566\n",
      "Epoch 4 Iter 1500 loss=0.4620 elbo -0.2735 sf 0.3831 selected 0.5746 kl 0.1336 ll -0.6546\n",
      "Epoch 4 Iter 1600 loss=0.3974 elbo -0.4763 sf 0.6453 selected 0.5344 kl 0.0884 ll -1.1202\n",
      "Epoch 4 Iter 1700 loss=0.3863 elbo -0.3724 sf 0.4550 selected 0.5384 kl 0.1195 ll -0.8254\n",
      "\n",
      "# epoch 4 iter 1705: dev loss 0.4274 elbo -0.4274 sf 0.4227 selected 0.5395 kl 0.1306 ll -0.7194 acc 0.3969\n",
      " dev0 [gold=3,pred=1]: it 's a **lovely** film **with** **lovely** **performances** by **buy** **and** **accorsi** .\n",
      " dev1 [gold=2,pred=3]: no one goes **unindicted** **here** , which **is** **probably** for the **best** **.**\n",
      " dev2 [gold=3,pred=1]: and **if** **you** **'re** **not** **nearly** **moved** **to** **tears** by a couple of scenes , **you** 've got ice **water** in **your** **veins** **.**\n",
      "\n",
      "Shuffling training data\n",
      "Epoch 5 Iter 1800 loss=0.3850 elbo -0.3170 sf 0.4433 selected 0.5528 kl 0.1226 ll -0.7581\n",
      "Epoch 5 Iter 1900 loss=0.3785 elbo -0.4179 sf 0.4955 selected 0.6165 kl 0.2090 ll -0.9095\n",
      "Epoch 5 Iter 2000 loss=0.4002 elbo -0.4586 sf 0.5339 selected 0.5770 kl 0.1853 ll -0.9888\n",
      "\n",
      "# epoch 5 iter 2046: dev loss 0.4816 elbo -0.4816 sf 0.3974 selected 0.5786 kl 0.1644 ll -0.7147 acc 0.3960\n",
      " dev0 [gold=3,pred=1]: **it** **'s** a **lovely** **film** with lovely **performances** **by** **buy** and accorsi **.**\n",
      " dev1 [gold=2,pred=3]: no **one** **goes** **unindicted** **here** , which is **probably** **for** the **best** .\n",
      " dev2 [gold=3,pred=1]: **and** **if** **you** 're **not** **nearly** **moved** **to** **tears** **by** a **couple** of **scenes** , **you** **'ve** got ice water in your **veins** .\n",
      "\n",
      "Shuffling training data\n",
      "Epoch 6 Iter 2100 loss=0.3895 elbo -0.3287 sf 0.5230 selected 0.5660 kl 0.1293 ll -0.8490\n",
      "Epoch 6 Iter 2200 loss=0.3869 elbo -0.2983 sf 0.4257 selected 0.5411 kl 0.1363 ll -0.7210\n",
      "Epoch 6 Iter 2300 loss=0.3852 elbo -0.5116 sf 0.6828 selected 0.5807 kl 0.1771 ll -1.1903\n",
      "\n",
      "# epoch 6 iter 2387: dev loss 0.4213 elbo -0.4213 sf 0.4090 selected 0.5297 kl 0.1189 ll -0.7115 acc 0.4005\n",
      " dev0 [gold=3,pred=1]: it **'s** **a** **lovely** film **with** **lovely** **performances** by **buy** **and** **accorsi** .\n",
      " dev1 [gold=2,pred=1]: **no** one **goes** unindicted **here** , which is probably **for** the **best** .\n",
      " dev2 [gold=3,pred=1]: and **if** **you** 're **not** **nearly** moved to **tears** by a **couple** **of** **scenes** , you 've got **ice** **water** **in** **your** veins **.**\n",
      "\n",
      "Epoch     6: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Shuffling training data\n",
      "Epoch 7 Iter 2400 loss=0.3747 elbo -0.4127 sf 0.5462 selected 0.5341 kl 0.1184 ll -0.9561\n",
      "Epoch 7 Iter 2500 loss=0.3979 elbo -0.5022 sf 0.6962 selected 0.5449 kl 0.1175 ll -1.1954\n",
      "Epoch 7 Iter 2600 loss=0.4002 elbo -0.3134 sf 0.4199 selected 0.5158 kl 0.1206 ll -0.7302\n",
      "Epoch 7 Iter 2700 loss=0.4036 elbo -0.6261 sf 0.8078 selected 0.5406 kl 0.1658 ll -1.4294\n",
      "\n",
      "# epoch 7 iter 2728: dev loss 0.4138 elbo -0.4138 sf 0.4116 selected 0.5240 kl 0.1138 ll -0.7115 acc 0.4087\n",
      " dev0 [gold=3,pred=1]: **it** 's a **lovely** **film** **with** lovely **performances** **by** **buy** **and** **accorsi** .\n",
      " dev1 [gold=2,pred=1]: **no** **one** goes unindicted here , **which** is probably for the **best** **.**\n",
      " dev2 [gold=3,pred=3]: and if **you** 're not nearly **moved** to tears by a couple of **scenes** , **you** 've got **ice** water in **your** veins .\n",
      "\n",
      "Shuffling training data\n",
      "Epoch 8 Iter 2800 loss=0.4094 elbo -0.3092 sf 0.4012 selected 0.4957 kl 0.0933 ll -0.7077\n",
      "Epoch 8 Iter 2900 loss=0.3831 elbo -0.6128 sf 0.7592 selected 0.5360 kl 0.1029 ll -1.3690\n",
      "Epoch 8 Iter 3000 loss=0.3907 elbo -0.3124 sf 0.3963 selected 0.5015 kl 0.0782 ll -0.7063\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "# epoch 8 iter 3069: dev loss 0.4139 elbo -0.4139 sf 0.3978 selected 0.4895 kl 0.0973 ll -0.7144 acc 0.3987\n",
      " dev0 [gold=3,pred=1]: it **'s** a **lovely** film with **lovely** **performances** **by** buy and accorsi .\n",
      " dev1 [gold=2,pred=1]: no **one** goes **unindicted** **here** , which **is** **probably** **for** the **best** .\n",
      " dev2 [gold=3,pred=1]: and **if** you 're **not** **nearly** moved to tears by a **couple** of **scenes** , **you** **'ve** got ice **water** in **your** **veins** .\n",
      "\n",
      "Shuffling training data\n",
      "Epoch 9 Iter 3100 loss=0.4085 elbo -0.3791 sf 0.3863 selected 0.4941 kl 0.0942 ll -0.7625\n",
      "Epoch 9 Iter 3200 loss=0.4011 elbo -0.3416 sf 0.4963 selected 0.5100 kl 0.1100 ll -0.8344\n",
      "Epoch 9 Iter 3300 loss=0.3858 elbo -0.5830 sf 0.6799 selected 0.5112 kl 0.1625 ll -1.2575\n",
      "Epoch 9 Iter 3400 loss=0.3636 elbo -0.3574 sf 0.5162 selected 0.5385 kl 0.1140 ll -0.8697\n",
      "\n",
      "# epoch 9 iter 3410: dev loss 0.4222 elbo -0.4222 sf 0.4097 selected 0.5307 kl 0.1248 ll -0.7071 acc 0.4051\n",
      " dev0 [gold=3,pred=1]: **it** **'s** a **lovely** **film** **with** **lovely** **performances** **by** **buy** and **accorsi** .\n",
      " dev1 [gold=2,pred=3]: **no** **one** goes **unindicted** here , which **is** **probably** for the **best** .\n",
      " dev2 [gold=3,pred=1]: **and** if **you** **'re** **not** **nearly** moved to **tears** **by** a **couple** of **scenes** , **you** **'ve** **got** **ice** **water** in **your** **veins** .\n",
      "\n",
      "Shuffling training data\n",
      "Epoch 10 Iter 3500 loss=0.3907 elbo -0.2581 sf 0.3622 selected 0.5143 kl 0.1102 ll -0.6165\n",
      "Epoch 10 Iter 3600 loss=0.3855 elbo -0.2766 sf 0.4946 selected 0.5112 kl 0.1095 ll -0.7672\n",
      "Epoch 10 Iter 3700 loss=0.3952 elbo -0.4283 sf 0.5210 selected 0.5189 kl 0.1218 ll -0.9448\n",
      "\n",
      "# epoch 10 iter 3751: dev loss 0.4191 elbo -0.4191 sf 0.4068 selected 0.5270 kl 0.1187 ll -0.7073 acc 0.4214\n",
      " dev0 [gold=3,pred=1]: **it** **'s** a **lovely** film **with** **lovely** **performances** by **buy** **and** **accorsi** .\n",
      " dev1 [gold=2,pred=3]: **no** one goes **unindicted** here , which is probably for **the** **best** .\n",
      " dev2 [gold=3,pred=1]: **and** **if** you **'re** not nearly **moved** **to** tears by a **couple** of scenes , you **'ve** got ice **water** in your **veins** .\n",
      "\n",
      "Shuffling training data\n",
      "Epoch 11 Iter 3800 loss=0.3783 elbo -0.3413 sf 0.4686 selected 0.5424 kl 0.1252 ll -0.8051\n",
      "Epoch 11 Iter 3900 loss=0.4179 elbo -0.4554 sf 0.5384 selected 0.5200 kl 0.1192 ll -0.9892\n",
      "Epoch 11 Iter 4000 loss=0.4032 elbo -0.3204 sf 0.4460 selected 0.4823 kl 0.0988 ll -0.7624\n",
      "\n",
      "# epoch 11 iter 4092: dev loss 0.4187 elbo -0.4187 sf 0.3858 selected 0.4693 kl 0.0929 ll -0.7115 acc 0.4024\n",
      " dev0 [gold=3,pred=1]: it 's a **lovely** **film** **with** **lovely** **performances** by buy **and** accorsi .\n",
      " dev1 [gold=2,pred=1]: **no** one **goes** **unindicted** **here** , which is **probably** for the best .\n",
      " dev2 [gold=3,pred=1]: and if you **'re** **not** nearly moved to **tears** by a **couple** of **scenes** , you 've **got** ice water in **your** **veins** .\n",
      "\n",
      "Epoch 12 Iter 4100 loss=0.4315 elbo -0.4279 sf 0.5216 selected 0.4620 kl 0.0799 ll -0.9462\n",
      "Shuffling training data\n",
      "Epoch 12 Iter 4200 loss=0.4169 elbo -0.3163 sf 0.3259 selected 0.5054 kl 0.0948 ll -0.6383\n",
      "Epoch 12 Iter 4300 loss=0.4174 elbo -0.4351 sf 0.4330 selected 0.5151 kl 0.0691 ll -0.8652\n",
      "Epoch 12 Iter 4400 loss=0.4224 elbo -0.3389 sf 0.4094 selected 0.4872 kl 0.0770 ll -0.7449\n",
      "\n",
      "# epoch 12 iter 4433: dev loss 0.4291 elbo -0.4291 sf 0.3801 selected 0.4767 kl 0.0987 ll -0.7104 acc 0.3933\n",
      " dev0 [gold=3,pred=1]: it **'s** a **lovely** film **with** **lovely** **performances** by **buy** and **accorsi** .\n",
      " dev1 [gold=2,pred=1]: **no** **one** **goes** unindicted here , which is **probably** for the best .\n",
      " dev2 [gold=3,pred=1]: and **if** you **'re** not nearly moved to **tears** by **a** **couple** of scenes , you **'ve** **got** ice water **in** your **veins** .\n",
      "\n",
      "Shuffling training data\n",
      "Epoch 13 Iter 4500 loss=0.4165 elbo -0.4268 sf 0.4805 selected 0.5031 kl 0.0977 ll -0.9029\n",
      "Epoch 13 Iter 4600 loss=0.4302 elbo -0.3508 sf 0.3954 selected 0.4967 kl 0.1078 ll -0.7412\n",
      "Epoch 13 Iter 4700 loss=0.3992 elbo -0.3616 sf 0.4343 selected 0.5320 kl 0.0957 ll -0.7914\n",
      "\n",
      "# epoch 13 iter 4774: dev loss 0.4223 elbo -0.4223 sf 0.3966 selected 0.5095 kl 0.1126 ll -0.7063 acc 0.4142\n",
      " dev0 [gold=3,pred=1]: **it** **'s** **a** **lovely** film with **lovely** **performances** by **buy** **and** **accorsi** **.**\n",
      " dev1 [gold=2,pred=1]: **no** one goes **unindicted** **here** **,** which **is** **probably** for **the** best .\n",
      " dev2 [gold=3,pred=1]: **and** if **you** **'re** **not** nearly moved **to** **tears** by **a** **couple** **of** **scenes** , **you** 've got **ice** water in your **veins** .\n",
      "\n",
      "Shuffling training data\n",
      "Epoch 14 Iter 4800 loss=0.3591 elbo -0.1807 sf 0.3168 selected 0.4889 kl 0.0872 ll -0.4933\n",
      "Epoch 14 Iter 4900 loss=0.3918 elbo -0.6158 sf 0.7080 selected 0.4873 kl 0.1163 ll -1.3180\n",
      "Epoch 14 Iter 5000 loss=0.3845 elbo -0.5248 sf 0.5740 selected 0.5154 kl 0.1404 ll -1.0917\n",
      "Epoch 14 Iter 5100 loss=0.3921 elbo -0.2054 sf 0.3617 selected 0.4952 kl 0.1125 ll -0.5613\n",
      "\n",
      "# epoch 14 iter 5115: dev loss 0.4188 elbo -0.4188 sf 0.3980 selected 0.5060 kl 0.1093 ll -0.7075 acc 0.4051\n",
      " dev0 [gold=3,pred=1]: it 's **a** **lovely** **film** with **lovely** **performances** by **buy** and **accorsi** **.**\n",
      " dev1 [gold=2,pred=1]: **no** **one** **goes** **unindicted** **here** , which **is** **probably** for the best **.**\n",
      " dev2 [gold=3,pred=1]: and if you **'re** not nearly moved to **tears** by **a** couple of scenes **,** **you** 've got ice **water** in **your** **veins** .\n",
      "\n",
      "Shuffling training data\n",
      "Epoch 15 Iter 5200 loss=0.3973 elbo -0.5171 sf 0.7712 selected 0.5345 kl 0.1100 ll -1.2825\n",
      "Epoch 15 Iter 5300 loss=0.3987 elbo -0.4149 sf 0.5466 selected 0.4964 kl 0.0769 ll -0.9574\n",
      "Epoch 15 Iter 5400 loss=0.3732 elbo -0.2321 sf 0.3642 selected 0.4979 kl 0.1055 ll -0.5907\n",
      "\n",
      "# epoch 15 iter 5456: dev loss 0.4092 elbo -0.4092 sf 0.4019 selected 0.4921 kl 0.1005 ll -0.7106 acc 0.4015\n",
      " dev0 [gold=3,pred=1]: it 's **a** **lovely** film with **lovely** **performances** by **buy** and accorsi **.**\n",
      " dev1 [gold=2,pred=1]: no one goes **unindicted** **here** **,** which is probably for the **best** **.**\n",
      " dev2 [gold=3,pred=1]: **and** **if** you 're **not** nearly moved to tears by a **couple** of scenes , **you** **'ve** **got** ice **water** in your **veins** .\n",
      "\n",
      "Shuffling training data\n",
      "Epoch 16 Iter 5500 loss=0.4059 elbo -0.3730 sf 0.4757 selected 0.4724 kl 0.0831 ll -0.8442\n",
      "Epoch 16 Iter 5600 loss=0.3917 elbo -0.4955 sf 0.5561 selected 0.4595 kl 0.0876 ll -1.0467\n",
      "Epoch 16 Iter 5700 loss=0.3977 elbo -0.2788 sf 0.3597 selected 0.4978 kl 0.0742 ll -0.6343\n",
      "\n",
      "# epoch 16 iter 5797: dev loss 0.4142 elbo -0.4142 sf 0.3821 selected 0.4649 kl 0.0883 ll -0.7081 acc 0.4005\n",
      " dev0 [gold=3,pred=1]: it 's a **lovely** film with **lovely** **performances** **by** buy and **accorsi** **.**\n",
      " dev1 [gold=2,pred=1]: **no** one goes **unindicted** here , **which** is **probably** for the best .\n",
      " dev2 [gold=3,pred=1]: and if you 're not **nearly** moved to **tears** **by** a **couple** of **scenes** **,** you 've got **ice** **water** in **your** veins **.**\n",
      "\n",
      "Epoch 17 Iter 5800 loss=0.4157 elbo -0.4223 sf 0.5760 selected 0.4537 kl 0.0814 ll -0.9936\n",
      "Shuffling training data\n",
      "Epoch 17 Iter 5900 loss=0.4334 elbo -0.4784 sf 0.5314 selected 0.4583 kl 0.0675 ll -1.0059\n",
      "Epoch 17 Iter 6000 loss=0.4258 elbo -0.3217 sf 0.3875 selected 0.4569 kl 0.0774 ll -0.7046\n",
      "Epoch 17 Iter 6100 loss=0.4132 elbo -0.5080 sf 0.6144 selected 0.4778 kl 0.0864 ll -1.1171\n",
      "\n",
      "# epoch 17 iter 6138: dev loss 0.4055 elbo -0.4055 sf 0.3902 selected 0.4730 kl 0.0905 ll -0.7052 acc 0.4160\n",
      " dev0 [gold=3,pred=1]: it **'s** **a** **lovely** **film** **with** **lovely** **performances** **by** buy and **accorsi** .\n",
      " dev1 [gold=2,pred=3]: **no** one goes **unindicted** here , **which** is probably for the **best** .\n",
      " dev2 [gold=3,pred=1]: and **if** **you** 're not nearly moved to tears by a couple **of** **scenes** **,** you **'ve** **got** **ice** **water** in your **veins** **.**\n",
      "\n",
      "Epoch    17: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Shuffling training data\n",
      "Epoch 18 Iter 6200 loss=0.3962 elbo -0.2641 sf 0.2996 selected 0.4821 kl 0.0896 ll -0.5581\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18 Iter 6300 loss=0.4183 elbo -0.4243 sf 0.5225 selected 0.4595 kl 0.0874 ll -0.9412\n",
      "Epoch 18 Iter 6400 loss=0.4131 elbo -0.2345 sf 0.4135 selected 0.4712 kl 0.0683 ll -0.6436\n",
      "\n",
      "# epoch 18 iter 6479: dev loss 0.4028 elbo -0.4028 sf 0.3904 selected 0.4657 kl 0.0856 ll -0.7076 acc 0.3933\n",
      " dev0 [gold=3,pred=1]: it **'s** a **lovely** film with **lovely** performances by **buy** and **accorsi** **.**\n",
      " dev1 [gold=2,pred=1]: **no** one goes unindicted here , which **is** probably for the **best** .\n",
      " dev2 [gold=3,pred=1]: and **if** you **'re** **not** nearly moved to tears by a couple of **scenes** , **you** **'ve** **got** **ice** **water** in **your** **veins** .\n",
      "\n",
      "Shuffling training data\n",
      "Epoch 19 Iter 6500 loss=0.4253 elbo -0.3113 sf 0.4556 selected 0.4688 kl 0.0928 ll -0.7609\n",
      "Epoch 19 Iter 6600 loss=0.3885 elbo -0.3647 sf 0.4804 selected 0.4863 kl 0.0820 ll -0.8397\n",
      "Epoch 19 Iter 6700 loss=0.4019 elbo -0.3480 sf 0.3504 selected 0.4839 kl 0.0665 ll -0.6940\n",
      "Epoch 19 Iter 6800 loss=0.4028 elbo -0.4616 sf 0.5279 selected 0.4903 kl 0.0966 ll -0.9829\n",
      "\n",
      "# epoch 19 iter 6820: dev loss 0.3927 elbo -0.3927 sf 0.3986 selected 0.4736 kl 0.0867 ll -0.7046 acc 0.4169\n",
      " dev0 [gold=3,pred=1]: it **'s** **a** **lovely** **film** with **lovely** **performances** by **buy** and **accorsi** .\n",
      " dev1 [gold=2,pred=1]: **no** one **goes** unindicted here , which **is** probably for **the** **best** .\n",
      " dev2 [gold=3,pred=1]: **and** **if** **you** 're not **nearly** moved **to** **tears** by **a** **couple** of **scenes** , **you** **'ve** got **ice** **water** in **your** veins **.**\n",
      "\n",
      "Shuffling training data\n",
      "Epoch 20 Iter 6900 loss=0.3802 elbo -0.3328 sf 0.4020 selected 0.4582 kl 0.0791 ll -0.7293\n",
      "Epoch 20 Iter 7000 loss=0.4048 elbo -0.3510 sf 0.5132 selected 0.4731 kl 0.0487 ll -0.8608\n",
      "Epoch 20 Iter 7100 loss=0.3901 elbo -0.6739 sf 0.8677 selected 0.4763 kl 0.0914 ll -1.5350\n",
      "\n",
      "# epoch 20 iter 7161: dev loss 0.3846 elbo -0.3846 sf 0.4011 selected 0.4625 kl 0.0773 ll -0.7084 acc 0.4142\n",
      " dev0 [gold=3,pred=1]: **it** 's **a** **lovely** film with **lovely** **performances** by **buy** and accorsi .\n",
      " dev1 [gold=2,pred=1]: **no** one goes **unindicted** here **,** which is probably **for** the best **.**\n",
      " dev2 [gold=3,pred=1]: and if **you** 're not **nearly** moved to **tears** **by** **a** **couple** of **scenes** , **you** 've got ice **water** in **your** **veins** .\n",
      "\n",
      "Shuffling training data\n",
      "Epoch 21 Iter 7200 loss=0.4137 elbo -0.5482 sf 0.7119 selected 0.4320 kl 0.0831 ll -1.2541\n",
      "Epoch 21 Iter 7300 loss=0.4059 elbo -0.3283 sf 0.4781 selected 0.4502 kl 0.0538 ll -0.8025\n",
      "Epoch 21 Iter 7400 loss=0.3886 elbo -0.3837 sf 0.5389 selected 0.4611 kl 0.0666 ll -0.9177\n",
      "Epoch 21 Iter 7500 loss=0.3918 elbo -0.3521 sf 0.4852 selected 0.4430 kl 0.0622 ll -0.8326\n",
      "\n",
      "# epoch 21 iter 7502: dev loss 0.3752 elbo -0.3752 sf 0.4119 selected 0.4719 kl 0.0767 ll -0.7104 acc 0.3987\n",
      " dev0 [gold=3,pred=1]: it **'s** **a** **lovely** **film** with **lovely** **performances** by buy and **accorsi** .\n",
      " dev1 [gold=2,pred=1]: **no** one goes unindicted **here** **,** which is probably for the best **.**\n",
      " dev2 [gold=3,pred=1]: and if you **'re** **not** **nearly** **moved** to **tears** by **a** couple of **scenes** , **you** 've got **ice** **water** in your veins .\n",
      "\n",
      "Shuffling training data\n",
      "Epoch 22 Iter 7600 loss=0.3811 elbo -0.2878 sf 0.4440 selected 0.4906 kl 0.0526 ll -0.7278\n",
      "Epoch 22 Iter 7700 loss=0.3818 elbo -0.3374 sf 0.3939 selected 0.4715 kl 0.0787 ll -0.7253\n",
      "Epoch 22 Iter 7800 loss=0.3725 elbo -0.4451 sf 0.5355 selected 0.4856 kl 0.0569 ll -0.9762\n",
      "\n",
      "# epoch 22 iter 7843: dev loss 0.3621 elbo -0.3621 sf 0.4207 selected 0.4828 kl 0.0750 ll -0.7078 acc 0.3896\n",
      " dev0 [gold=3,pred=1]: it **'s** **a** **lovely** film with **lovely** **performances** by **buy** **and** **accorsi** .\n",
      " dev1 [gold=2,pred=3]: **no** one goes unindicted here , which is probably for the best .\n",
      " dev2 [gold=3,pred=1]: and if **you** **'re** **not** nearly moved to **tears** by a couple **of** **scenes** , **you** **'ve** got **ice** water in your **veins** **.**\n",
      "\n",
      "Shuffling training data\n",
      "Epoch 23 Iter 7900 loss=0.3667 elbo -0.5577 sf 0.9028 selected 0.4937 kl 0.0645 ll -1.4554\n",
      "Epoch 23 Iter 8000 loss=0.3518 elbo -0.5394 sf 0.6864 selected 0.4863 kl 0.0693 ll -1.2203\n",
      "Epoch 23 Iter 8100 loss=0.3745 elbo -0.2828 sf 0.3660 selected 0.5012 kl 0.0719 ll -0.6430\n",
      "\n",
      "# epoch 23 iter 8184: dev loss 0.3637 elbo -0.3637 sf 0.4178 selected 0.4676 kl 0.0689 ll -0.7126 acc 0.3996\n",
      " dev0 [gold=3,pred=1]: it 's **a** **lovely** film with **lovely** **performances** by buy and accorsi **.**\n",
      " dev1 [gold=2,pred=1]: **no** one goes unindicted here , which is probably for **the** best .\n",
      " dev2 [gold=3,pred=1]: and **if** you 're not nearly moved to tears by a couple of scenes **,** you 've **got** ice **water** in your **veins** .\n",
      "\n",
      "Epoch 24 Iter 8200 loss=0.3680 elbo -0.4007 sf 0.5474 selected 0.4612 kl 0.0656 ll -0.9427\n",
      "Shuffling training data\n",
      "Epoch 24 Iter 8300 loss=0.3606 elbo -0.2193 sf 0.2751 selected 0.4680 kl 0.0719 ll -0.4884\n",
      "Epoch 24 Iter 8400 loss=0.3789 elbo -0.2429 sf 0.3331 selected 0.4741 kl 0.0630 ll -0.5708\n",
      "Epoch 24 Iter 8500 loss=0.3708 elbo -0.4217 sf 0.6142 selected 0.4591 kl 0.0575 ll -1.0310\n",
      "\n",
      "# epoch 24 iter 8525: dev loss 0.3562 elbo -0.3562 sf 0.4197 selected 0.4662 kl 0.0665 ll -0.7093 acc 0.4124\n",
      " dev0 [gold=3,pred=1]: it 's a **lovely** film **with** **lovely** **performances** by **buy** and accorsi **.**\n",
      " dev1 [gold=2,pred=3]: **no** one **goes** unindicted here **,** which **is** probably **for** **the** best .\n",
      " dev2 [gold=3,pred=1]: and if you 're not **nearly** moved **to** tears by **a** couple of **scenes** , you 've got **ice** **water** in your veins .\n",
      "\n",
      "Shuffling training data\n",
      "Epoch 25 Iter 8600 loss=0.3711 elbo -0.3451 sf 0.5718 selected 0.4949 kl 0.0730 ll -0.9106\n",
      "Epoch 25 Iter 8700 loss=0.3832 elbo -0.4185 sf 0.5693 selected 0.4735 kl 0.0479 ll -0.9837\n",
      "Epoch 25 Iter 8800 loss=0.3717 elbo -0.3094 sf 0.3879 selected 0.4452 kl 0.0440 ll -0.6934\n",
      "\n",
      "# epoch 25 iter 8866: dev loss 0.3590 elbo -0.3590 sf 0.4133 selected 0.4522 kl 0.0591 ll -0.7131 acc 0.3987\n",
      " dev0 [gold=3,pred=1]: **it** 's a **lovely** film with **lovely** **performances** by buy and **accorsi** **.**\n",
      " dev1 [gold=2,pred=3]: no one **goes** unindicted here , which is probably for the **best** **.**\n",
      " dev2 [gold=3,pred=1]: and if you 're **not** nearly moved to **tears** by a **couple** **of** **scenes** , **you** 've got **ice** **water** in **your** **veins** .\n",
      "\n",
      "Shuffling training data\n",
      "Epoch 26 Iter 8900 loss=0.4039 elbo -0.5318 sf 0.7393 selected 0.4411 kl 0.0554 ll -1.2662\n",
      "Epoch 26 Iter 9000 loss=0.3791 elbo -0.1941 sf 0.2213 selected 0.4589 kl 0.0715 ll -0.4089\n",
      "Epoch 26 Iter 9100 loss=0.3917 elbo -0.3638 sf 0.4739 selected 0.5033 kl 0.0626 ll -0.8320\n",
      "Epoch 26 Iter 9200 loss=0.3774 elbo -0.3217 sf 0.4600 selected 0.4533 kl 0.0617 ll -0.7760\n",
      "\n",
      "# epoch 26 iter 9207: dev loss 0.3514 elbo -0.3514 sf 0.4161 selected 0.4573 kl 0.0596 ll -0.7079 acc 0.3933\n",
      " dev0 [gold=3,pred=3]: **it** **'s** **a** **lovely** film **with** **lovely** **performances** by **buy** and **accorsi** .\n",
      " dev1 [gold=2,pred=1]: no one goes unindicted here , which is probably for the **best** .\n",
      " dev2 [gold=3,pred=1]: and **if** you 're not **nearly** **moved** to **tears** by a **couple** **of** scenes , you **'ve** got ice water **in** **your** **veins** .\n",
      "\n",
      "Shuffling training data\n",
      "Epoch 27 Iter 9300 loss=0.3519 elbo -0.3193 sf 0.5020 selected 0.4828 kl 0.0573 ll -0.8160\n",
      "Epoch 27 Iter 9400 loss=0.3693 elbo -0.3679 sf 0.4157 selected 0.4471 kl 0.0640 ll -0.7776\n",
      "Epoch 27 Iter 9500 loss=0.3572 elbo -0.4425 sf 0.6087 selected 0.4908 kl 0.0677 ll -1.0448\n",
      "\n",
      "# epoch 27 iter 9548: dev loss 0.3433 elbo -0.3433 sf 0.4223 selected 0.4596 kl 0.0581 ll -0.7074 acc 0.4042\n",
      " dev0 [gold=3,pred=1]: it 's **a** **lovely** film with **lovely** **performances** by **buy** **and** **accorsi** .\n",
      " dev1 [gold=2,pred=3]: **no** **one** **goes** **unindicted** here , which is **probably** for **the** best .\n",
      " dev2 [gold=3,pred=1]: and if you **'re** not nearly moved to **tears** by **a** couple of scenes **,** you 've **got** ice **water** in **your** veins .\n",
      "\n",
      "Shuffling training data\n",
      "Epoch 28 Iter 9600 loss=0.3648 elbo -0.3150 sf 0.5060 selected 0.4340 kl 0.0466 ll -0.8165\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28 Iter 9700 loss=0.3548 elbo -0.3587 sf 0.6045 selected 0.4412 kl 0.0642 ll -0.9570\n",
      "Epoch 28 Iter 9800 loss=0.3657 elbo -0.3092 sf 0.5062 selected 0.4444 kl 0.0501 ll -0.8105\n",
      "\n",
      "# epoch 28 iter 9889: dev loss 0.3406 elbo -0.3406 sf 0.4265 selected 0.4596 kl 0.0568 ll -0.7103 acc 0.3878\n",
      " dev0 [gold=3,pred=1]: it 's **a** **lovely** film with **lovely** **performances** by buy and **accorsi** .\n",
      " dev1 [gold=2,pred=3]: no one goes **unindicted** here , which **is** probably for the **best** .\n",
      " dev2 [gold=3,pred=1]: and if you 're **not** nearly moved to **tears** by **a** couple of scenes **,** you 've **got** **ice** water **in** **your** **veins** .\n",
      "\n",
      "Epoch    28: reducing learning rate of group 0 to 2.5000e-04.\n",
      "Epoch 29 Iter 9900 loss=0.3747 elbo -0.5304 sf 0.7928 selected 0.4596 kl 0.0569 ll -1.3176\n",
      "Shuffling training data\n",
      "Epoch 29 Iter 10000 loss=0.3540 elbo -0.3923 sf 0.4826 selected 0.4343 kl 0.0554 ll -0.8693\n",
      "Epoch 29 Iter 10100 loss=0.3857 elbo -0.2859 sf 0.4007 selected 0.4588 kl 0.0404 ll -0.6825\n",
      "Epoch 29 Iter 10200 loss=0.3853 elbo -0.2625 sf 0.3987 selected 0.4400 kl 0.0589 ll -0.6552\n",
      "\n",
      "# epoch 29 iter 10230: dev loss 0.3448 elbo -0.3448 sf 0.4274 selected 0.4593 kl 0.0554 ll -0.7168 acc 0.3978\n",
      " dev0 [gold=3,pred=1]: it 's a **lovely** film with **lovely** performances by buy and **accorsi** **.**\n",
      " dev1 [gold=2,pred=3]: **no** **one** goes unindicted here **,** which **is** probably for **the** **best** .\n",
      " dev2 [gold=3,pred=1]: and if you **'re** not nearly **moved** to tears by **a** **couple** of scenes , you 've got **ice** **water** in your veins .\n",
      "\n",
      "Shuffling training data\n",
      "Epoch 30 Iter 10300 loss=0.3633 elbo -0.4038 sf 0.5483 selected 0.4539 kl 0.0510 ll -0.9469\n",
      "Epoch 30 Iter 10400 loss=0.3736 elbo -0.4406 sf 0.6704 selected 0.4526 kl 0.0499 ll -1.1058\n",
      "Epoch 30 Iter 10500 loss=0.3635 elbo -0.4902 sf 0.7117 selected 0.4558 kl 0.0633 ll -1.1953\n",
      "\n",
      "# epoch 30 iter 10571: dev loss 0.3381 elbo -0.3381 sf 0.4273 selected 0.4564 kl 0.0537 ll -0.7117 acc 0.3987\n",
      " dev0 [gold=3,pred=1]: it 's **a** lovely film with **lovely** **performances** by buy and **accorsi** .\n",
      " dev1 [gold=2,pred=3]: no one goes unindicted here , which is **probably** **for** **the** **best** .\n",
      " dev2 [gold=3,pred=1]: **and** if **you** 're not nearly moved to tears by a **couple** **of** scenes **,** you 've got ice water in **your** **veins** .\n",
      "\n",
      "Epoch 31 Iter 10600 loss=0.3717 elbo -0.4505 sf 0.7297 selected 0.4482 kl 0.0634 ll -1.1735\n",
      "Shuffling training data\n",
      "Epoch 31 Iter 10700 loss=0.3753 elbo -0.4526 sf 0.7115 selected 0.4634 kl 0.0638 ll -1.1573\n",
      "Epoch 31 Iter 10800 loss=0.3605 elbo -0.5517 sf 0.8648 selected 0.4617 kl 0.0695 ll -1.4090\n",
      "Epoch 31 Iter 10900 loss=0.3702 elbo -0.4148 sf 0.5242 selected 0.4595 kl 0.0538 ll -0.9332\n",
      "\n",
      "# epoch 31 iter 10912: dev loss 0.3420 elbo -0.3420 sf 0.4232 selected 0.4493 kl 0.0524 ll -0.7128 acc 0.3896\n",
      " dev0 [gold=3,pred=1]: **it** 's **a** **lovely** **film** with **lovely** performances by buy and **accorsi** .\n",
      " dev1 [gold=2,pred=3]: **no** one goes **unindicted** **here** , which is probably for **the** best **.**\n",
      " dev2 [gold=3,pred=1]: and if you 're not nearly moved to **tears** by a couple of scenes , you **'ve** got **ice** water in **your** **veins** .\n",
      "\n",
      "Shuffling training data\n",
      "Epoch 32 Iter 11000 loss=0.3720 elbo -0.4258 sf 0.6601 selected 0.4568 kl 0.0484 ll -1.0806\n",
      "Epoch 32 Iter 11100 loss=0.3723 elbo -0.3543 sf 0.5695 selected 0.4746 kl 0.0563 ll -0.9176\n",
      "Epoch 32 Iter 11200 loss=0.3540 elbo -0.4610 sf 0.5827 selected 0.4492 kl 0.0558 ll -1.0374\n",
      "\n",
      "# epoch 32 iter 11253: dev loss 0.3386 elbo -0.3386 sf 0.4267 selected 0.4495 kl 0.0528 ll -0.7125 acc 0.3933\n",
      " dev0 [gold=3,pred=1]: it 's a **lovely** film with lovely **performances** by **buy** **and** accorsi .\n",
      " dev1 [gold=2,pred=3]: **no** one **goes** unindicted **here** , which **is** probably for **the** **best** **.**\n",
      " dev2 [gold=3,pred=1]: and if you 're **not** nearly moved to tears by a **couple** of scenes **,** you 've got ice **water** in **your** **veins** .\n",
      "\n",
      "Shuffling training data\n",
      "Epoch 33 Iter 11300 loss=0.3535 elbo -0.2963 sf 0.5080 selected 0.4810 kl 0.0524 ll -0.7983\n",
      "Epoch 33 Iter 11400 loss=0.3703 elbo -0.4250 sf 0.6560 selected 0.4440 kl 0.0437 ll -1.0760\n",
      "Epoch 33 Iter 11500 loss=0.3717 elbo -0.3948 sf 0.5864 selected 0.4537 kl 0.0490 ll -0.9755\n",
      "\n",
      "# epoch 33 iter 11594: dev loss 0.3396 elbo -0.3396 sf 0.4215 selected 0.4453 kl 0.0498 ll -0.7113 acc 0.4024\n",
      " dev0 [gold=3,pred=1]: it 's a **lovely** film with **lovely** **performances** by buy and accorsi .\n",
      " dev1 [gold=2,pred=1]: **no** one goes **unindicted** here , which **is** probably **for** the best .\n",
      " dev2 [gold=3,pred=1]: and if **you** 're not **nearly** **moved** **to** **tears** by a **couple** of scenes , you **'ve** got ice **water** in **your** **veins** .\n",
      "\n",
      "Epoch 34 Iter 11600 loss=0.3743 elbo -0.2615 sf 0.4034 selected 0.4605 kl 0.0479 ll -0.6594\n",
      "Shuffling training data\n",
      "Epoch 34 Iter 11700 loss=0.3896 elbo -0.3109 sf 0.4999 selected 0.4389 kl 0.0492 ll -0.8050\n",
      "Epoch 34 Iter 11800 loss=0.3700 elbo -0.2931 sf 0.3852 selected 0.4307 kl 0.0439 ll -0.6731\n",
      "Epoch 34 Iter 11900 loss=0.3758 elbo -0.7367 sf 0.9949 selected 0.4376 kl 0.0594 ll -1.7245\n",
      "\n",
      "# epoch 34 iter 11935: dev loss 0.3356 elbo -0.3356 sf 0.4312 selected 0.4447 kl 0.0492 ll -0.7175 acc 0.3978\n",
      " dev0 [gold=3,pred=3]: it **'s** a **lovely** **film** **with** **lovely** **performances** by **buy** **and** accorsi .\n",
      " dev1 [gold=2,pred=3]: **no** **one** goes unindicted **here** **,** which **is** probably **for** the **best** .\n",
      " dev2 [gold=3,pred=1]: and if you 're **not** nearly **moved** **to** **tears** **by** **a** **couple** of **scenes** , **you** 've **got** **ice** water in **your** **veins** **.**\n",
      "\n",
      "Shuffling training data\n",
      "Epoch 35 Iter 12000 loss=0.3564 elbo -0.4384 sf 0.6033 selected 0.4335 kl 0.0488 ll -1.0359\n",
      "Epoch 35 Iter 12100 loss=0.3718 elbo -0.4173 sf 0.5889 selected 0.4509 kl 0.0724 ll -0.9974\n",
      "Epoch 35 Iter 12200 loss=0.3575 elbo -0.4548 sf 0.6181 selected 0.4184 kl 0.0482 ll -1.0670\n",
      "\n",
      "# epoch 35 iter 12276: dev loss 0.3345 elbo -0.3345 sf 0.4338 selected 0.4508 kl 0.0503 ll -0.7179 acc 0.3887\n",
      " dev0 [gold=3,pred=1]: it **'s** a **lovely** film **with** **lovely** **performances** **by** buy and accorsi .\n",
      " dev1 [gold=2,pred=3]: no **one** **goes** unindicted **here** , which **is** **probably** for the **best** .\n",
      " dev2 [gold=3,pred=1]: and **if** **you** 're **not** nearly moved to **tears** by a couple of scenes **,** you 've got ice **water** in your **veins** .\n",
      "\n",
      "Epoch 36 Iter 12300 loss=0.3667 elbo -0.3971 sf 0.4524 selected 0.4611 kl 0.0436 ll -0.8441\n",
      "Shuffling training data\n",
      "Epoch 36 Iter 12400 loss=0.3801 elbo -0.4124 sf 0.7134 selected 0.4520 kl 0.0530 ll -1.1192\n",
      "Epoch 36 Iter 12500 loss=0.3604 elbo -0.3697 sf 0.5293 selected 0.4791 kl 0.0457 ll -0.8934\n",
      "Epoch 36 Iter 12600 loss=0.3541 elbo -0.4977 sf 0.7363 selected 0.4519 kl 0.0588 ll -1.2266\n",
      "\n",
      "# epoch 36 iter 12617: dev loss 0.3337 elbo -0.3337 sf 0.4257 selected 0.4428 kl 0.0470 ll -0.7124 acc 0.4024\n",
      " dev0 [gold=3,pred=1]: it 's a **lovely** film with **lovely** **performances** by **buy** and accorsi **.**\n",
      " dev1 [gold=2,pred=3]: **no** one goes **unindicted** here **,** which is probably for **the** **best** .\n",
      " dev2 [gold=3,pred=1]: **and** if you 're **not** nearly moved to **tears** **by** a **couple** of **scenes** , you 've got ice **water** in your **veins** **.**\n",
      "\n",
      "Shuffling training data\n",
      "Epoch 37 Iter 12700 loss=0.3691 elbo -0.4683 sf 0.6236 selected 0.4590 kl 0.0458 ll -1.0861\n",
      "Epoch 37 Iter 12800 loss=0.3711 elbo -0.3882 sf 0.4809 selected 0.4621 kl 0.0478 ll -0.8630\n",
      "Epoch 37 Iter 12900 loss=0.3827 elbo -0.3147 sf 0.3098 selected 0.4210 kl 0.0453 ll -0.6186\n",
      "\n",
      "# epoch 37 iter 12958: dev loss 0.3418 elbo -0.3418 sf 0.4215 selected 0.4352 kl 0.0466 ll -0.7167 acc 0.4051\n",
      " dev0 [gold=3,pred=1]: it 's a **lovely** film with **lovely** **performances** by buy **and** **accorsi** .\n",
      " dev1 [gold=2,pred=3]: no one goes **unindicted** here , **which** is **probably** for the best .\n",
      " dev2 [gold=3,pred=1]: and if you 're not nearly **moved** to tears by a **couple** **of** scenes , you 've **got** **ice** **water** **in** your **veins** .\n",
      "\n",
      "Shuffling training data\n",
      "Epoch 38 Iter 13000 loss=0.3824 elbo -0.2074 sf 0.3038 selected 0.4595 kl 0.0526 ll -0.5044\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 38 Iter 13100 loss=0.3694 elbo -0.2831 sf 0.4224 selected 0.4484 kl 0.0433 ll -0.6998\n",
      "Epoch 38 Iter 13200 loss=0.3731 elbo -0.5109 sf 0.8685 selected 0.4274 kl 0.0520 ll -1.3725\n",
      "\n",
      "# epoch 38 iter 13299: dev loss 0.3336 elbo -0.3336 sf 0.4259 selected 0.4396 kl 0.0472 ll -0.7123 acc 0.3996\n",
      " dev0 [gold=3,pred=1]: it **'s** a **lovely** film **with** **lovely** **performances** by **buy** **and** accorsi .\n",
      " dev1 [gold=2,pred=3]: **no** one goes unindicted here **,** **which** is probably for **the** best .\n",
      " dev2 [gold=3,pred=1]: and if you 're **not** **nearly** moved to **tears** **by** a couple **of** scenes **,** you 've got ice **water** in your **veins** .\n",
      "\n",
      "Epoch 39 Iter 13300 loss=0.3635 elbo -0.3776 sf 0.5937 selected 0.4611 kl 0.0481 ll -0.9650\n",
      "Shuffling training data\n",
      "Epoch 39 Iter 13400 loss=0.3786 elbo -0.3722 sf 0.5689 selected 0.4585 kl 0.0409 ll -0.9356\n",
      "Epoch 39 Iter 13500 loss=0.3578 elbo -0.1857 sf 0.3698 selected 0.4500 kl 0.0363 ll -0.5506\n",
      "Epoch 39 Iter 13600 loss=0.3663 elbo -0.4059 sf 0.6567 selected 0.4354 kl 0.0471 ll -1.0562\n",
      "\n",
      "# epoch 39 iter 13640: dev loss 0.3315 elbo -0.3315 sf 0.4259 selected 0.4384 kl 0.0452 ll -0.7122 acc 0.4024\n",
      " dev0 [gold=3,pred=3]: it **'s** a **lovely** film with lovely performances by **buy** and **accorsi** .\n",
      " dev1 [gold=2,pred=1]: no **one** **goes** unindicted here **,** which is probably for the **best** .\n",
      " dev2 [gold=3,pred=1]: and if you **'re** **not** **nearly** moved **to** **tears** by a couple of scenes , you 've **got** **ice** **water** in **your** **veins** **.**\n",
      "\n",
      "Epoch    39: reducing learning rate of group 0 to 1.2500e-04.\n",
      "Shuffling training data\n",
      "Epoch 40 Iter 13700 loss=0.3617 elbo -0.5164 sf 0.7389 selected 0.4251 kl 0.0545 ll -1.2478\n",
      "Epoch 40 Iter 13800 loss=0.3525 elbo -0.4341 sf 0.6281 selected 0.4200 kl 0.0659 ll -1.0531\n",
      "Epoch 40 Iter 13900 loss=0.3692 elbo -0.4405 sf 0.6293 selected 0.4261 kl 0.0344 ll -1.0650\n",
      "\n",
      "# epoch 40 iter 13981: dev loss 0.3314 elbo -0.3314 sf 0.4268 selected 0.4396 kl 0.0443 ll -0.7139 acc 0.3915\n",
      " dev0 [gold=3,pred=1]: it 's **a** **lovely** film with **lovely** **performances** by buy and accorsi .\n",
      " dev1 [gold=2,pred=3]: **no** **one** goes unindicted **here** **,** which **is** **probably** **for** the best .\n",
      " dev2 [gold=3,pred=1]: and **if** you 're not **nearly** **moved** to tears by a **couple** of scenes , you **'ve** got **ice** water in your veins **.**\n",
      "\n",
      "Epoch 41 Iter 14000 loss=0.3880 elbo -0.3313 sf 0.4474 selected 0.4560 kl 0.0488 ll -0.7719\n",
      "Shuffling training data\n",
      "Epoch 41 Iter 14100 loss=0.3719 elbo -0.4045 sf 0.5326 selected 0.4034 kl 0.0406 ll -0.9313\n",
      "Epoch 41 Iter 14200 loss=0.3666 elbo -0.4050 sf 0.5669 selected 0.4176 kl 0.0335 ll -0.9671\n",
      "Epoch 41 Iter 14300 loss=0.3639 elbo -0.4621 sf 0.6886 selected 0.4469 kl 0.0410 ll -1.1448\n",
      "\n",
      "# epoch 41 iter 14322: dev loss 0.3303 elbo -0.3303 sf 0.4308 selected 0.4354 kl 0.0437 ll -0.7174 acc 0.3933\n",
      " dev0 [gold=3,pred=3]: it 's a **lovely** **film** with **lovely** **performances** by **buy** and accorsi .\n",
      " dev1 [gold=2,pred=3]: **no** one goes unindicted here , which is probably for the **best** **.**\n",
      " dev2 [gold=3,pred=1]: **and** **if** you 're **not** nearly **moved** to tears **by** **a** couple of scenes **,** **you** 've got ice **water** in **your** **veins** .\n",
      "\n",
      "Shuffling training data\n",
      "Epoch 42 Iter 14400 loss=0.3730 elbo -0.3881 sf 0.5683 selected 0.4743 kl 0.0476 ll -0.9495\n",
      "Epoch 42 Iter 14500 loss=0.3455 elbo -0.6695 sf 1.0079 selected 0.4389 kl 0.0447 ll -1.6709\n",
      "Epoch 42 Iter 14600 loss=0.3681 elbo -0.4276 sf 0.6395 selected 0.4459 kl 0.0536 ll -1.0593\n",
      "\n",
      "# epoch 42 iter 14663: dev loss 0.3248 elbo -0.3248 sf 0.4348 selected 0.4407 kl 0.0442 ll -0.7154 acc 0.3951\n",
      " dev0 [gold=3,pred=1]: it 's **a** **lovely** film **with** **lovely** **performances** **by** buy **and** accorsi **.**\n",
      " dev1 [gold=2,pred=3]: **no** **one** goes **unindicted** **here** , **which** is probably for the **best** .\n",
      " dev2 [gold=3,pred=1]: and **if** **you** 're not **nearly** moved to tears by a couple of **scenes** , **you** 've got **ice** **water** in **your** veins **.**\n",
      "\n",
      "Epoch 43 Iter 14700 loss=0.3747 elbo -0.3828 sf 0.6122 selected 0.4305 kl 0.0329 ll -0.9902\n",
      "Shuffling training data\n",
      "Epoch 43 Iter 14800 loss=0.3588 elbo -0.2100 sf 0.3181 selected 0.4019 kl 0.0398 ll -0.5223\n",
      "Epoch 43 Iter 14900 loss=0.3744 elbo -0.2896 sf 0.4074 selected 0.4484 kl 0.0331 ll -0.6920\n",
      "Epoch 43 Iter 15000 loss=0.3722 elbo -0.3088 sf 0.4067 selected 0.4449 kl 0.0402 ll -0.7095\n",
      "\n",
      "# epoch 43 iter 15004: dev loss 0.3307 elbo -0.3307 sf 0.4303 selected 0.4339 kl 0.0427 ll -0.7183 acc 0.3887\n",
      " dev0 [gold=3,pred=3]: **it** 's **a** **lovely** film with lovely performances by buy and **accorsi** .\n",
      " dev1 [gold=2,pred=3]: **no** one goes **unindicted** here **,** which is probably for the best **.**\n",
      " dev2 [gold=3,pred=1]: and if you **'re** **not** nearly moved **to** tears by a couple **of** scenes , **you** **'ve** **got** **ice** **water** in your **veins** .\n",
      "\n",
      "Shuffling training data\n",
      "Epoch 44 Iter 15100 loss=0.3708 elbo -0.4156 sf 0.4586 selected 0.4129 kl 0.0454 ll -0.8674\n",
      "Epoch 44 Iter 15200 loss=0.3855 elbo -0.3575 sf 0.5326 selected 0.4478 kl 0.0395 ll -0.8841\n",
      "Epoch 44 Iter 15300 loss=0.3662 elbo -0.2464 sf 0.4542 selected 0.4612 kl 0.0348 ll -0.6953\n",
      "\n",
      "# epoch 44 iter 15345: dev loss 0.3322 elbo -0.3322 sf 0.4357 selected 0.4380 kl 0.0435 ll -0.7243 acc 0.3942\n",
      " dev0 [gold=3,pred=1]: it 's **a** **lovely** film with **lovely** performances **by** buy and accorsi **.**\n",
      " dev1 [gold=2,pred=1]: **no** one goes **unindicted** here , which **is** probably for **the** **best** **.**\n",
      " dev2 [gold=3,pred=3]: and if you 're not **nearly** **moved** **to** **tears** by a couple **of** scenes , you **'ve** got **ice** **water** in **your** veins .\n",
      "\n",
      "Shuffling training data\n",
      "Epoch 45 Iter 15400 loss=0.3654 elbo -0.4406 sf 0.7440 selected 0.4543 kl 0.0431 ll -1.1780\n",
      "Epoch 45 Iter 15500 loss=0.3535 elbo -0.5295 sf 0.7716 selected 0.4114 kl 0.0672 ll -1.2907\n",
      "Epoch 45 Iter 15600 loss=0.3674 elbo -0.3546 sf 0.5510 selected 0.4441 kl 0.0282 ll -0.9012\n",
      "\n",
      "# epoch 45 iter 15686: dev loss 0.3309 elbo -0.3309 sf 0.4268 selected 0.4358 kl 0.0426 ll -0.7151 acc 0.3924\n",
      " dev0 [gold=3,pred=1]: **it** 's a **lovely** film **with** **lovely** **performances** by buy and accorsi .\n",
      " dev1 [gold=2,pred=1]: **no** one **goes** **unindicted** **here** , which **is** **probably** for **the** **best** .\n",
      " dev2 [gold=3,pred=1]: and if you **'re** **not** **nearly** **moved** to **tears** by **a** couple **of** scenes , you 've got **ice** **water** **in** **your** **veins** .\n",
      "\n",
      "Epoch 46 Iter 15700 loss=0.3763 elbo -0.2551 sf 0.3628 selected 0.4574 kl 0.0439 ll -0.6110\n",
      "Shuffling training data\n",
      "Epoch 46 Iter 15800 loss=0.3427 elbo -0.4154 sf 0.7096 selected 0.4573 kl 0.0389 ll -1.1189\n",
      "Epoch 46 Iter 15900 loss=0.3838 elbo -0.4410 sf 0.5842 selected 0.4170 kl 0.0331 ll -1.0199\n",
      "Epoch 46 Iter 16000 loss=0.3625 elbo -0.2851 sf 0.3650 selected 0.4213 kl 0.0474 ll -0.6425\n",
      "\n",
      "# epoch 46 iter 16027: dev loss 0.3295 elbo -0.3295 sf 0.4356 selected 0.4333 kl 0.0409 ll -0.7242 acc 0.3987\n",
      " dev0 [gold=3,pred=3]: **it** 's **a** **lovely** film **with** **lovely** **performances** by **buy** and accorsi **.**\n",
      " dev1 [gold=2,pred=3]: **no** one goes unindicted here , which is **probably** for the best .\n",
      " dev2 [gold=3,pred=1]: **and** if **you** 're **not** **nearly** moved to tears by a couple of scenes **,** you **'ve** **got** **ice** **water** in **your** veins **.**\n",
      "\n",
      "Shuffling training data\n",
      "Epoch 47 Iter 16100 loss=0.3633 elbo -0.3639 sf 0.4853 selected 0.4353 kl 0.0458 ll -0.8418\n",
      "Epoch 47 Iter 16200 loss=0.3692 elbo -0.4292 sf 0.5490 selected 0.4254 kl 0.0358 ll -0.9723\n",
      "Epoch 47 Iter 16300 loss=0.3700 elbo -0.5641 sf 0.8045 selected 0.4376 kl 0.0530 ll -1.3599\n",
      "\n",
      "# epoch 47 iter 16368: dev loss 0.3260 elbo -0.3260 sf 0.4301 selected 0.4334 kl 0.0404 ll -0.7158 acc 0.3996\n",
      " dev0 [gold=3,pred=1]: **it** 's **a** **lovely** film with **lovely** **performances** by buy and accorsi .\n",
      " dev1 [gold=2,pred=3]: no **one** goes unindicted here , which is probably **for** the **best** .\n",
      " dev2 [gold=3,pred=1]: and if you 're **not** nearly moved to **tears** **by** a couple of scenes , you **'ve** **got** ice **water** **in** your **veins** **.**\n",
      "\n",
      "Epoch 48 Iter 16400 loss=0.3673 elbo -0.2165 sf 0.3622 selected 0.4345 kl 0.0344 ll -0.5730\n",
      "Shuffling training data\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 48 Iter 16500 loss=0.3760 elbo -0.5330 sf 0.7319 selected 0.4295 kl 0.0401 ll -1.2582\n",
      "Epoch 48 Iter 16600 loss=0.3879 elbo -0.2106 sf 0.2981 selected 0.4678 kl 0.0309 ll -0.5036\n",
      "Epoch 48 Iter 16700 loss=0.3703 elbo -0.4917 sf 0.7510 selected 0.4450 kl 0.0401 ll -1.2360\n",
      "\n",
      "# epoch 48 iter 16709: dev loss 0.3323 elbo -0.3323 sf 0.4279 selected 0.4315 kl 0.0396 ll -0.7207 acc 0.4078\n",
      " dev0 [gold=3,pred=1]: it **'s** a **lovely** film with **lovely** **performances** by **buy** and accorsi .\n",
      " dev1 [gold=2,pred=3]: **no** one goes **unindicted** here , which is probably for the best .\n",
      " dev2 [gold=3,pred=1]: and if you 're not nearly **moved** to **tears** by a couple of scenes , you **'ve** **got** **ice** **water** in **your** **veins** **.**\n",
      "\n",
      "Shuffling training data\n",
      "Epoch 49 Iter 16800 loss=0.3778 elbo -0.3293 sf 0.5645 selected 0.4048 kl 0.0417 ll -0.8868\n",
      "Epoch 49 Iter 16900 loss=0.3737 elbo -0.3303 sf 0.4644 selected 0.4123 kl 0.0354 ll -0.7888\n",
      "Epoch 49 Iter 17000 loss=0.3848 elbo -0.3251 sf 0.4835 selected 0.4185 kl 0.0370 ll -0.8023\n",
      "\n",
      "# epoch 49 iter 17050: dev loss 0.3264 elbo -0.3264 sf 0.4295 selected 0.4280 kl 0.0397 ll -0.7161 acc 0.3887\n",
      " dev0 [gold=3,pred=3]: it **'s** a lovely film with **lovely** **performances** by buy **and** accorsi **.**\n",
      " dev1 [gold=2,pred=3]: no one goes **unindicted** here , **which** **is** probably **for** the best .\n",
      " dev2 [gold=3,pred=1]: **and** **if** you 're **not** nearly moved **to** **tears** by a **couple** **of** scenes , you 've **got** ice water in your **veins** **.**\n",
      "\n",
      "Epoch 50 Iter 17100 loss=0.3683 elbo -0.2311 sf 0.3660 selected 0.4386 kl 0.0386 ll -0.5905\n",
      "Shuffling training data\n",
      "Epoch 50 Iter 17200 loss=0.3731 elbo -0.3007 sf 0.5013 selected 0.4527 kl 0.0345 ll -0.7961\n",
      "Epoch 50 Iter 17300 loss=0.3753 elbo -0.4264 sf 0.6004 selected 0.3976 kl 0.0501 ll -1.0181\n",
      "\n",
      "# epoch 50 iter 17391: dev loss 0.3257 elbo -0.3257 sf 0.4316 selected 0.4314 kl 0.0397 ll -0.7176 acc 0.3933\n",
      " dev0 [gold=3,pred=3]: it **'s** a **lovely** film with lovely performances by buy and accorsi .\n",
      " dev1 [gold=2,pred=1]: no **one** goes unindicted **here** **,** which **is** probably for the **best** .\n",
      " dev2 [gold=3,pred=1]: **and** **if** **you** 're not nearly moved **to** **tears** by a **couple** **of** **scenes** , you 've got **ice** **water** **in** your veins **.**\n",
      "\n",
      "Epoch    50: reducing learning rate of group 0 to 6.2500e-05.\n"
     ]
    }
   ],
   "source": [
    "model = train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7w_Ko657vRGo"
   },
   "source": [
    "# Variance reduction\n",
    "\n",
    "**This is an extra**\n",
    "\n",
    "We can use a *control variate* to reduce the variance of our gradient estimates.\n",
    "\n",
    "Let's recap the idea in general terms. We are looking to solve some expectation\n",
    "\\begin{align}\n",
    "\\mu_f = \\mathbb E[f(Z)]\n",
    "\\end{align}\n",
    "but unfortunatelly, realising the full sum (or integral for continuous variables) is intractable. Thus we employ MC estimation\n",
    "\\begin{align}\n",
    "\\hat \\mu_f &\\overset{\\text{MC}}{\\approx} \\frac{1}{S} \\sum_{s=1}^S f(z_s) & \\text{where }z_s \\sim Q(z|x)\n",
    "\\end{align}\n",
    "Note that the variance of this estimate is\n",
    "\\begin{align}\n",
    "\\text{Var}(\\hat \\mu_f) &=  \\frac{1}{S}\\text{Var}(f(Z)) \\\\\n",
    "&= \\frac{1}{S} \\mathbb E[( f(Z) - \\mathbb E[f(Z)])^2]\n",
    "\\end{align}\n",
    "Note that this variance is such that it goes down as we sample more, in a rate $\\mathcal O(S^{-1})$.\n",
    "See that if we sample $10$ times more, we will only obtain an decrease in variance in the order of $10^{-1}$. This means that sampling more is generally not the most convenient way to decrease variance.\n",
    "\n",
    "*Digression* we can estimate the variance itself via MC, an unbiased estimate looks like\n",
    "\\begin{align}\n",
    "\\hat \\sigma^2_f = \\frac{1}{S(S-1)} \\sum_{s=1}^S (f(z_s) - \\hat \\mu_f)^2\n",
    "\\end{align}\n",
    "but not that this estimate is even hard to improve since it decreases with $\\mathcal O(S^{-2})$.\n",
    "\n",
    "Back to out main problem: let's try and improve the variance of our estimator to $\\mu_f$.\n",
    "\n",
    "It's a fact, and it can be shown trivially, that\n",
    "\\begin{align}\n",
    "\\mu_f &=  \\mathbb E[f(Z) - \\psi(Z)] + \\underbrace{\\mathbb E[\\psi(Z)]}_{\\mu_\\psi} \\\\\n",
    " &\\overset{\\text{MC}}{\\approx} \\underbrace{\\left(\\frac{1}{S} \\sum_{s=1}^S f(z_s) - \\psi(z_s) \\right) + \\mu_\\psi}_{\\hat c}\n",
    "\\end{align}\n",
    "where we assume the existence of some function $\\psi(z)$ for which the expected value $\\mu_\\psi$ is known and we estimate the expected difference $\\mathbb E[f(Z) - \\psi(Z)]$ via MC. We used this axuxiliary function, also known as a *control variate*, to derive a new estimator, which we will denote by $\\hat c$.\n",
    "\n",
    "The variance of this new estimator is show below:\n",
    "\n",
    "\\begin{align}\n",
    "\\text{Var}( \\hat c ) &= \\text{Var}(\\hat \\mu_{f-\\psi}) + 2\\underbrace{\\text{Cov}(\\hat \\mu_{f-\\psi}, \\mu_\\psi)}_{\\mathbb E[\\hat \\mu_{f-\\psi}  \\mu_\\psi] - \\mathbb E[\\hat \\mu_{f-\\psi}] \\mathbb E[\\mu_\\psi]} + \\underbrace{\\text{Var}(\\mu_\\psi)}_{\\color{blue}{0} } \\\\\n",
    "&= \\frac{1}{S}\\text{Var}(f- \\psi)  + 2 \\underbrace{\\left( \\mu_\\psi \\mu_{f-\\psi} - \\mu_{f-\\psi} \\mu_\\psi \\right)}_{\\color{blue}{0}} \n",
    "\\end{align}\n",
    "where the variance of $\\mu_\\psi$ is 0 because we know it in closed form (no need for MC estimation), and the covariance is $0$ as shown in the second row.\n",
    "\n",
    "That is, the variance of $\\hat c$ is essentially the variance of estimating $\\mathbb E[f(Z) - \\psi(Z)]$, which in turn depends on the variance \n",
    "\n",
    "\\begin{align}\n",
    "\\text{Var}(f-\\psi) &= \\text{Var}(f) - 2\\text{Cov}(f, \\psi) + \\text{Var}(\\psi)\n",
    "\\end{align}\n",
    "where we can see that if $\\text{Cov}(f, \\psi) > \\frac{\\text{Var}(\\psi)}{2}$ we achieve variance reduction as then $\\text{Var}(f-\\psi)$ would be smaller than $\\text{Var(f)}$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ovKcRnqH_PGp"
   },
   "source": [
    "\n",
    "## Baselines\n",
    "\n",
    "Baslines are control variates of a very simple form:\n",
    "\\begin{align}\n",
    "\\mathbb E[f(Z)] = \\mathbb E[f(Z) - C] + \\mathbb E[C]\n",
    "\\end{align}\n",
    "where $C$ is a constant with respect to $z$.\n",
    "\n",
    "In the context of the score function estimator, a baseline looks like a quantity $C(x; \\omega)$, this may be\n",
    "* just a constant;\n",
    "* or a function of the input (but not of the latent variable), which could be itself implemented as a neural network;\n",
    "* a combination of the two.\n",
    " \n",
    "\n",
    "Let's focus on the first term of the ELBO (so I'm omitting the KL term here). The gradient with respect to parameters of the inference model becomes:\n",
    "\n",
    "\\begin{align}\n",
    "&\\mathbb E_{Q(z|x, \\lambda)}\\left[ \\log P(x|z, \\theta) \\nabla_\\lambda \\log Q(z|x, \\lambda)\\right]\\\\\n",
    "&=\\mathbb E_{Q(z|x, \\lambda)}\\left[\\log P(x|z, \\theta) \\nabla_\\lambda \\log Q(z|x, \\lambda) - \\color{red}{C(x; \\omega)\\nabla_\\lambda \\log Q(z|x, \\lambda) }  \\right] + \\underbrace{\\mathbb E_{Q(z|x, \\lambda)}\\left[\\color{red}{C(x; \\omega)\\nabla_\\lambda \\log Q(z|x, \\lambda) }  \\right] }_{=0} \\\\\n",
    "&= \\mathbb E_{Q(z|x, \\lambda)}\\left[ \\color{blue}{\\left(\\log P(x|z, \\theta) - C(x; \\omega) \\right)}\\nabla_\\lambda \\log Q(z|x, \\lambda)\\right] \\\\\n",
    "&\n",
    "\\end{align}\n",
    "We can show that the last term is $0$\n",
    "\n",
    "\\begin{align}\n",
    "&\\mathbb E_{Q(z|x, \\lambda)}\\left[C(x; \\omega)\\nabla_\\lambda \\log Q(z|x, \\lambda)   \\right]  \\\\&= C(x; \\omega) \\mathbb E_{Q(z|x, \\lambda)}\\left[\\nabla_\\lambda \\log Q(z|x, \\lambda)   \\right]\\\\\n",
    "&= C(x; \\omega) \\mathbb E_{Q(z|x, \\lambda)}\\left[\\frac{1}{Q(z|x, \\lambda)} \\nabla_\\lambda Q(z|x, \\lambda)   \\right] \\\\\n",
    "&= C(x; \\omega) \\sum_z Q(z|x, \\lambda) \\frac{1}{Q(z|x, \\lambda)} \\nabla_\\lambda Q(z|x, \\lambda)   \\\\\n",
    "&= C(x; \\omega) \\sum_z\\nabla_\\lambda Q(z|x, \\lambda)  \\\\\n",
    "&= C(x; \\omega) \\nabla_\\lambda \\underbrace{\\sum_z Q(z|x, \\lambda)  }_{=1}\\\\\n",
    "&=0\n",
    "\\end{align}\n",
    "\n",
    "Examples of useful baselines:\n",
    "\n",
    "* a running average of the learning signal: at some iteration $t$ we can use a running average of $\\log P(x|z, \\theta)$ using parameter estimates $\\theta$ from iterations $i < t$, this is a baseline that likely leads to high correlation between control variate and learning signal and can lead to variance reduction;\n",
    "* another technique is to have an MLP with parameters $\\omega$ predict a scalar and train this MLP to approximate the learning signal $\\log P(x|z, \\theta)$ via regression:\n",
    "\\begin{align}\n",
    "\\arg\\max_\\omega \\left( C(x; \\omega) - \\log P(x|z, \\theta) \\right)^2\n",
    "\\end{align}\n",
    "its left as an extra to implement these ideas.\n",
    "\n",
    "One more note: we can also use something called a *multiplicative baseline* in the literature of reinforcement learning, whereby we incorporate a running estimate of the standard deviation of the learning signal computed based on the values attained on previous iterations:\n",
    "\\begin{align}\n",
    "\\mathbb E_{Q(z|x, \\lambda)}\\left[ \\frac{1}{\\hat\\sigma_{\\text{past}}}\\left(\\log P(x|z, \\theta) - \\hat \\mu_{\\text{past}}\\right)\\nabla_\\lambda \\log Q(z|x, \\lambda)\\right]\n",
    "\\end{align}\n",
    "this form of contorl variate aim at promoting the learning signal (or reward in reinforcement learning literature) to be distributed by $\\mathcal N(0, 1)$. Note that multiplying the reward by a constant does not bias the estimator, and in this case, may lead to variance reduction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tokens(model, data, batch_size=25, device=None, iter_i=0, cfg=None):\n",
    "    \"\"\"Extracts important tokens using model on given data set (using minibatches)\"\"\"\n",
    "\n",
    "    model.eval()  # disable dropout\n",
    "    important_tokens, all_tokens = [], []\n",
    "    for mb in get_minibatch(data, batch_size=batch_size, shuffle=False):\n",
    "        x, targets, reverse_map = prepare_minibatch(mb, model.vocab, device=device)\n",
    "        batch_size = targets.size(0)\n",
    "        with torch.no_grad():\n",
    "            # reverse sort \n",
    "            py, qz, z = model(x)\n",
    "            z = z.cpu().numpy()             \n",
    "            z = z[reverse_map]             \n",
    "            for idx in range(batch_size):  # iterate over instances in a mini batch\n",
    "                example = []\n",
    "                for ti, zi in zip(mb[idx].tokens, z[idx]):  # iterate over tokens in an instance\n",
    "                    if zi == 1:\n",
    "                        important_tokens.append(ti)\n",
    "                    all_tokens.append(ti)\n",
    "\n",
    "    return important_tokens, all_tokens\n",
    "\n",
    "def make_tags_counter(tokens):\n",
    "    text = nltk.word_tokenize(\" \".join(tokens))\n",
    "    pos_tags = nltk.pos_tag(text)\n",
    "    tags_list = [tag[1] for tag in pos_tags]\n",
    "    return Counter(tags_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "important_tokens, all_tokens = get_tokens(\n",
    "    model,\n",
    "    dev_data,\n",
    "    batch_size=cfg['eval_batch_size'],\n",
    "    device=None,\n",
    "    iter_i=0,\n",
    "    cfg=cfg\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "important_counter = make_tags_counter(important_tokens)\n",
    "all_counter = make_tags_counter(all_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VBP 0.6953405017921147\n",
      "VBG 0.6359338061465721\n",
      "FW 0.625\n",
      "NNP 0.5952380952380952\n",
      "JJ 0.5562687205819427\n",
      "WP$ 0.5555555555555556\n",
      "VBD 0.5345911949685535\n",
      "JJS 0.5072463768115942\n",
      "RBS 0.5\n",
      "RB 0.4745222929936306\n",
      "NNS 0.4610655737704918\n",
      "POS 0.43103448275862066\n",
      "NN 0.4069329314242653\n",
      "VBN 0.3591331269349845\n",
      "PRP$ 0.35125448028673834\n",
      "CC 0.34478371501272265\n",
      "RBR 0.3333333333333333\n",
      "JJR 0.3333333333333333\n",
      "PDT 0.3333333333333333\n",
      "VB 0.3235772357723577\n",
      "'' 0.32142857142857145\n",
      "VBZ 0.3154875717017208\n",
      "CD 0.3076923076923077\n",
      ", 0.2983957219251337\n",
      "MD 0.29743589743589743\n",
      "PRP 0.2927215189873418\n",
      ". 0.28876508820798513\n",
      "DT 0.28868906767628966\n",
      "IN 0.28001854427445527\n",
      "EX 0.27450980392156865\n",
      "TO 0.2682926829268293\n",
      "WRB 0.2608695652173913\n",
      "RP 0.25757575757575757\n",
      ": 0.2538860103626943\n",
      "`` 0.20833333333333334\n",
      "WP 0.18181818181818182\n",
      "WDT 0.10204081632653061\n",
      "UH 0.0\n",
      "$ 0.0\n"
     ]
    }
   ],
   "source": [
    "fractions = []\n",
    "for token in all_counter:\n",
    "    fractions.append((token, float(important_counter[token]) / all_counter[token]))\n",
    "for (token, frac) in sorted(fractions, key=lambda pair: -pair[1]):\n",
    "    print(token, frac)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, rationales = evaluate(model, dev_data, batch_size=len(dev_data))\n",
    "important_tag_encounters = defaultdict(int)\n",
    "all_tag_encounters = defaultdict(int)\n",
    "for dev_example, rational in zip(dev_data, rationales):\n",
    "    word_tag_pairs = nltk.pos_tag(dev_example.tokens)\n",
    "    for (_, tag), rational_word in zip(word_tag_pairs, rational[0]):\n",
    "        all_tag_encounters[tag] += 1\n",
    "        if rational_word.startswith('**') and rational_word.endswith('**'):\n",
    "            important_tag_encounters[tag] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RBS 0.6666666666666666\n",
      "WP$ 0.6666666666666666\n",
      "NNP 0.5897435897435898\n",
      "JJ 0.5721807810443177\n",
      "FW 0.5333333333333333\n",
      "JJS 0.5072463768115942\n",
      "UH 0.5\n",
      "VBG 0.49282296650717705\n",
      "RB 0.4671936758893281\n",
      "NNS 0.4507462686567164\n",
      "NN 0.4294539164813442\n",
      "VBP 0.39568345323741005\n",
      "VBN 0.39209726443769\n",
      "VBZ 0.3867832847424684\n",
      "PDT 0.38461538461538464\n",
      "CC 0.382842509603073\n",
      "'' 0.35294117647058826\n",
      "VB 0.34175084175084175\n",
      "VBD 0.3333333333333333\n",
      "PRP$ 0.3154121863799283\n",
      ". 0.31092436974789917\n",
      "CD 0.3106060606060606\n",
      "MD 0.3076923076923077\n",
      ", 0.306951871657754\n",
      ": 0.2849740932642487\n",
      "DT 0.28368794326241137\n",
      "RP 0.2714285714285714\n",
      "JJR 0.2698412698412698\n",
      "PRP 0.26582278481012656\n",
      "IN 0.26242452392011145\n",
      "POS 0.2471264367816092\n",
      "TO 0.24634146341463414\n",
      "WRB 0.24285714285714285\n",
      "RBR 0.2413793103448276\n",
      "WP 0.22077922077922077\n",
      "EX 0.19148936170212766\n",
      "WDT 0.15753424657534246\n",
      "`` 0.14583333333333334\n",
      "$ 0.0\n"
     ]
    }
   ],
   "source": [
    "fractions = []\n",
    "for token in all_counter:\n",
    "    fractions.append((token, float(important_tag_encounters[token]) / all_tag_encounters[token]))\n",
    "for (token, frac) in sorted(fractions, key=lambda pair: -pair[1]):\n",
    "    print(token, frac)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "SST.ipynb",
   "provenance": [],
   "toc_visible": true,
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
