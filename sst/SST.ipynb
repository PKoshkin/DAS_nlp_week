{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/probabll/dgm4nlp/blob/master/notebooks/sst/SST.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Udt3kHMdWvYe"
   },
   "source": [
    "We will need to import some helper code, so we need to run this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "U8eXUCRiWvYi"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "nb_dir = os.path.split(os.getcwd())[0]\n",
    "if nb_dir not in sys.path:\n",
    "    sys.path.append(nb_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rYhItDYMZi6a"
   },
   "source": [
    "# Colab\n",
    "\n",
    "We will need to download some data for this notebook, so if you are using [colab](https://colab.research.google.com), set the `using_colab` flag below to `True` in order to clone our [github repo](https://github.com/probabll/dgm4nlp)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_shCMftIx1rW"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "README.md                     kolka_4_SST.ipynb\r\n",
      "SST.ipynb                     kolka_4_SST_modify.ipynb\r\n",
      "__init__.py                   kolka_working_SST-Copy1.ipynb\r\n",
      "\u001b[1m\u001b[36m__pycache__\u001b[m\u001b[m                   kolka_working_SST.ipynb\r\n",
      "\u001b[1m\u001b[36mdata\u001b[m\u001b[m                          my_first_SST.ipynb\r\n",
      "evaluate.py                   \u001b[1m\u001b[36mnn\u001b[m\u001b[m\r\n",
      "\u001b[1m\u001b[36mimg\u001b[m\u001b[m                           \u001b[31mplotting.py\u001b[m\u001b[m\r\n",
      "kolka_2_SST.ipynb             sstutil.py\r\n",
      "kolka_3_SST.ipynb             util.py\r\n"
     ]
    }
   ],
   "source": [
    "using_colab = not True\n",
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "N-fFME2OW22i"
   },
   "outputs": [],
   "source": [
    "if using_colab:\n",
    "  !rm -fr dgm4nlp sst\n",
    "  !git clone https://github.com/probabll/dgm4nlp.git\n",
    "  !cp -R dgm4nlp/notebooks/sst ./  \n",
    "  !ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "l_7NCZlZacNu"
   },
   "source": [
    "Now we can start our lab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "s9mH-rUhWvYq"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "# CPU should be fine for this lab\n",
    "device = torch.device('cpu')  \n",
    "#device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')  \n",
    "from collections import OrderedDict\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "okMoxTJ9bWjc"
   },
   "source": [
    "# Sentiment Classification \n",
    "\n",
    "\n",
    "We are going to augment a sentiment classifier with a layer of discrete latent variables which will help us improve the model's interpretability. But first, let's quickly review the baseline task.\n",
    "\n",
    "\n",
    "In sentiment classification, we have some text input $x = \\langle x_1, \\ldots, x_n \\rangle$, e.g. a sentence or short paragraph, which expresses a certain sentiment $y$, i.e. one of $K$ classes, towards a subject (e.g. a film or a product). \n",
    "\n",
    "\n",
    "\n",
    "We can learn a sentiment classifier by learning a categorical distribution over classes for a given input:\n",
    "\n",
    "\\begin{align}\n",
    "Y|x &\\sim \\text{Cat}(f(x; \\theta))\n",
    "\\end{align}\n",
    "\n",
    "where the Categorical pmf is $\\text{Cat}(y|\\pi) = \\pi_y$.\n",
    "\n",
    "A categorical distribution over $K$ classes is parameterised by a $K$-dimensional probability vector, here we use a neural network $f$ to map from the input to this probability vector. Technically we say *a neural network parameterise our model*, that is, it computes the parameters of our categorical observation model. The figure below is a graphical depiction of the model: circled nodes are random variables (a shaded node is an observed variable), uncircled nodes are deterministic, a plate indicates multiple draws.\n",
    "\n",
    "<img src=\"https://github.com/probabll/dgm4nlp/raw/master/notebooks/sst/img/classifier.png\"  height=\"100\">\n",
    "\n",
    "The neural network (NN) $f(\\cdot; \\theta)$ has parameters of its own, i.e. the weights of the various architecture blocks used, which we denoted generically by $\\theta$.\n",
    "\n",
    "Suppose we have a dataset $\\mathcal D = \\{(x^{(1)}, y^{(1)}), \\ldots, (x^{(N)}, y^{(N)})\\}$ containing $N$ i.i.d. observations. Then we can use the log-likelihood function \n",
    "\\begin{align}\n",
    "\\mathcal L(\\theta|\\mathcal D) &= \\sum_{k=1}^{N} \\log P(y^{(k)}|x^{(k)}, \\theta) \\\\\n",
    "&= \\sum_{k=1}^{N} \\log \\text{Cat}(y^{(k)}|f(x^{(k)}; \\theta))\n",
    "\\end{align}\n",
    " to estimate $\\theta$ by maximisation:\n",
    " \\begin{align}\n",
    " \\theta^\\star = \\arg\\max_{\\theta \\in \\Theta} \\mathcal L(\\theta|\\mathcal D) ~ .\n",
    " \\end{align}\n",
    " \n",
    "\n",
    "We can use stochastic gradient-ascent to find a local optimum of $\\mathcal L(\\theta|\\mathcal D)$, which only requires a gradient estimate:\n",
    "\n",
    "\\begin{align}\n",
    "\\nabla_\\theta \\mathcal L(\\theta|\\mathcal D) &= \\sum_{k=1}^{|\\mathcal D|} \\nabla_\\theta  \\log P(y^{(k)}|x^{(k)}, \\theta) \\\\ \n",
    "&= \\sum_{k=1}^{|\\mathcal D|} \\frac{1}{N} N \\nabla_\\theta  \\log P(y^{(k)}|x^{(k)}, \\theta)  \\\\\n",
    "&= \\mathbb E_{\\mathcal U(1/N)} \\left[ N \\nabla_\\theta  \\log P(y^{(K)}|x^{(K)}, \\theta) \\right]  \\\\\n",
    "&\\overset{\\text{MC}}{\\approx} \\frac{N}{M} \\sum_{m=1}^M \\nabla_\\theta  \\log P(y^{(k_m)}|x^{(k_m)}, \\theta) \\\\\n",
    "&\\text{where }K_m \\sim \\mathcal U(1/N)\n",
    "\\end{align}\n",
    "\n",
    "This is a Monte Carlo (MC) estimate of the gradient computed on $M$ data points selected uniformly at random from $\\mathcal D$.\n",
    "\n",
    "For as long as $f$ remains differentiable wrt to its inputs and parameters, we can rely on automatic differentiation to obtain gradient estimates.\n",
    "\n",
    "In what follows we show how to design $f$ and how to extend this basic model to a latent-variable model.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4LUjyO-39zan"
   },
   "source": [
    "## Data\n",
    "\n",
    "We provide you some code to load the data (see `sst.sstutil.examplereader`). Play with the snippet below and inspect a few training instances:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4z8Bt5no9z6w"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data\n",
      "train 8544\n",
      "dev 1101\n",
      "test 2210\n",
      "\n",
      "# Examples\n",
      "First dev example: Example(tokens=['It', \"'s\", 'a', 'lovely', 'film', 'with', 'lovely', 'performances', 'by', 'Buy', 'and', 'Accorsi', '.'], label=3, transitions=[0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1], token_labels=[2, 2, 2, 3, 2, 2, 3, 2, 2, 2, 2, 2, 2])\n",
      "First dev example tokens: ['It', \"'s\", 'a', 'lovely', 'film', 'with', 'lovely', 'performances', 'by', 'Buy', 'and', 'Accorsi', '.']\n",
      "First dev example label: 3\n"
     ]
    }
   ],
   "source": [
    "from sst.sstutil import examplereader, Vocabulary, load_glove    \n",
    "\n",
    "\n",
    "# Let's load the data into memory.\n",
    "print(\"Loading data\")\n",
    "train_data = list(examplereader('../sst/data/sst/train.txt'))\n",
    "dev_data = list(examplereader('../sst/data/sst/dev.txt'))\n",
    "test_data = list(examplereader('../sst/data/sst/test.txt'))\n",
    "\n",
    "print(\"train\", len(train_data))\n",
    "print(\"dev\", len(dev_data))\n",
    "print(\"test\", len(test_data))\n",
    "\n",
    "print('\\n# Examples')\n",
    "example = dev_data[0]\n",
    "print(\"First dev example:\", example)\n",
    "print(\"First dev example tokens:\", example.tokens)\n",
    "print(\"First dev example label:\", example.label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lB2lEsNuWvYx"
   },
   "source": [
    "## Architecture\n",
    "\n",
    "\n",
    "The function $f$ conditions on a high-dimensional input (i.e. text), so we need to convert it to continuous real vectors. This is the job an *encoder*. \n",
    "\n",
    "**Embedding Layer**\n",
    "\n",
    "The first step is to convert the words in $x$ to vectors, which in this lab we will do with a pre-trained embedding layer (we will use GloVe).\n",
    "\n",
    "We will denote the embedding of the $i$th word of the input by:\n",
    "\n",
    "\\begin{equation}\n",
    "\\mathbf x_i = \\text{glove}(x_i)\n",
    "\\end{equation}\n",
    "\n",
    "**Encoder Layer**\n",
    "\n",
    "In this lab, an encoder takes a sequence of input vectors $\\mathbf x_1^n$, each $I$-dimensional, and produces a sequence of output vectors $\\mathbf t_1^n$, each $O$-dimensional and a summary vector $\\mathbf h \\in \\mathbb R^O$:\n",
    "\n",
    "\\begin{equation}\n",
    "    \\mathbf t_1^n, \\mathbf h = \\text{encoder}(\\mathbf x_1^n; \\theta_{\\text{enc}})\n",
    "\\end{equation}\n",
    "\n",
    "where we use $\\theta_{\\text{enc}}$ to denote the subset of parameters in $\\theta$ that are specific to this encoder block. \n",
    "\n",
    "*Remark:* in practice for a correct batched implementation, our encoders also take a mask matrix and a vector of lengths.\n",
    "\n",
    "Examples of encoding functions can be a feed-forward NN (with an aggregator based on sum or average/max pooling) or a recurrent NN (e.g. an LSTM/GRU). Other architectures are also possible.\n",
    "\n",
    "**Output Layer**\n",
    "\n",
    "From our summary vector $\\mathbf h$, we need to parameterise a categorical distribution over $K$ classes, thus we use\n",
    "\n",
    "\\begin{align}\n",
    "f(x; \\theta) &= \\text{softmax}(\\text{dense}_K(\\mathbf h; \\theta_{\\text{output}}))\n",
    "\\end{align}\n",
    "\n",
    "where $\\text{dense}_K$ is a dense layer with $K=5$ outputs and $\\theta_{\\text{output}}$ corresponds to its parameters (weight matrix and bias vector). Note that we need to use the softmax activation function in order to guarantee that the output of $f$ is a normalised probability vector.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Kc15Nv2i41cq"
   },
   "source": [
    "## Implementation\n",
    "\n",
    "To leave an indication of the shape of tensors in the code, we use the following convention\n",
    "\n",
    "```python\n",
    "[B, T, D]\n",
    "```\n",
    "\n",
    "where `B` stands for `batch_size`, `T` stands for `time` (or rather *maximum sequence length*), and `D` is the size of the representation.\n",
    "\n",
    "\n",
    "Consider the following abstract Encoder class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xwEPXT2MWvYz",
    "tags": [
     "encoders"
    ]
   },
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    \"\"\"\n",
    "    An Encoder for us is a function that\n",
    "      1. transforms a sequence of I-dimensional vectors into a sequence of O-dimensional vectors\n",
    "      2. summarises a sequence of I-dimensional vectors into one O-dimensional vector\n",
    "      \n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    def __init__(self):\n",
    "        super(Encoder, self).__init__()\n",
    "        \n",
    "    def forward(self, inputs, mask, lengths):\n",
    "        \"\"\"\n",
    "        The input is a batch-first tensor of token ids. Here is an example:\n",
    "        \n",
    "        Example of inputs (though rather than words, we have word ids):\n",
    "            INPUTS                     MASK       LENGTHS\n",
    "            [the nice cat -PAD-]    -> [1 1 1 0]  [3]\n",
    "            [the nice dog running]  -> [1 1 1 1]  [4]\n",
    "            \n",
    "        Note that:\n",
    "              mask =  inputs == 1\n",
    "              lengths = mask.sum(dim=-1)\n",
    "        \n",
    "        :param inputs: [B, T, I]\n",
    "        :param mask: [B, T]\n",
    "        :param lengths: [B]\n",
    "        :returns: [B, T, O], [B, O]\n",
    "            where the first tensor is the transformed input\n",
    "            and the second tensor is a summary of all inputs\n",
    "        \"\"\"\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WA5wmkcRg9Am"
   },
   "source": [
    "Let's start easy, implement a *bag of words* encoder:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "U-9hLQ0lF5SG"
   },
   "outputs": [],
   "source": [
    "class BagOfWordsEncoder(Encoder):\n",
    "    \"\"\"\n",
    "    This encoder does not transform the input sequence, \n",
    "     and its summary output is just a sum.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(BagOfWordsEncoder, self).__init__()\n",
    "        \n",
    "    def forward(self, inputs, mask, lengths=None, **kwargs):\n",
    "        reshaped_mask = mask.float().unsqueeze(-1)  # shape: [B, T, 1]\n",
    "        return inputs, (inputs * reshaped_mask).sum(1) / (reshaped_mask.sum(1) + 1e-8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IS7x0hLrUXfN"
   },
   "source": [
    "You can also consider implementing\n",
    "\n",
    "* a feed-forward encoder with average pooling\n",
    "* and a biLSTM encoder\n",
    "\n",
    "but these are certainly optional."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BpOGFpK_Uo0-"
   },
   "outputs": [],
   "source": [
    "class FFEncoder(Encoder):\n",
    "    \"\"\"\n",
    "    A typical feed-forward NN with tanh hidden activations.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self,\n",
    "                 input_size,\n",
    "                 output_size, \n",
    "                 activation=None, \n",
    "                 hidden_sizes=[], \n",
    "                 aggregator='sum',\n",
    "                 dropout=0.5):\n",
    "        \"\"\"\n",
    "        :param input_size: int\n",
    "        :param output_size: int\n",
    "        :param hidden_sizes: list of integers (dimensionality of hidden layers)\n",
    "        :param aggregator: 'sum' or 'avg'\n",
    "        :param dropout: dropout rate\n",
    "        \"\"\"\n",
    "        super(FFEncoder, self).__init__()\n",
    "        layers = []\n",
    "        \n",
    "        def add_droput(name):\n",
    "            if dropout > 0:\n",
    "                layers.append((name, nn.Dropout(p=dropout)))\n",
    "        \n",
    "        if len(hidden_sizes) > 0:                    \n",
    "            for i, size in enumerate(hidden_sizes):\n",
    "                add_droput('dropout_{}'.format(i))\n",
    "                layers.append(('linear_{}'.format(i), nn.Linear(input_size, size)))\n",
    "                layers.append(('tanh_{}'.format(i), nn.Tanh()))\n",
    "                input_size = size\n",
    "\n",
    "        last_output_size = hidden_sizes[-1] if len(hidden_sizes) > 0 else input_size\n",
    "        add_droput('final_dropout')\n",
    "        layers.append(('final_linear', nn.Linear(last_output_size, output_size)))       \n",
    "        self.layer = nn.Sequential(OrderedDict(layers))\n",
    "\n",
    "        self.activation = activation\n",
    "\n",
    "        if not aggregator in ['sum', 'avg']:\n",
    "            raise ValueError(\"I can only aggregate outputs using 'sum' or 'avg'\")\n",
    "        self.aggregator = aggregator\n",
    "        \n",
    "    def forward(self, inputs, mask, lengths):\n",
    "        outputs = self.layer(inputs)  # shape: [B, T, O]\n",
    "        if not self.activation is None:\n",
    "            outputs = self.activation(outputs)  # shape: [B, T, O]\n",
    "        reshaped_mask = mask.float().unsqueeze(-1)  # shape: [B, T, 1]\n",
    "        summary = (outputs * reshaped_mask).sum(dim=1)  # shape: [B, O]\n",
    "        if self.aggregator == 'avg':\n",
    "            reshaped_lens = lengths.float().unsqueeze(-1)  # shape: [B, 1]\n",
    "            summary /= reshaped_lens  # shape: [B, O]\n",
    "        return outputs, summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IxQ5djZ_VAvK"
   },
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "\n",
    "\n",
    "class LSTMEncoder(Encoder):\n",
    "    \"\"\"\n",
    "    This module encodes a sequence into a single vector using an LSTM,\n",
    "     it also returns the hidden states at each time step.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 in_features,\n",
    "                 hidden_size: int=200,\n",
    "                 batch_first: bool=True,\n",
    "                 bidirectional: bool=True):\n",
    "        \"\"\"\n",
    "        :param in_features:\n",
    "        :param hidden_size:\n",
    "        :param batch_first:\n",
    "        :param bidirectional:\n",
    "        \"\"\"\n",
    "        super(LSTMEncoder, self).__init__()\n",
    "        self.lstm = nn.LSTM(\n",
    "            in_features,\n",
    "            hidden_size,\n",
    "            batch_first=batch_first,\n",
    "            bidirectional=bidirectional\n",
    "        )\n",
    "        self.batch_first = batch_first\n",
    "\n",
    "    def forward(self, x, mask, lengths):\n",
    "        \"\"\"\n",
    "        Encode sentence x\n",
    "        :param x: sequence of word embeddings, shape [B, T, E]\n",
    "        :param mask: byte mask that is 0 for invalid positions, shape [B, T]\n",
    "        :param lengths: the lengths of each input sequence [B]\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        mask = mask.unsqueeze(2).float()\n",
    "        res = self.lstm(x * mask)[0]\n",
    "        return res, res[:, -1, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "s_zz5zIyVkSh"
   },
   "source": [
    "Here is some helper code to select and return an encoder:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "59ZU6JddVjMV"
   },
   "outputs": [],
   "source": [
    "def get_encoder(layer, in_features, hidden_size, bidirectional=True):\n",
    "    \"\"\"Returns the requested layer.\"\"\"\n",
    "\n",
    "    # TODO: make pass and average layers\n",
    "    if layer == \"bow\":\n",
    "        return BagOfWordsEncoder()\n",
    "    elif layer == 'ff':\n",
    "        return FFEncoder(\n",
    "            in_features, \n",
    "            2 * hidden_size,   # for convenience\n",
    "            hidden_sizes=[hidden_size], \n",
    "            aggregator='avg'\n",
    "        )\n",
    "    elif layer == \"lstm\":\n",
    "        return LSTMEncoder(\n",
    "            in_features, \n",
    "            hidden_size,\n",
    "            bidirectional=bidirectional\n",
    "        )\n",
    "    else:\n",
    "        raise ValueError(\"Unknown layer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kY8LZiMN5CHW"
   },
   "source": [
    "# Sentiment Classification with Latent Rationale\n",
    "\n",
    "A latent rationale is a compact and informative fragment of the input based on which a NN classifier makes its decisions. [Lei et al (2016)](http://aclweb.org/anthology/D16-1011) proposed to induce such rationales along with a regression model for multi-aspect sentiment analsysis, their model is trained via REINFORCE on a dataset of beer reviews.\n",
    "\n",
    "*Remark:* the model we will develop here can be seen as a probabilistic version of their model. The rest of this notebook focus on our own probabilitisc view of the model.\n",
    "\n",
    "The picture below depicts our latent-variable model for rationale extraction:\n",
    "\n",
    "<img src=\"https://github.com/probabll/dgm4nlp/raw/master/notebooks/sst/img/rationale.png\"  height=\"200\">\n",
    "\n",
    "where we augment the model with a collection of latent variables $z = \\langle z_1, \\ldots, z_n\\rangle$ where $z_i$ is a binary latent variable. Each latent variable $z_i$ regulates whether or not the input $x_i$ is available to the classifier.  We use $x \\odot z$ to denote the selected words, which, in the terminology of Lei et al, is a latent rationale.\n",
    "\n",
    "Again the classifier parameterises a Categorical distribution over $K=5$ outcomes, though this time it can encode only a selection of the input:\n",
    "\n",
    "\\begin{align}\n",
    "    Z_i & \\sim \\text{Bern}(p_1) \\\\\n",
    "    Y|z,x &\\sim \\text{Cat}(f(x \\odot z; \\theta))\n",
    "\\end{align}\n",
    "\n",
    "where we have a shared and fixed Bernoulli prior (with parameter $p_1$) for all $n$ latent variables.\n",
    "\n",
    "\n",
    "Here is an example design for $f$:\n",
    "\n",
    "\\begin{align}\n",
    "\\mathbf x_i &= z_i \\, \\text{glove}(x_i) \\\\\n",
    "\\mathbf t_1^n, \\mathbf h &= \\text{encoder}(\\mathbf x_1^n; \\theta_{\\text{enc}}) \\\\\n",
    "f(x \\odot z; \\theta) &= \\text{softmax}(\\text{dense}_K(\\mathbf h; \\theta_{\\text{output}}))\n",
    "\\end{align}\n",
    "\n",
    "where:\n",
    "* $z_i$ either leaves $\\mathbf x_i$ unchanged or turns it into a vector of zeros;\n",
    "* the encoder only sees features from selected inputs, i.e. $x_i$ for which $z_i = 1$;\n",
    "* $\\text{dense}_K$ is a linear layer with $K=5$ outputs.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hDHNxLHMWvY-"
   },
   "source": [
    "## Prior\n",
    "\n",
    "\n",
    "Our prior is a Bernoulli with fixed parameter $0 < p_1 < 1$:\n",
    "\n",
    "\\begin{align}\n",
    "Z_i & \\sim \\text{Bern}(p_1)\n",
    "\\end{align}\n",
    "\n",
    "whose pmf is $\\text{Bern}(z_i|p_1) = p_1^{z_i}\\times (1-p_1)^{1-z_i}$.\n",
    "\n",
    "As we will be using Bernoulli priors and posteriors, it is a good idea to implement a Bernoulli class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iCBcHnTsOuDr"
   },
   "outputs": [],
   "source": [
    "class Bernoulli:\n",
    "    \"\"\"\n",
    "    This class encapsulates a collection of Bernoulli distributions. \n",
    "    Each Bernoulli is uniquely specified by p_1, where\n",
    "        Bernoulli(X=x|p_1) = pow(p_1, x) + pow(1 - p_1, 1 - x)\n",
    "    is the Bernoulli probability mass function (pmf).    \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, logits=None, probs=None):\n",
    "        \"\"\"\n",
    "        We can specify a Bernoulli distribution via a logit or a probability. \n",
    "         You need to specify at least one, and if you specify both, beware that\n",
    "         in this implementation logits will be used.\n",
    "         \n",
    "        Recall that: probs = sigmoid(logits).\n",
    "         \n",
    "        :param logits: a tensor of logits (a logit is defined as log (p_1 / p_0))\n",
    "            where p_0 = 1 - p_1\n",
    "        :param probs: a tensor of probabilities, each in (0, 1)\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        #print(\"probs:\", type(probs))\n",
    "        #print(\"logits:\", type(logits))\n",
    "                \n",
    "        if probs is None and logits is None:\n",
    "            raise ValueError('I need probabilities or logits')   \n",
    "        \n",
    "        self.probs = probs if logits is None else torch.sigmoid(logits)\n",
    "    \n",
    "    def sample(self):\n",
    "        \"\"\"Returns a sample with the same shape as the parameters\"\"\"\n",
    "        return torch.rand(self.probs.shape) < self.probs\n",
    "    \n",
    "    def log_pmf(self, x):\n",
    "        \"\"\"\n",
    "        Assess the log probability of a sample. \n",
    "        :param x: either a single sample (0 or 1) or a tensor of samples with the same shape as the parameters.\n",
    "        :returns: tensor with log probabilities with the same shape as parameters\n",
    "            (if the input is a single sample we broadcast it to the shape of the parameters)\n",
    "        \"\"\"\n",
    "        return x * torch.log(self.probs) + (1.0 - x) * torch.log(1.0 - self.probs)\n",
    "    \n",
    "    def kl(self, other: 'Bernoulli'):\n",
    "        \"\"\"\n",
    "        Compute the KL divergence between two Bernoulli distributions (from self to other).\n",
    "        \n",
    "        :return: KL[self||other] with same shape parameters\n",
    "        \"\"\"\n",
    "        return self.probs * (torch.log(self.probs) - torch.log(other.probs)) \\\n",
    "            + (1 - self.probs) * (torch.log(1 - self.probs) - torch.log(1 - other.probs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "u0yfkCZlWvZP"
   },
   "source": [
    "## Classifier\n",
    "\n",
    "The classifier encodes only a selection of the input, which we denote $x \\odot z$, and parameterises a Categorical distribution over $5$ outcomes (sentiment levels).\n",
    "\n",
    "Thus let's implement a Categorical distribution (we will only need to be able to assess its lgo pmf):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "F-6JLDnBQcdg"
   },
   "outputs": [],
   "source": [
    "class Categorical:\n",
    "    \n",
    "    def __init__(self, log_probs):\n",
    "        # [B, K]: class probs\n",
    "        self.log_probs = log_probs\n",
    "        \n",
    "    def log_pmf(self, x):\n",
    "        \"\"\"\n",
    "        :param x: [B] integers (targets)\n",
    "        :returns: [B] scalars (log probabilities)\n",
    "        \"\"\"\n",
    "        return self.log_probs[np.arange(len(x)), x]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CdrM_YRI8xBF"
   },
   "source": [
    "and a classifier architecture:\n",
    "\n",
    "* implement the forward method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sz7GaKbgRCd8"
   },
   "outputs": [],
   "source": [
    "class Classifier(nn.Module):\n",
    "    \"\"\"\n",
    "    The Encoder takes an input text (and rationale z) and computes p(y|x,z)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 embed: nn.Embedding=None,\n",
    "                 hidden_size: int=200,\n",
    "                 output_size: int=1,\n",
    "                 dropout: float=0.1,\n",
    "                 layer: str=\"pass\"):\n",
    "\n",
    "        super(Classifier, self).__init__()\n",
    "\n",
    "        emb_size = embed.weight.shape[1]\n",
    "        enc_size = hidden_size * 2\n",
    "        # Here we embed the words\n",
    "        self.embed_layer = nn.Sequential(embed)\n",
    "\n",
    "        self.enc_layer = get_encoder(layer, emb_size, hidden_size)\n",
    "\n",
    "        # and here we predict categorical parameters\n",
    "        self.output_layer = nn.Sequential(\n",
    "            nn.Dropout(p=dropout),\n",
    "            nn.Linear(enc_size, output_size),\n",
    "            nn.LogSoftmax(dim=-1)\n",
    "        )\n",
    "\n",
    "        self.report_params()\n",
    "\n",
    "    def report_params(self):\n",
    "        count = 0\n",
    "        for name, p in self.named_parameters():\n",
    "            if p.requires_grad and \"embed\" not in name:\n",
    "                count += np.prod(list(p.shape))\n",
    "        print(\"{} #params: {}\".format(self.__class__.__name__, count))\n",
    "\n",
    "    def forward(self, x, mask, z) -> Categorical:\n",
    "        \"\"\"\n",
    "        :params x: [B, T, I] word representations\n",
    "        :params mask: [B, T] indicates valid positions\n",
    "        :params z: [B, T] binary selectors\n",
    "        :returns: one Categorical distribution per instance in the batch\n",
    "          each conditioning only on x_i for which z_i = 1\n",
    "        \"\"\"\n",
    "        embeddings = self.embed_layer(x)  # [B, T, E]\n",
    "        embedding_mask = z.float().unsqueeze(-1)  # [B, T, 1]\n",
    "        masked_embeddings = embeddings * embedding_mask  # [B, T, E]\n",
    "        lengths = mask.long().sum(1)  # [B]\n",
    "\n",
    "        # encode the sentence\n",
    "        _, final = self.enc_layer(masked_embeddings, mask, lengths)\n",
    "\n",
    "        # predict sentiment from final state(s)\n",
    "        log_probs = self.output_layer(final)  # [B, T, O]\n",
    "        return Categorical(log_probs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "p2waCCBF9MaH"
   },
   "source": [
    "## Inference\n",
    "\n",
    "\n",
    "Computing the log-likelihood of an observation requires marginalising over assignments of $z$:\n",
    "\n",
    "\\begin{align}\n",
    "P(y|x,\\theta,p_1) &= \\sum_{z_1 = 0}^1 \\cdots \\sum_{z_n=0}^1 P(z|p_1)\\times P(y|x,z, \\theta) \\\\\n",
    "&= \\sum_{z_1 = 0}^1 \\cdots \\sum_{z_n=0}^1 \\left( \\prod_{i=1}^n \\text{Bern}(z_i|p_1)\\right) \\times \\text{Cat}(y|f(x \\odot z; \\theta)) \n",
    "\\end{align}\n",
    "\n",
    "This is clearly intractable: there are $2^n$ possible assignments to $z$ and because the classifier conditions on all latent selectors, there's no way to simplify the expression.\n",
    "\n",
    "We will avoid computing this intractable marginal by instead employing an independently parameterised inference model.\n",
    "This inference model $Q(z|x, y, \\lambda)$ is an approximation to the true postrerior $P(z|x, y, \\theta, p_1)$, and we use $\\lambda$ to denote its parameters.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0jcVdYTg8Wun"
   },
   "source": [
    "We make a *mean field* assumption, whereby we model latent variables independently given the input:\n",
    "\\begin{align}\n",
    "Q(z|x, y, \\lambda) \n",
    "    &= \\prod_{i=1}^{n} Q(z_i|x; \\lambda) \\\\\n",
    "    &= \\prod_{i=1}^{n} \\text{Bern}(z_i|g_i(x; \\lambda)) \n",
    "\\end{align}\n",
    "\n",
    "where $g(x; \\lambda)$ is a NN that maps from $x = \\langle x_1, \\ldots, x_n\\rangle$ to $n$ Bernoulli parameters, each of which, is a probability value (thus $0 < g_i(x; \\lambda) < 1$).\n",
    "\n",
    "Note that though we could condition on $y$ for approximate posterior inference, we are opportunistically leaving it out. This way, $Q$ is directly available at test time for making predictions. The figure below is a graphical depiction of the inference model (we show a dashed arrow from $y$ to $z$ to remind you that in principle the label is also available).\n",
    "\n",
    "<img src=\"https://github.com/probabll/dgm4nlp/raw/master/notebooks/sst/img/inference.png\"  height=\"200\">\n",
    "\n",
    "Here is an example design for $g$:\n",
    "\\begin{align}\n",
    "\\mathbf x_i &= \\text{glove}(x_i) \\\\\n",
    "\\mathbf t_1^n, \\mathbf h &= \\text{encoder}(\\mathbf x_1^n; \\lambda_{\\text{enc}}) \\\\\n",
    "g_i(x; \\lambda) &= \\sigma(\\text{dense}_1(\\mathbf t_i; \\lambda_{\\text{output}}))\n",
    "\\end{align}\n",
    "where\n",
    "* $\\text{glove}$ is a pre-trained embedding function;\n",
    "* $\\text{dense}_1$ is a dense layer with a single output;\n",
    "* and $\\sigma(\\cdot)$ is the sigmoid function, necessary to parameterise a Bernoulli distribution.\n",
    "\n",
    "From now on we will write $Q(z|x, \\lambda)$, that is, without $y.\n",
    "\n",
    "Here we implement this product of Bernoulli distributions:\n",
    "\n",
    "* implement $g$ in the constructor \n",
    "* and the forward pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YLxfcAbuSiFo"
   },
   "outputs": [],
   "source": [
    "class ProductOfBernoullis(nn.Module):\n",
    "    \"\"\"\n",
    "    This is an inference network that parameterises independent Bernoulli distributions.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 embed: nn.Embedding,\n",
    "                 hidden_size: int=200,\n",
    "                 layer: str=\"bow\"):\n",
    "        \"\"\"\n",
    "        :param embed: an embedding layer\n",
    "        :param hidden_suze: hidden size for transformed inputs\n",
    "        :param layer: 'bow' for BoW encoding\n",
    "          you may alternatively implement and 'lstm' option\n",
    "          which uses a biLSTM to transform the inputs         \n",
    "        \"\"\"\n",
    "        super(ProductOfBernoullis, self).__init__()\n",
    "        # 1. we should have an embedding layer \n",
    "        # 2. we may transform the representations\n",
    "        # 3. and we should compute parameters for Bernoulli distributions\n",
    "        \n",
    "        emb_size = embed.weight.shape[1]\n",
    "        # Using twice the units just to make the output as large as that of a biLSTM\n",
    "        enc_size = hidden_size * 2  \n",
    "\n",
    "        self.embed_layer = nn.Sequential(embed)\n",
    "        self.enc_layer = get_encoder(layer, emb_size, hidden_size)\n",
    "        self.logit_layer = nn.Linear(enc_size, 1, bias=True)\n",
    "                \n",
    "        self.report_params()\n",
    "    \n",
    "    def report_params(self):\n",
    "        count = 0\n",
    "        for name, p in self.named_parameters():\n",
    "            if p.requires_grad and \"embed\" not in name:\n",
    "                count += np.prod(list(p.shape))\n",
    "        print(\"{} #params: {}\".format(self.__class__.__name__, count))\n",
    "\n",
    "    def forward(self, x, mask) -> Bernoulli:\n",
    "        \"\"\"\n",
    "        It takes a tensor of tokens (integers)\n",
    "         and predicts a Bernoulli distribution for each position.\n",
    "        \n",
    "        :param x: [B, T]\n",
    "        :param mask: [B, T]\n",
    "        :returns: Bernoulli\n",
    "        \"\"\"\n",
    "\n",
    "        # encode sentence\n",
    "        lengths = mask.long().sum(1)  # [B]\n",
    "        embeddings = self.embed_layer(x)  # [B, T, E]\n",
    "        h, _ = self.enc_layer(embeddings, mask, lengths)  # [B, T, d]\n",
    "\n",
    "        # compute parameters for Bernoulli p(z|x)\n",
    "        # Bernoulli distributions\n",
    "        logits = self.logit_layer(h).squeeze(-1)  # [B, T]\n",
    "\n",
    "        return Bernoulli(logits=logits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fcCu7vkKWvZX"
   },
   "source": [
    "## Parameter Estimation\n",
    "\n",
    "In variational inference, our objective is to maximise the *evidence lowerbound* (ELBO):\n",
    "\n",
    "\\begin{align}\n",
    "\\log P(y|x) &\\ge \\mathbb E_{Q(z|x, y, \\lambda)}\\left[ \\log P(y|x, z, \\theta, p_1) \\right] - \\text{KL}(Q(z|x, y, \\lambda) || P(z|p_1)) \\\\\n",
    "\\text{ELBO}&\\overset{\\text{MF}}{=}\\mathbb E_{Q(z|x, y, \\lambda)}\\left[ \\log P(y|x, z, \\theta, p_1) \\right] - \\sum_{i=1}^n \\text{KL}(Q(z_i|x, \\lambda) || P(z_i|p_1)) \n",
    "\\end{align}\n",
    "\n",
    "where the *mean field* assumption we made implies that the KL term is simply a sum of KL divergences from a Bernoulli posterior to a Bernoulli prior.\n",
    "\n",
    "Note that the ELBO remains intractable, namely, solving the expectation in closed form still requires $2^n$ evaluations of the classifier network. Though unlike the true posterior $P(z|x,y, \\lambda)$, the approximation $Q(z|x,\\lambda)$ is tractable (it does not require an intractable normalisation) and can be used to obtain gradient estimates based on samples.\n",
    "\n",
    "### Gradient of the classifier network\n",
    "\n",
    "For the classifier, we encounter no problem:\n",
    "\n",
    "\\begin{align}\n",
    "\\nabla_\\theta \\text{ELBO} &=\\nabla_\\theta\\sum_{z} Q(z|x, \\lambda)\\log P(y|x,z,\\theta) - \\underbrace{\\nabla_\\theta \\sum_{i=1}^n \\text{KL}(Q(z_i|x, \\lambda) || P(z_i|p_1))}_{\\color{blue}{0}}  \\\\\n",
    "&=\\sum_{z} Q(z|x, \\lambda)\\nabla_\\theta\\log P(y|x,z,\\theta) \\\\\n",
    "&= \\mathbb E_{Q(z|x, \\lambda)}\\left[\\nabla_\\theta\\log P(y|x,z,\\theta) \\right] \\\\\n",
    "&\\overset{\\text{MC}}{\\approx} \\frac{1}{S} \\sum_{s=1}^S \\nabla_\\theta \\log P(y|x, z^{(s)}, \\theta) \n",
    "\\end{align}\n",
    "where $z^{(s)} \\sim Q(z|x,\\lambda)$.\n",
    "\n",
    "\n",
    "### Gradient of the inference network\n",
    "\n",
    "For the inference model, we have to use the *score function estimator* (a.k.a. REINFORCE):\n",
    "\n",
    "\\begin{align}\n",
    "\\nabla_\\lambda \\text{ELBO} &=\\nabla_\\lambda\\sum_{z} Q(z|x, \\lambda)\\log P(y|x,z,\\theta) - \\nabla_\\lambda \\underbrace{\\sum_{i=1}^n \\text{KL}(Q(z_i|x, \\lambda) || P(z_i|p_1))}_{ \\color{blue}{\\text{tractable} }}  \\\\\n",
    "&=\\sum_{z} \\nabla_\\lambda Q(z|x, \\lambda)\\log P(y|x,z,\\theta) - \\sum_{i=1}^n \\nabla_\\lambda \\text{KL}(Q(z_i|x, \\lambda) || P(z_i|p_1))   \\\\\n",
    "&=\\sum_{z}  \\underbrace{Q(z|x, \\lambda) \\nabla_\\lambda \\log Q(z|x, \\lambda)}_{\\nabla_\\lambda Q(z|x, \\lambda)} \\log P(y|x,z,\\theta) - \\sum_{i=1}^n \\nabla_\\lambda \\text{KL}(Q(z_i|x, \\lambda) || P(z_i|p_1))   \\\\\n",
    "&= \\mathbb E_{Q(z|x, \\lambda)}\\left[ \\log P(y|x,z,\\theta) \\nabla_\\lambda \\log Q(z|x, \\lambda) \\right] - \\sum_{i=1}^n \\nabla_\\lambda \\text{KL}(Q(z_i|x, \\lambda) || P(z_i|p_1))   \\\\\n",
    "&\\overset{\\text{MC}}{\\approx} \\left(\\frac{1}{S} \\sum_{s=1}^S  \\log P(y|x, z^{(s)}, \\theta) \\nabla_\\lambda \\log Q(z^{(s)}|x, \\lambda)  \\right) - \\sum_{i=1}^n \\nabla_\\lambda \\text{KL}(Q(z_i|x, \\lambda) || P(z_i|p_1))  \n",
    "\\end{align}\n",
    "\n",
    "where $z^{(s)} \\sim Q(z|x,\\lambda)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6cdfkOYdC0LQ"
   },
   "source": [
    "## Implementation\n",
    "\n",
    "Let's implement the model and the loss (negative ELBO). We work with the notion of a *surrogate loss*, that is, a computation node whose gradients wrt to parameters are equivalent to the gradients we need.\n",
    "\n",
    "For a given sample $z \\sim Q(z|x, \\lambda)$, the following is a single-sample surrogate loss:\n",
    "\n",
    "\\begin{align}\n",
    "\\mathcal S(\\theta, \\lambda|x, y) = \\log P(y|x, z, \\theta) + \\color{red}{\\text{detach}(\\log P(y|x, z, \\theta) )}\\log Q(z|x, \\lambda) - \\sum_{i=1}^n \\text{KL}(Q(z_i|x, \\lambda) || P(z_i|\\phi))\n",
    "\\end{align}\n",
    "where we introduce an auxiliary function such that\n",
    "\\begin{align}\n",
    "\\text{detach}(f(\\alpha))  &= h(\\alpha) \\\\\n",
    "\\nabla_\\beta \\text{detach}(h(\\alpha))  &= 0 \n",
    "\\end{align}\n",
    "or in words, *detach* does not alter the forward call of its argument function $h$, but it alters $h$'s backward call by setting gradients to zero.\n",
    "\n",
    "Show that it's gradients wrt $\\theta$ and $\\lambda$ are exactly what we need:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FednEChaX6WI"
   },
   "source": [
    "\\begin{align}\n",
    "\\nabla_\\theta\\mathcal S(\\theta,\\lambda|x,y)=\\nabla_\\theta\\log P(y|x,z,\\theta)+0=\\nabla_\\theta\\text{ELBO}\n",
    "\\end{align}$$$$\\begin{align}\n",
    "\\nabla_\\lambda \\mathcal S(\\theta, \\lambda|x, y)= 0 + \\underbrace{\\log Q(z|x, \\lambda)\\nabla_\\lambda \\log P(y|x, z, \\theta)  + \\log P(y|x, z, \\theta) \\nabla_\\lambda \\log Q(z|x, \\lambda)}_{\\text{chain rule}}-\\sum_{i=1}^n \\nabla_\\lambda \\text{KL}(Q(z_i|x, \\lambda) || P(z_i|p_1))=\\\\ \n",
    "= 0+ 0 + \\log P(y|x, z, \\theta) \\nabla_\\lambda \\log Q(z|x, \\lambda)-\\sum_{i=1}^n \\nabla_\\lambda \\text{KL}(Q(z_i|x, \\lambda) || P(z_i|p_1))=\\nabla_\\lambda\\text{ELBO}\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OaUMKDShx9T0"
   },
   "source": [
    "Implement the forward pass and loss below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Cnwwk-7tfR02"
   },
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    \"\"\"\n",
    "    \n",
    "    Classifier model:\n",
    "        Z_i ~ Bern(p_1) for i in 1..n\n",
    "        Y|x,z ~ Cat(f([x_i if z_i 1 else 0 for i in 1..n ]))\n",
    "    \n",
    "    Inference model:\n",
    "        Z_i|x ~ Bern(b_i) for i in 1..n\n",
    "            where b_i = g_i(x)\n",
    "    \n",
    "    Objective:\n",
    "        Single-sample MC estimate of ELBO\n",
    "    \n",
    "    Loss: \n",
    "        Surrogate loss\n",
    "\n",
    "    Consists of:\n",
    "        - a product of Bernoulli distributions inference network\n",
    "        - a classifier network\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 vocab: object = None,\n",
    "                 vocab_size: int = 0,\n",
    "                 emb_size: int = 200,\n",
    "                 hidden_size: int = 200,\n",
    "                 num_classes: int = 5,\n",
    "                 prior_p1: float = 0.3,                 \n",
    "                 det_prior: bool = True,\n",
    "                 beta_shape: list = [0.6, 0.6],\n",
    "                 dropout: float = 0.1,\n",
    "                 layer_cls: str = 'bow',\n",
    "                 layer_inf: str = 'bow'):\n",
    "        \"\"\"\n",
    "        :param vocab: Vocabulary\n",
    "        :param vocab_size: necessary for embedding layer\n",
    "        :param emb_size: dimensionality of embedding layer\n",
    "        :param hidden_size: dimensionality of hidden layers\n",
    "        :param num_classes: number of classes\n",
    "        :param prior_p1: (scalar) prior Bernoulli parameter\n",
    "        :param det_prior: (boolean) whether the prior parameter is deterministic\n",
    "        :param beta_shape: (pair of positive scalars) \n",
    "            when the prior parameter is stochastic\n",
    "            it is sampled from a Beta distribution (ignore this at first)\n",
    "        :param dropout: (scalar) dropout rate\n",
    "        :param layer_cls: type of encoder for classification\n",
    "        :param layer_inf: type of encoder for inference\n",
    "        \"\"\"\n",
    "        super(Model, self).__init__()\n",
    "\n",
    "        self.vocab = vocab\n",
    "        self.embed = embed = nn.Embedding(vocab_size, emb_size, padding_idx=1)\n",
    "\n",
    "        self.cls_net = Classifier(\n",
    "            embed=embed, \n",
    "            hidden_size=hidden_size, \n",
    "            output_size=num_classes,\n",
    "            dropout=dropout, \n",
    "            layer=layer_cls\n",
    "        )\n",
    "        \n",
    "        self.inference_net = ProductOfBernoullis(\n",
    "            embed=embed, \n",
    "            hidden_size=hidden_size,\n",
    "            layer=layer_inf\n",
    "        )\n",
    "        \n",
    "        self._prior_p1 = prior_p1\n",
    "        self._det_prior = det_prior\n",
    "        self._beta_shape = beta_shape\n",
    "        \n",
    "    def get_prior_p1(self, p_min=0.001, p_max=0.999):\n",
    "        \"\"\"Return the prior Bernoulli parameter\"\"\"\n",
    "        if self._det_prior:\n",
    "            return torch.tensor(self._prior_p1)\n",
    "        else:\n",
    "            a, b = self._beta_shape\n",
    "            prior_p1 = np.random.beta(a, b)\n",
    "            prior_p1 = max(prior_p1, p_min)\n",
    "            prior_p1 = min(prior_p1, p_max)\n",
    "        return torch.tensor(prior_p1)\n",
    "\n",
    "    def predict(self, py: Categorical, **kwargs):\n",
    "        \"\"\"\n",
    "        Predict deterministically using argmax.\n",
    "        :param py: B Categorical distributions (one per instance in batch)\n",
    "        :return: predictions\n",
    "            [B] sentiment levels\n",
    "        \"\"\"\n",
    "        assert not self.training, \"should be in eval mode for prediction\"\n",
    "        return py.log_probs.argmax(-1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Generate a sequence z with inference model, \n",
    "         then predict with rationale xz, that is, x masked by z.\n",
    "\n",
    "        :param x: [B, T] documents        \n",
    "        :param mask: [B, T] indicates valid positions vs padded positions\n",
    "        :return: \n",
    "            Categorical distributions P(y|x, z)\n",
    "            Bernoulli distributions Q(z|x)\n",
    "            Single sample z ~ Q(z|x) used for the conditional P(y|x, z)\n",
    "        \"\"\"\n",
    "        bern = self.inference_net(x, x != 1)\n",
    "        z = bern.sample() if self.training else (bern.probs >= 0.5).byte()\n",
    "        res = self.cls_net(x, x != 1, z)\n",
    "        return res, bern, z\n",
    "\n",
    "    def get_loss(self,                   \n",
    "                 y, \n",
    "                 py: Categorical,\n",
    "                 qz: Bernoulli, \n",
    "                 z, \n",
    "                 mask,\n",
    "                 iter_i=0, \n",
    "                 # you may ignore the rest of the arguments for the time being\n",
    "                 #  leave them as they are\n",
    "                 kl_weight=1.0,\n",
    "                 min_kl=0.0,\n",
    "                 ll_mean=0.,\n",
    "                 ll_std=1.,\n",
    "                 **kwargs):\n",
    "        \"\"\"\n",
    "        This computes the loss for the whole model.\n",
    "\n",
    "        :param y: target labels [B]\n",
    "        :param py: conditionals P(y|x, z)\n",
    "        :param qz: approximate posteriors Q(z|x)\n",
    "        :param z: sample of binary selectors [B, T]\n",
    "        :param mask: indicates valid positions [B, T]\n",
    "        :param iter_i: indicates the iteration\n",
    "        :param kl_weight: (scalar) multiplies the KL term\n",
    "        :param min_kl: (scalar) sets a minimum for the KL (aka free bits)\n",
    "        :param ll_mean: (scalar) running average of reward\n",
    "        :param ll_std: (scalar) running standard deviation of reward\n",
    "        :return: loss (torch node), terms (dict)\n",
    "        \n",
    "            terms is an OrderedDict that holds the scalar items involved in the loss\n",
    "            e.g. `terms['ll'] = ll.item()` is the log-likelihood term\n",
    "            \n",
    "            Consider tracking the following:\n",
    "            Single-sample ELBO: terms['elbo']\n",
    "            Log-Likelihood log P(y|x,z): terms['ll']\n",
    "            KL: terms['kl']\n",
    "            Score function surrogate log P(y|z, x) log Q(z|x): terms['sf']            \n",
    "            Rate of selected words: terms['selected']\n",
    "        \"\"\"\n",
    "        float_mask = mask.float()\n",
    "        \n",
    "        ll = (py.log_pmf(y).unsqueeze(-1) - ll_mean) * float_mask / ll_std\n",
    "        kl = qz.kl(Bernoulli(probs=self.get_prior_p1())) * float_mask\n",
    "        sf = qz.log_pmf(z.float()) * ll.detach() * float_mask\n",
    "        elbo = ll + sf - kl_weight * kl\n",
    "        \n",
    "        terms = OrderedDict()\n",
    "        terms['elbo'] = elbo.mean().item()\n",
    "        terms['sf'] = sf.mean().item()\n",
    "        terms['selected'] = z.sum().item() * 1.0 / z.shape[0] / z.shape[1]\n",
    "        terms['kl'] = kl.mean().item()\n",
    "        terms['ll'] = ll.mean().item()\n",
    "        \n",
    "        return -elbo.mean(), terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wNQDXTpqWvZa"
   },
   "outputs": [],
   "source": [
    "# This will be used later for maintaining runnin averages of quantites like \n",
    "#  terms in the ELBO\n",
    "from collections import deque\n",
    "\n",
    "class MovingStats:\n",
    "    \n",
    "    def __init__(self, memory=-1):\n",
    "        self.data = deque([])\n",
    "        self.memory = memory\n",
    "        \n",
    "    def append(self, value):\n",
    "        if self.memory != 0:\n",
    "            if self.memory > 0 and len(self.data) == self.memory:\n",
    "                self.data.popleft()\n",
    "            self.data.append(value)\n",
    "        \n",
    "    def mean(self):\n",
    "        if len(self.data):\n",
    "            return np.mean([x for x in self.data])\n",
    "        else:\n",
    "            return 0.\n",
    "    \n",
    "    def std(self):\n",
    "        return np.std(self.data) if len(self.data) > 1 else 1.\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "081YSfU9WvZc"
   },
   "source": [
    "# Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9Pc80gseWvZd"
   },
   "outputs": [],
   "source": [
    "# some helper code for mini batching\n",
    "#  this will take care of annoying things such as \n",
    "#  sorting training instances by length (necessary for pytorch's LSTM, for example)\n",
    "from sst.util import make_kv_string, get_minibatch, prepare_minibatch, print_parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_WVr97kilIRV"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Configuration\n",
      "training_path        : ../sst/data/sst/train.txt\n",
      "dev_path             : ../sst/data/sst/dev.txt\n",
      "test_path            : ../sst/data/sst/test.txt\n",
      "word_vectors         : ../sst/data/sst/glove.840B.300d.filtered.txt\n",
      "prior_p1             :        0.3\n",
      "beta_a               :        0.6\n",
      "beta_b               :        0.6\n",
      "det_prior            :          1\n",
      "num_epochs           :         50\n",
      "print_every          :        100\n",
      "eval_every           :         -1\n",
      "batch_size           :         25\n",
      "eval_batch_size      :         25\n",
      "subphrases           :          0\n",
      "min_phrase_length    :          2\n",
      "lowercase            :          1\n",
      "fix_emb              :          1\n",
      "embed_size           :        300\n",
      "hidden_size          :        150\n",
      "num_layers           :          1\n",
      "dropout              :        0.5\n",
      "layer_inf            : bow       \n",
      "layer_cls            : bow       \n",
      "save_path            : data/results\n",
      "baseline_memory      :       1000\n",
      "min_kl               :        0.0\n",
      "kl_weight            :        0.0\n",
      "kl_inc               :      1e-05\n",
      "lr                   :     0.0002\n",
      "weight_decay         :      1e-05\n",
      "lr_decay             :        0.5\n",
      "patience             :          5\n",
      "cooldown             :          5\n",
      "threshold            :     0.0001\n",
      "min_lr               :      1e-05\n",
      "max_grad_norm        :        5.0\n",
      "Set eval_every to 341\n",
      "Loading data\n",
      "train 8544\n",
      "dev 1101\n",
      "test 2210\n",
      "\n",
      "# Example\n",
      "First dev example: Example(tokens=['it', \"'s\", 'a', 'lovely', 'film', 'with', 'lovely', 'performances', 'by', 'buy', 'and', 'accorsi', '.'], label=3, transitions=[0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1], token_labels=[2, 2, 2, 3, 2, 2, 3, 2, 2, 2, 2, 2, 2])\n",
      "First dev example tokens: ['it', \"'s\", 'a', 'lovely', 'film', 'with', 'lovely', 'performances', 'by', 'buy', 'and', 'accorsi', '.']\n",
      "First dev example label: 3\n"
     ]
    }
   ],
   "source": [
    "import torch.optim\n",
    "# We will use Adam\n",
    "from torch.optim import Adam\n",
    "# and a couple of tricks to reduce learning rate on plateau\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "# here is some helper code to evaluate your model\n",
    "from sst.evaluate import evaluate\n",
    "\n",
    "\n",
    "cfg = dict()\n",
    "\n",
    "# Data\n",
    "cfg['training_path'] = \"../sst/data/sst/train.txt\"\n",
    "cfg['dev_path'] = \"../sst/data/sst/dev.txt\"\n",
    "cfg['test_path'] = \"../sst/data/sst/test.txt\"\n",
    "cfg['word_vectors'] = '../sst/data/sst/glove.840B.300d.filtered.txt'\n",
    "# Model\n",
    "cfg['prior_p1'] = 0.3\n",
    "cfg['beta_a'] = 0.6\n",
    "cfg['beta_b'] = 0.6\n",
    "cfg['det_prior'] = True\n",
    "# Architecture\n",
    "cfg['num_epochs'] = 50\n",
    "cfg['print_every'] = 100\n",
    "cfg['eval_every'] = -1\n",
    "cfg['batch_size'] = 25\n",
    "cfg['eval_batch_size'] = 25\n",
    "cfg['subphrases'] = False\n",
    "cfg['min_phrase_length'] = 2\n",
    "cfg['lowercase'] = True\n",
    "cfg['fix_emb'] = True\n",
    "cfg['embed_size'] = 300\n",
    "cfg['hidden_size'] = 150\n",
    "cfg['num_layers'] = 1\n",
    "cfg['dropout'] = 0.5\n",
    "cfg['layer_inf'] = 'bow'\n",
    "cfg['layer_cls'] = 'bow'\n",
    "cfg['save_path'] = 'data/results'\n",
    "cfg['baseline_memory'] = 1000\n",
    "cfg['min_kl'] = 0.  # use more than 0 to enable free bits\n",
    "cfg['kl_weight'] = 0.0  # start from zero to enable annealing\n",
    "cfg['kl_inc'] = 0.00001\n",
    "# Optimiser (leave as is)\n",
    "cfg['lr'] = 0.0002\n",
    "cfg['weight_decay'] = 1e-5\n",
    "cfg['lr_decay'] = 0.5\n",
    "cfg['patience'] = 5\n",
    "cfg['cooldown'] = 5\n",
    "cfg['threshold'] = 1e-4\n",
    "cfg['min_lr'] = 1e-5\n",
    "cfg['max_grad_norm'] = 5.\n",
    "\n",
    "\n",
    "print('# Configuration')\n",
    "for k, v in cfg.items():\n",
    "    print(\"{:20} : {:10}\".format(k, v))\n",
    "\n",
    "\n",
    "iters_per_epoch = len(train_data) // cfg[\"batch_size\"]\n",
    "\n",
    "if cfg[\"eval_every\"] == -1:\n",
    "    eval_every = iters_per_epoch\n",
    "    print(\"Set eval_every to {}\".format(iters_per_epoch))\n",
    "\n",
    "\n",
    "# Let's load the data into memory.\n",
    "print(\"Loading data\")\n",
    "train_data = list(examplereader(\n",
    "    cfg['training_path'],\n",
    "    lower=cfg['lowercase'], \n",
    "    subphrases=cfg['subphrases'],\n",
    "    min_length=cfg['min_phrase_length']))\n",
    "dev_data = list(examplereader(cfg['dev_path'], lower=cfg['lowercase']))\n",
    "test_data = list(examplereader(cfg['test_path'], lower=cfg['lowercase']))\n",
    "\n",
    "print(\"train\", len(train_data))\n",
    "print(\"dev\", len(dev_data))\n",
    "print(\"test\", len(test_data))\n",
    "\n",
    "print('\\n# Example')\n",
    "example = dev_data[0]\n",
    "print(\"First dev example:\", example)\n",
    "print(\"First dev example tokens:\", example.tokens)\n",
    "print(\"First dev example label:\", example.label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6PMqtVj0WvZf",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def train():\n",
    "\n",
    "    # Create a vocabulary object to map str <-> int\n",
    "    vocab = Vocabulary()  # populated by load_glove\n",
    "    glove_path = cfg[\"word_vectors\"]\n",
    "    vectors = load_glove(glove_path, vocab)\n",
    "\n",
    "    # You may consider using tensorboardX\n",
    "    # writer = SummaryWriter(log_dir=cfg[\"save_path\"])\n",
    "\n",
    "    # Map the sentiment labels 0-4 to a more readable form (and the opposite)\n",
    "    i2t = [\"very negative\", \"negative\", \"neutral\", \"positive\", \"very positive\"]\n",
    "    t2i = OrderedDict({p: i for p, i in zip(i2t, range(len(i2t)))})\n",
    "\n",
    "\n",
    "    print('\\n# Constructing model')\n",
    "    model = Model(\n",
    "        vocab_size=len(vocab.w2i), \n",
    "        emb_size=cfg[\"embed_size\"],\n",
    "        hidden_size=cfg[\"hidden_size\"], \n",
    "        num_classes=len(t2i),\n",
    "        prior_p1=cfg['prior_p1'],\n",
    "        det_prior=cfg['det_prior'],\n",
    "        beta_shape=[cfg['beta_a'], cfg['beta_b']],\n",
    "        vocab=vocab, \n",
    "        dropout=cfg[\"dropout\"], \n",
    "        layer_cls=cfg[\"layer_cls\"],\n",
    "        layer_inf=cfg[\"layer_inf\"]\n",
    "    )\n",
    "\n",
    "    print('\\n# Loading embeddings')\n",
    "    with torch.no_grad():\n",
    "        model.embed.weight.data.copy_(torch.from_numpy(vectors))\n",
    "        if cfg[\"fix_emb\"]:\n",
    "            print(\"fixed word embeddings\")\n",
    "            model.embed.weight.requires_grad = False\n",
    "        model.embed.weight[1] = 0.  # padding zero\n",
    "\n",
    "        \n",
    "    # Congigure optimiser\n",
    "    optimizer = Adam(model.parameters(), lr=cfg[\"lr\"],\n",
    "                     weight_decay=cfg[\"weight_decay\"])\n",
    "    # and learning rate scheduler\n",
    "    scheduler = ReduceLROnPlateau(\n",
    "        optimizer, mode=\"min\", factor=cfg[\"lr_decay\"], patience=cfg[\"patience\"],\n",
    "        verbose=True, cooldown=cfg[\"cooldown\"], threshold=cfg[\"threshold\"],\n",
    "        min_lr=cfg[\"min_lr\"]\n",
    "    )\n",
    "\n",
    "    # Prepare a few auxiliary variables\n",
    "    iter_i = 0\n",
    "    train_loss = 0.\n",
    "    print_num = 0\n",
    "    losses = []\n",
    "    accuracies = []\n",
    "    best_eval = 1.0e9\n",
    "    best_iter = 0\n",
    "\n",
    "    model = model.to(device)\n",
    "\n",
    "    # Some debugging info\n",
    "    print(model)\n",
    "    print_parameters(model)\n",
    "\n",
    "    batch_size = cfg['batch_size']\n",
    "    eval_batch_size = cfg['eval_batch_size']\n",
    "    print_every = cfg['print_every']\n",
    "\n",
    "    # Parameters of tricks to better optimise the ELBO \n",
    "    kl_inc = cfg['kl_inc']\n",
    "    kl_weight = cfg['kl_weight']\n",
    "    min_kl = cfg['min_kl']\n",
    "    # Running estimates for baselines\n",
    "    ll_moving_stats = MovingStats(cfg['baseline_memory'])\n",
    "\n",
    "    while True:  # when we run out of examples, shuffle and continue\n",
    "        epoch = iter_i // iters_per_epoch\n",
    "        if epoch > cfg['num_epochs']:\n",
    "            break\n",
    "        \n",
    "        for batch in get_minibatch(train_data, batch_size=batch_size, shuffle=True):\n",
    "\n",
    "            epoch = iter_i // iters_per_epoch\n",
    "            if epoch > cfg['num_epochs']:\n",
    "                break\n",
    "\n",
    "            # forward pass\n",
    "            model.train()\n",
    "            x, y, _ = prepare_minibatch(batch, model.vocab, device=device)\n",
    "            \n",
    "            mask = (x != 1)\n",
    "            py, qz, z = model(x)\n",
    "\n",
    "            # \"KL annealing\"\n",
    "            kl_weight += kl_inc\n",
    "            if kl_weight > 1.:\n",
    "                kl_weight = 1.0\n",
    "                \n",
    "            loss, terms = model.get_loss(\n",
    "                y,\n",
    "                py=py, \n",
    "                qz=qz,\n",
    "                z=z,\n",
    "                mask=mask, \n",
    "                kl_weight=kl_weight,\n",
    "                min_kl=min_kl,\n",
    "                ll_mean=ll_moving_stats.mean(),\n",
    "                ll_std=ll_moving_stats.std(),\n",
    "                iter_i=iter_i)\n",
    "\n",
    "            train_loss += loss.item()\n",
    "            \n",
    "            # keep an running estimate of the reward (log P(y|x,z))\n",
    "            ll_moving_stats.append(terms['ll'])\n",
    "\n",
    "            # backward pass\n",
    "            model.zero_grad()  # erase previous gradients\n",
    "\n",
    "            loss.backward()  # compute new gradients\n",
    "\n",
    "            # gradient clipping generally helps\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=cfg['max_grad_norm'])\n",
    "\n",
    "            # update weights\n",
    "            optimizer.step()\n",
    "\n",
    "            print_num += 1\n",
    "            iter_i += 1\n",
    "\n",
    "            # print info\n",
    "            if iter_i % print_every == 0:\n",
    "\n",
    "                train_loss = train_loss / print_every\n",
    "\n",
    "                print_str = make_kv_string(terms)\n",
    "                print(\"Epoch %r Iter %r loss=%.4f %s\" %\n",
    "                      (epoch, iter_i, train_loss, print_str))\n",
    "                losses.append(train_loss)\n",
    "                print_num = 0\n",
    "                train_loss = 0.\n",
    "\n",
    "            # evaluate\n",
    "            if iter_i % eval_every == 0:\n",
    "\n",
    "                dev_eval, rationales = evaluate(\n",
    "                    model, dev_data, \n",
    "                    batch_size=eval_batch_size, \n",
    "                    device=device,\n",
    "                    cfg=cfg, iter_i=iter_i\n",
    "                )\n",
    "                accuracies.append(dev_eval[\"acc\"])\n",
    "\n",
    "                print(\"\\n# epoch %r iter %r: dev %s\" % (\n",
    "                    epoch, iter_i, make_kv_string(dev_eval)))\n",
    "                \n",
    "                for exid in range(3):\n",
    "                    print(' dev%d [gold=%d,pred=%d]:' % (exid, dev_data[exid].label, rationales[exid][1]),  \n",
    "                          ' '.join(rationales[exid][0]))\n",
    "                print()\n",
    "\n",
    "                # adjust learning rate\n",
    "                scheduler.step(dev_eval[\"loss\"])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "A5uYKcw-WvZl",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "# Constructing model\n",
      "Classifier #params: 1505\n",
      "ProductOfBernoullis #params: 301\n",
      "\n",
      "# Loading embeddings\n",
      "fixed word embeddings\n",
      "Model(\n",
      "  (embed): Embedding(20727, 300, padding_idx=1)\n",
      "  (cls_net): Classifier(\n",
      "    (embed_layer): Sequential(\n",
      "      (0): Embedding(20727, 300, padding_idx=1)\n",
      "    )\n",
      "    (enc_layer): BagOfWordsEncoder()\n",
      "    (output_layer): Sequential(\n",
      "      (0): Dropout(p=0.5)\n",
      "      (1): Linear(in_features=300, out_features=5, bias=True)\n",
      "      (2): LogSoftmax()\n",
      "    )\n",
      "  )\n",
      "  (inference_net): ProductOfBernoullis(\n",
      "    (embed_layer): Sequential(\n",
      "      (0): Embedding(20727, 300, padding_idx=1)\n",
      "    )\n",
      "    (enc_layer): BagOfWordsEncoder()\n",
      "    (logit_layer): Linear(in_features=300, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "embed.weight             [20727, 300] requires_grad=False\n",
      "cls_net.output_layer.1.weight [5, 300]     requires_grad=True\n",
      "cls_net.output_layer.1.bias [5]          requires_grad=True\n",
      "inference_net.logit_layer.weight [1, 300]     requires_grad=True\n",
      "inference_net.logit_layer.bias [1]          requires_grad=True\n",
      "\n",
      "Total parameters: 6219906\n",
      "\n",
      "Shuffling training data\n",
      "Epoch 0 Iter 100 loss=0.3081 elbo -0.4180 sf 0.9329 selected 0.4903 kl 0.0753 ll -1.3508\n",
      "Epoch 0 Iter 200 loss=0.3464 elbo -0.3840 sf 0.8700 selected 0.5038 kl 0.0695 ll -1.2539\n",
      "Epoch 0 Iter 300 loss=0.3522 elbo -0.3134 sf 0.6507 selected 0.5313 kl 0.0598 ll -0.9640\n",
      "\n",
      "# epoch 0 iter 341: dev loss 0.4069 elbo -0.4069 sf 0.4720 selected 0.9426 kl 0.0743 ll -0.8046 acc 0.3143\n",
      " dev0 [gold=3,pred=3]: **it** **'s** **a** **lovely** **film** **with** **lovely** **performances** by **buy** **and** accorsi **.**\n",
      " dev1 [gold=2,pred=3]: **no** **one** goes **unindicted** **here** **,** **which** **is** **probably** **for** **the** **best** **.**\n",
      " dev2 [gold=3,pred=3]: **and** **if** **you** **'re** **not** **nearly** moved **to** **tears** by **a** couple **of** **scenes** **,** **you** **'ve** got ice water **in** **your** **veins** **.**\n",
      "\n",
      "Shuffling training data\n",
      "Epoch 1 Iter 400 loss=0.3466 elbo -0.3535 sf 0.7345 selected 0.5354 kl 0.0722 ll -1.0878\n",
      "Epoch 1 Iter 500 loss=0.3618 elbo -0.3983 sf 0.8843 selected 0.5463 kl 0.0787 ll -1.2822\n",
      "Epoch 1 Iter 600 loss=0.3589 elbo -0.2746 sf 0.6612 selected 0.4853 kl 0.0625 ll -0.9354\n",
      "\n",
      "# epoch 1 iter 682: dev loss 0.3991 elbo -0.3991 sf 0.4740 selected 0.9223 kl 0.0724 ll -0.8006 acc 0.3188\n",
      " dev0 [gold=3,pred=3]: **it** **'s** **a** **lovely** **film** **with** **lovely** **performances** by **buy** **and** accorsi **.**\n",
      " dev1 [gold=2,pred=3]: **no** **one** goes **unindicted** **here** **,** **which** **is** **probably** **for** **the** **best** **.**\n",
      " dev2 [gold=3,pred=3]: **and** if **you** **'re** not **nearly** moved to **tears** by **a** couple **of** **scenes** **,** **you** **'ve** got ice water **in** **your** **veins** **.**\n",
      "\n",
      "Shuffling training data\n",
      "Epoch 2 Iter 700 loss=0.3497 elbo -0.3098 sf 0.6783 selected 0.5433 kl 0.0635 ll -0.9876\n",
      "Epoch 2 Iter 800 loss=0.3465 elbo -0.2946 sf 0.6378 selected 0.5096 kl 0.0695 ll -0.9319\n",
      "Epoch 2 Iter 900 loss=0.3335 elbo -0.3158 sf 0.6861 selected 0.5263 kl 0.0684 ll -1.0012\n",
      "Epoch 2 Iter 1000 loss=0.3429 elbo -0.3713 sf 0.8466 selected 0.5187 kl 0.0709 ll -1.2172\n",
      "\n",
      "# epoch 2 iter 1023: dev loss 0.3948 elbo -0.3948 sf 0.4725 selected 0.9134 kl 0.0715 ll -0.7958 acc 0.3243\n",
      " dev0 [gold=3,pred=3]: **it** **'s** **a** **lovely** **film** **with** **lovely** **performances** by **buy** **and** accorsi **.**\n",
      " dev1 [gold=2,pred=3]: **no** **one** goes **unindicted** **here** **,** **which** **is** probably **for** **the** **best** **.**\n",
      " dev2 [gold=3,pred=3]: **and** if **you** **'re** not **nearly** moved to **tears** by **a** couple **of** **scenes** **,** **you** **'ve** got ice water **in** **your** **veins** **.**\n",
      "\n",
      "Shuffling training data\n",
      "Epoch 3 Iter 1100 loss=0.3653 elbo -0.3459 sf 0.7418 selected 0.5190 kl 0.0673 ll -1.0870\n",
      "Epoch 3 Iter 1200 loss=0.3454 elbo -0.4238 sf 0.9325 selected 0.5590 kl 0.0771 ll -1.3554\n",
      "Epoch 3 Iter 1300 loss=0.3367 elbo -0.3283 sf 0.6839 selected 0.5298 kl 0.0652 ll -1.0114\n",
      "\n",
      "# epoch 3 iter 1364: dev loss 0.4179 elbo -0.4179 sf 0.4521 selected 0.9476 kl 0.0798 ll -0.7902 acc 0.3351\n",
      " dev0 [gold=3,pred=3]: **it** **'s** **a** **lovely** **film** **with** **lovely** **performances** by **buy** **and** accorsi **.**\n",
      " dev1 [gold=2,pred=3]: **no** **one** goes **unindicted** **here** **,** **which** **is** **probably** **for** **the** **best** **.**\n",
      " dev2 [gold=3,pred=3]: **and** **if** **you** **'re** not **nearly** moved **to** **tears** by **a** couple **of** **scenes** **,** **you** **'ve** got ice **water** **in** **your** **veins** **.**\n",
      "\n",
      "Shuffling training data\n",
      "Epoch 4 Iter 1400 loss=0.3475 elbo -0.3020 sf 0.6448 selected 0.5274 kl 0.0703 ll -0.9458\n",
      "Epoch 4 Iter 1500 loss=0.3409 elbo -0.4805 sf 0.9673 selected 0.5425 kl 0.0916 ll -1.4464\n",
      "Epoch 4 Iter 1600 loss=0.3284 elbo -0.2975 sf 0.5934 selected 0.5478 kl 0.0735 ll -0.8896\n",
      "Epoch 4 Iter 1700 loss=0.3490 elbo -0.4151 sf 0.8732 selected 0.5423 kl 0.0809 ll -1.2869\n",
      "\n",
      "# epoch 4 iter 1705: dev loss 0.4296 elbo -0.4296 sf 0.4393 selected 0.9490 kl 0.0846 ll -0.7843 acc 0.3370\n",
      " dev0 [gold=3,pred=1]: **it** **'s** **a** **lovely** **film** **with** **lovely** **performances** by **buy** **and** accorsi **.**\n",
      " dev1 [gold=2,pred=3]: **no** **one** goes **unindicted** **here** **,** **which** **is** **probably** **for** **the** **best** **.**\n",
      " dev2 [gold=3,pred=3]: **and** **if** **you** **'re** not **nearly** moved **to** **tears** by **a** **couple** **of** **scenes** **,** **you** **'ve** got ice **water** **in** **your** **veins** **.**\n",
      "\n",
      "Shuffling training data\n",
      "Epoch 5 Iter 1800 loss=0.3479 elbo -0.3931 sf 0.8421 selected 0.5381 kl 0.1038 ll -1.2333\n",
      "Epoch 5 Iter 1900 loss=0.3433 elbo -0.4448 sf 0.8054 selected 0.5408 kl 0.0736 ll -1.2488\n",
      "Epoch 5 Iter 2000 loss=0.3476 elbo -0.3814 sf 0.7669 selected 0.5351 kl 0.0882 ll -1.1465\n",
      "\n",
      "# epoch 5 iter 2046: dev loss 0.4462 elbo -0.4462 sf 0.4232 selected 0.9561 kl 0.0911 ll -0.7782 acc 0.3406\n",
      " dev0 [gold=3,pred=1]: **it** **'s** **a** **lovely** **film** **with** **lovely** **performances** by **buy** **and** accorsi **.**\n",
      " dev1 [gold=2,pred=3]: **no** **one** **goes** **unindicted** **here** **,** **which** **is** **probably** **for** **the** **best** **.**\n",
      " dev2 [gold=3,pred=3]: **and** **if** **you** **'re** **not** **nearly** moved **to** **tears** by **a** **couple** **of** **scenes** **,** **you** **'ve** got ice **water** **in** **your** **veins** **.**\n",
      "\n",
      "Shuffling training data\n",
      "Epoch 6 Iter 2100 loss=0.3524 elbo -0.3143 sf 0.6702 selected 0.5280 kl 0.0857 ll -0.9827\n",
      "Epoch 6 Iter 2200 loss=0.3611 elbo -0.2946 sf 0.5953 selected 0.5395 kl 0.0694 ll -0.8884\n",
      "Epoch 6 Iter 2300 loss=0.3415 elbo -0.4370 sf 0.8724 selected 0.5611 kl 0.0926 ll -1.3072\n",
      "\n",
      "# epoch 6 iter 2387: dev loss 0.4305 elbo -0.4305 sf 0.4298 selected 0.9561 kl 0.0865 ll -0.7738 acc 0.3406\n",
      " dev0 [gold=3,pred=1]: **it** **'s** **a** **lovely** **film** **with** **lovely** **performances** by **buy** **and** accorsi **.**\n",
      " dev1 [gold=2,pred=3]: **no** **one** **goes** **unindicted** **here** **,** **which** **is** **probably** **for** **the** **best** **.**\n",
      " dev2 [gold=3,pred=1]: **and** **if** **you** **'re** **not** **nearly** moved **to** **tears** by **a** **couple** **of** **scenes** **,** **you** **'ve** got ice **water** **in** **your** **veins** **.**\n",
      "\n",
      "Shuffling training data\n",
      "Epoch 7 Iter 2400 loss=0.3436 elbo -0.3303 sf 0.6933 selected 0.5124 kl 0.0856 ll -1.0216\n",
      "Epoch 7 Iter 2500 loss=0.3530 elbo -0.2994 sf 0.6301 selected 0.5191 kl 0.0714 ll -0.9277\n",
      "Epoch 7 Iter 2600 loss=0.3491 elbo -0.3591 sf 0.7409 selected 0.5238 kl 0.0623 ll -1.0984\n",
      "Epoch 7 Iter 2700 loss=0.3349 elbo -0.3132 sf 0.6416 selected 0.5620 kl 0.0746 ll -0.9528\n",
      "\n",
      "# epoch 7 iter 2728: dev loss 0.4157 elbo -0.4157 sf 0.4360 selected 0.9335 kl 0.0830 ll -0.7687 acc 0.3488\n",
      " dev0 [gold=3,pred=1]: **it** **'s** **a** **lovely** **film** **with** **lovely** **performances** by **buy** **and** accorsi **.**\n",
      " dev1 [gold=2,pred=3]: **no** **one** goes **unindicted** **here** **,** **which** **is** **probably** **for** **the** **best** **.**\n",
      " dev2 [gold=3,pred=1]: **and** if **you** **'re** **not** **nearly** moved to **tears** by **a** **couple** **of** **scenes** **,** **you** **'ve** got ice **water** **in** **your** **veins** **.**\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shuffling training data\n",
      "Epoch 8 Iter 2800 loss=0.3314 elbo -0.4865 sf 1.0117 selected 0.5222 kl 0.0845 ll -1.4958\n",
      "Epoch 8 Iter 2900 loss=0.3398 elbo -0.2892 sf 0.6332 selected 0.5271 kl 0.0750 ll -0.9202\n",
      "Epoch 8 Iter 3000 loss=0.3356 elbo -0.2834 sf 0.5814 selected 0.5102 kl 0.0677 ll -0.8627\n",
      "\n",
      "# epoch 8 iter 3069: dev loss 0.3988 elbo -0.3988 sf 0.4436 selected 0.9200 kl 0.0777 ll -0.7647 acc 0.3515\n",
      " dev0 [gold=3,pred=1]: **it** **'s** **a** **lovely** **film** **with** **lovely** **performances** by **buy** **and** accorsi **.**\n",
      " dev1 [gold=2,pred=3]: **no** **one** goes **unindicted** **here** **,** **which** **is** **probably** for **the** **best** **.**\n",
      " dev2 [gold=3,pred=1]: **and** if **you** **'re** **not** **nearly** moved to **tears** by **a** couple **of** **scenes** **,** **you** **'ve** got ice water **in** **your** **veins** **.**\n",
      "\n",
      "Epoch     8: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Shuffling training data\n",
      "Epoch 9 Iter 3100 loss=0.3416 elbo -0.2397 sf 0.4859 selected 0.5395 kl 0.0615 ll -0.7236\n",
      "Epoch 9 Iter 3200 loss=0.3289 elbo -0.3923 sf 0.8336 selected 0.5153 kl 0.0867 ll -1.2232\n",
      "Epoch 9 Iter 3300 loss=0.3387 elbo -0.4754 sf 1.0047 selected 0.5326 kl 0.0802 ll -1.4775\n",
      "Epoch 9 Iter 3400 loss=0.3317 elbo -0.3122 sf 0.6492 selected 0.5286 kl 0.0718 ll -0.9590\n",
      "\n",
      "# epoch 9 iter 3410: dev loss 0.3935 elbo -0.3935 sf 0.4443 selected 0.8937 kl 0.0758 ll -0.7620 acc 0.3597\n",
      " dev0 [gold=3,pred=1]: **it** **'s** **a** **lovely** **film** **with** **lovely** **performances** by **buy** **and** accorsi **.**\n",
      " dev1 [gold=2,pred=3]: **no** **one** goes **unindicted** **here** **,** **which** **is** **probably** for the **best** **.**\n",
      " dev2 [gold=3,pred=1]: **and** if **you** **'re** **not** **nearly** moved to **tears** by **a** couple **of** **scenes** **,** **you** **'ve** got ice water **in** **your** **veins** **.**\n",
      "\n",
      "Shuffling training data\n",
      "Epoch 10 Iter 3500 loss=0.3316 elbo -0.3332 sf 0.6985 selected 0.5135 kl 0.0719 ll -1.0292\n",
      "Epoch 10 Iter 3600 loss=0.3379 elbo -0.3250 sf 0.7283 selected 0.5222 kl 0.0664 ll -1.0510\n",
      "Epoch 10 Iter 3700 loss=0.3438 elbo -0.3819 sf 0.8644 selected 0.4882 kl 0.0753 ll -1.2435\n",
      "\n",
      "# epoch 10 iter 3751: dev loss 0.3888 elbo -0.3888 sf 0.4455 selected 0.8770 kl 0.0739 ll -0.7603 acc 0.3697\n",
      " dev0 [gold=3,pred=1]: **it** **'s** **a** **lovely** **film** **with** **lovely** **performances** by **buy** and accorsi **.**\n",
      " dev1 [gold=2,pred=3]: **no** **one** goes **unindicted** **here** **,** **which** **is** **probably** for the **best** **.**\n",
      " dev2 [gold=3,pred=1]: and if **you** **'re** **not** **nearly** moved to **tears** by **a** couple **of** **scenes** **,** **you** **'ve** got ice water **in** **your** **veins** **.**\n",
      "\n",
      "Shuffling training data\n",
      "Epoch 11 Iter 3800 loss=0.3414 elbo -0.3352 sf 0.6953 selected 0.5167 kl 0.0817 ll -1.0275\n",
      "Epoch 11 Iter 3900 loss=0.3321 elbo -0.3224 sf 0.6375 selected 0.5400 kl 0.0669 ll -0.9573\n",
      "Epoch 11 Iter 4000 loss=0.3492 elbo -0.4633 sf 0.9654 selected 0.5150 kl 0.0913 ll -1.4251\n",
      "\n",
      "# epoch 11 iter 4092: dev loss 0.3944 elbo -0.3944 sf 0.4408 selected 0.8790 kl 0.0763 ll -0.7589 acc 0.3733\n",
      " dev0 [gold=3,pred=1]: **it** **'s** **a** **lovely** **film** **with** **lovely** **performances** by **buy** and accorsi **.**\n",
      " dev1 [gold=2,pred=3]: **no** **one** goes **unindicted** **here** **,** **which** **is** **probably** for the **best** **.**\n",
      " dev2 [gold=3,pred=1]: and if **you** **'re** **not** **nearly** moved to **tears** by **a** couple **of** **scenes** **,** **you** **'ve** got ice water **in** **your** **veins** **.**\n",
      "\n",
      "Epoch 12 Iter 4100 loss=0.3335 elbo -0.3764 sf 0.7695 selected 0.5624 kl 0.0902 ll -1.1423\n",
      "Shuffling training data\n",
      "Epoch 12 Iter 4200 loss=0.3294 elbo -0.3474 sf 0.7273 selected 0.5210 kl 0.0647 ll -1.0720\n",
      "Epoch 12 Iter 4300 loss=0.3563 elbo -0.2927 sf 0.6237 selected 0.5210 kl 0.0631 ll -0.9136\n",
      "Epoch 12 Iter 4400 loss=0.3282 elbo -0.4138 sf 0.8461 selected 0.5400 kl 0.0706 ll -1.2568\n",
      "\n",
      "# epoch 12 iter 4433: dev loss 0.3845 elbo -0.3845 sf 0.4436 selected 0.8343 kl 0.0717 ll -0.7563 acc 0.3797\n",
      " dev0 [gold=3,pred=1]: **it** **'s** a **lovely** **film** with **lovely** **performances** by buy and accorsi **.**\n",
      " dev1 [gold=2,pred=3]: **no** **one** goes **unindicted** **here** **,** which is **probably** for the **best** **.**\n",
      " dev2 [gold=3,pred=1]: and if **you** **'re** **not** **nearly** moved to **tears** by a couple **of** **scenes** **,** **you** **'ve** got ice water **in** **your** **veins** **.**\n",
      "\n",
      "Shuffling training data\n",
      "Epoch 13 Iter 4500 loss=0.3316 elbo -0.2673 sf 0.5228 selected 0.5351 kl 0.0653 ll -0.7871\n",
      "Epoch 13 Iter 4600 loss=0.3357 elbo -0.2916 sf 0.6477 selected 0.5405 kl 0.0663 ll -0.9363\n",
      "Epoch 13 Iter 4700 loss=0.3367 elbo -0.4365 sf 0.9920 selected 0.5166 kl 0.0834 ll -1.4246\n",
      "\n",
      "# epoch 13 iter 4774: dev loss 0.3795 elbo -0.3795 sf 0.4439 selected 0.8018 kl 0.0677 ll -0.7557 acc 0.3851\n",
      " dev0 [gold=3,pred=1]: it **'s** a **lovely** **film** with **lovely** **performances** by buy and accorsi **.**\n",
      " dev1 [gold=2,pred=3]: **no** one goes **unindicted** here **,** which is **probably** for the **best** **.**\n",
      " dev2 [gold=3,pred=1]: and if **you** **'re** not nearly moved to **tears** by a couple **of** **scenes** **,** **you** **'ve** got ice water **in** **your** **veins** **.**\n",
      "\n",
      "Shuffling training data\n",
      "Epoch 14 Iter 4800 loss=0.3282 elbo -0.4070 sf 0.7755 selected 0.5385 kl 0.0692 ll -1.1792\n",
      "Epoch 14 Iter 4900 loss=0.3392 elbo -0.3558 sf 0.7818 selected 0.5100 kl 0.0617 ll -1.1346\n",
      "Epoch 14 Iter 5000 loss=0.3287 elbo -0.2456 sf 0.5079 selected 0.5169 kl 0.0569 ll -0.7506\n",
      "Epoch 14 Iter 5100 loss=0.3516 elbo -0.3013 sf 0.6871 selected 0.5105 kl 0.0599 ll -0.9854\n",
      "\n",
      "# epoch 14 iter 5115: dev loss 0.3785 elbo -0.3785 sf 0.4424 selected 0.2767 kl 0.0662 ll -0.7548 acc 0.3896\n",
      " dev0 [gold=3,pred=1]: it **'s** a **lovely** **film** with **lovely** **performances** by buy and accorsi **.**\n",
      " dev1 [gold=2,pred=1]: **no** one goes **unindicted** here , which is **probably** for the **best** **.**\n",
      " dev2 [gold=3,pred=1]: and if you **'re** not nearly moved to **tears** by a couple of **scenes** , you **'ve** got ice water **in** **your** **veins** **.**\n",
      "\n",
      "Shuffling training data\n",
      "Epoch 15 Iter 5200 loss=0.3265 elbo -0.4148 sf 0.8526 selected 0.5006 kl 0.0729 ll -1.2637\n",
      "Epoch 15 Iter 5300 loss=0.3341 elbo -0.3587 sf 0.7686 selected 0.4946 kl 0.0611 ll -1.1241\n",
      "Epoch 15 Iter 5400 loss=0.3333 elbo -0.3620 sf 0.7418 selected 0.5300 kl 0.0637 ll -1.1003\n",
      "\n",
      "# epoch 15 iter 5456: dev loss 0.3797 elbo -0.3797 sf 0.4402 selected 0.2964 kl 0.0658 ll -0.7541 acc 0.3869\n",
      " dev0 [gold=3,pred=1]: it **'s** a **lovely** **film** with **lovely** **performances** by buy and accorsi **.**\n",
      " dev1 [gold=2,pred=3]: **no** one goes **unindicted** here **,** which is probably for the **best** **.**\n",
      " dev2 [gold=3,pred=1]: and if you **'re** not nearly moved to **tears** by a couple of **scenes** **,** you **'ve** got ice water **in** **your** **veins** **.**\n",
      "\n",
      "Shuffling training data\n",
      "Epoch 16 Iter 5500 loss=0.3313 elbo -0.3414 sf 0.6967 selected 0.5044 kl 0.0654 ll -1.0345\n",
      "Epoch 16 Iter 5600 loss=0.3314 elbo -0.3256 sf 0.7210 selected 0.4989 kl 0.0565 ll -1.0435\n",
      "Epoch 16 Iter 5700 loss=0.3258 elbo -0.2348 sf 0.4708 selected 0.5435 kl 0.0580 ll -0.7023\n",
      "\n",
      "# epoch 16 iter 5797: dev loss 0.3787 elbo -0.3787 sf 0.4397 selected 0.2498 kl 0.0660 ll -0.7525 acc 0.3915\n",
      " dev0 [gold=3,pred=1]: it **'s** a **lovely** **film** with **lovely** **performances** by buy and accorsi .\n",
      " dev1 [gold=2,pred=1]: **no** one goes **unindicted** here , which is probably for the **best** .\n",
      " dev2 [gold=3,pred=1]: and if you **'re** not nearly moved to **tears** by a couple of **scenes** , you **'ve** got ice water **in** **your** **veins** .\n",
      "\n",
      "Epoch 17 Iter 5800 loss=0.3488 elbo -0.2922 sf 0.6156 selected 0.4871 kl 0.0691 ll -0.9037\n",
      "Shuffling training data\n",
      "Epoch 17 Iter 5900 loss=0.3353 elbo -0.2751 sf 0.5930 selected 0.5333 kl 0.0631 ll -0.8644\n",
      "Epoch 17 Iter 6000 loss=0.3371 elbo -0.3876 sf 0.8019 selected 0.4735 kl 0.0730 ll -1.1851\n",
      "Epoch 17 Iter 6100 loss=0.3384 elbo -0.3859 sf 0.7767 selected 0.5100 kl 0.0570 ll -1.1591\n",
      "\n",
      "# epoch 17 iter 6138: dev loss 0.3801 elbo -0.3801 sf 0.4361 selected 0.2179 kl 0.0601 ll -0.7560 acc 0.3933\n",
      " dev0 [gold=3,pred=1]: it **'s** a **lovely** **film** with **lovely** **performances** by buy and accorsi .\n",
      " dev1 [gold=2,pred=1]: **no** one goes **unindicted** here , which is probably for the best .\n",
      " dev2 [gold=3,pred=1]: and if you **'re** not nearly moved to **tears** by a couple of **scenes** , you **'ve** got ice water in your **veins** .\n",
      "\n",
      "Shuffling training data\n",
      "Epoch 18 Iter 6200 loss=0.3315 elbo -0.3481 sf 0.7161 selected 0.4846 kl 0.0524 ll -1.0610\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18 Iter 6300 loss=0.3324 elbo -0.2071 sf 0.4345 selected 0.5098 kl 0.0432 ll -0.6389\n",
      "Epoch 18 Iter 6400 loss=0.3448 elbo -0.4682 sf 0.9461 selected 0.4800 kl 0.0624 ll -1.4104\n",
      "\n",
      "# epoch 18 iter 6479: dev loss 0.3814 elbo -0.3814 sf 0.4318 selected 0.2160 kl 0.0583 ll -0.7548 acc 0.3960\n",
      " dev0 [gold=3,pred=1]: it **'s** a **lovely** **film** with **lovely** **performances** by buy and accorsi .\n",
      " dev1 [gold=2,pred=1]: **no** one goes **unindicted** here , which is probably for the best .\n",
      " dev2 [gold=3,pred=1]: and if you **'re** not nearly moved to **tears** by a couple of **scenes** , you **'ve** got ice water in your **veins** .\n",
      "\n",
      "Shuffling training data\n",
      "Epoch 19 Iter 6500 loss=0.3419 elbo -0.4340 sf 0.9397 selected 0.5200 kl 0.0594 ll -1.3698\n",
      "Epoch 19 Iter 6600 loss=0.3300 elbo -0.4814 sf 1.0387 selected 0.4800 kl 0.0561 ll -1.5164\n",
      "Epoch 19 Iter 6700 loss=0.3259 elbo -0.3246 sf 0.7156 selected 0.5220 kl 0.0514 ll -1.0368\n",
      "Epoch 19 Iter 6800 loss=0.3423 elbo -0.3323 sf 0.6999 selected 0.4995 kl 0.0570 ll -1.0284\n",
      "\n",
      "# epoch 19 iter 6820: dev loss 0.3807 elbo -0.3807 sf 0.4286 selected 0.2050 kl 0.0546 ll -0.7547 acc 0.3906\n",
      " dev0 [gold=3,pred=1]: it **'s** a **lovely** **film** with **lovely** **performances** by buy and accorsi .\n",
      " dev1 [gold=2,pred=1]: **no** one goes **unindicted** here , which is probably for the best .\n",
      " dev2 [gold=3,pred=1]: and if you **'re** not nearly moved to **tears** by a couple of **scenes** , you **'ve** got ice water in your **veins** .\n",
      "\n",
      "Shuffling training data\n",
      "Epoch 20 Iter 6900 loss=0.3501 elbo -0.3356 sf 0.6849 selected 0.4717 kl 0.0680 ll -1.0159\n",
      "Epoch 20 Iter 7000 loss=0.3283 elbo -0.3262 sf 0.6778 selected 0.4971 kl 0.0446 ll -1.0008\n",
      "Epoch 20 Iter 7100 loss=0.3371 elbo -0.4560 sf 1.0009 selected 0.5087 kl 0.0552 ll -1.4529\n",
      "\n",
      "# epoch 20 iter 7161: dev loss 0.3835 elbo -0.3835 sf 0.4276 selected 0.1889 kl 0.0533 ll -0.7578 acc 0.3906\n",
      " dev0 [gold=3,pred=1]: it **'s** a **lovely** **film** with **lovely** **performances** by buy and accorsi .\n",
      " dev1 [gold=2,pred=1]: **no** one goes **unindicted** here , which is probably for the best .\n",
      " dev2 [gold=3,pred=1]: and if you **'re** not nearly moved to **tears** by a couple of **scenes** , you **'ve** got ice water in your **veins** .\n",
      "\n",
      "Epoch    20: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Shuffling training data\n",
      "Epoch 21 Iter 7200 loss=0.3353 elbo -0.3235 sf 0.6696 selected 0.5021 kl 0.0545 ll -0.9891\n",
      "Epoch 21 Iter 7300 loss=0.3415 elbo -0.3435 sf 0.6993 selected 0.4850 kl 0.0453 ll -1.0395\n",
      "Epoch 21 Iter 7400 loss=0.3391 elbo -0.3388 sf 0.7182 selected 0.4766 kl 0.0531 ll -1.0531\n",
      "Epoch 21 Iter 7500 loss=0.3365 elbo -0.1894 sf 0.4114 selected 0.5043 kl 0.0392 ll -0.5978\n",
      "\n",
      "# epoch 21 iter 7502: dev loss 0.3855 elbo -0.3855 sf 0.4258 selected 0.1782 kl 0.0526 ll -0.7586 acc 0.3969\n",
      " dev0 [gold=3,pred=1]: it 's a **lovely** **film** with **lovely** **performances** by buy and accorsi .\n",
      " dev1 [gold=2,pred=1]: **no** one goes **unindicted** here , which is probably for the best .\n",
      " dev2 [gold=3,pred=1]: and if you **'re** not nearly moved to **tears** by a couple of **scenes** , you **'ve** got ice water in your **veins** .\n",
      "\n",
      "Shuffling training data\n",
      "Epoch 22 Iter 7600 loss=0.3376 elbo -0.4025 sf 0.9509 selected 0.4880 kl 0.0541 ll -1.3493\n",
      "Epoch 22 Iter 7700 loss=0.3286 elbo -0.3777 sf 0.7905 selected 0.4846 kl 0.0584 ll -1.1638\n",
      "Epoch 22 Iter 7800 loss=0.3430 elbo -0.2540 sf 0.5172 selected 0.4939 kl 0.0362 ll -0.7684\n",
      "\n",
      "# epoch 22 iter 7843: dev loss 0.3877 elbo -0.3877 sf 0.4228 selected 0.1738 kl 0.0515 ll -0.7589 acc 0.3987\n",
      " dev0 [gold=3,pred=1]: it 's a **lovely** **film** with **lovely** **performances** by buy and accorsi .\n",
      " dev1 [gold=2,pred=3]: **no** one goes **unindicted** here , which is probably for the best .\n",
      " dev2 [gold=3,pred=1]: and if you **'re** not nearly moved to **tears** by a couple of **scenes** , you **'ve** got ice water in your **veins** .\n",
      "\n",
      "Shuffling training data\n",
      "Epoch 23 Iter 7900 loss=0.3353 elbo -0.3475 sf 0.7261 selected 0.4920 kl 0.0564 ll -1.0692\n",
      "Epoch 23 Iter 8000 loss=0.3436 elbo -0.2091 sf 0.4743 selected 0.4898 kl 0.0339 ll -0.6807\n",
      "Epoch 23 Iter 8100 loss=0.3289 elbo -0.5505 sf 1.1462 selected 0.5500 kl 0.0570 ll -1.6921\n",
      "\n",
      "# epoch 23 iter 8184: dev loss 0.3904 elbo -0.3904 sf 0.4200 selected 0.1651 kl 0.0495 ll -0.7609 acc 0.3960\n",
      " dev0 [gold=3,pred=1]: it 's a **lovely** **film** with **lovely** **performances** by buy and accorsi .\n",
      " dev1 [gold=2,pred=3]: **no** one goes **unindicted** here , which is probably for the best .\n",
      " dev2 [gold=3,pred=1]: and if you **'re** not nearly moved to **tears** by a couple of **scenes** , you 've got ice water in your **veins** .\n",
      "\n",
      "Epoch 24 Iter 8200 loss=0.3468 elbo -0.3425 sf 0.6893 selected 0.4833 kl 0.0511 ll -1.0276\n",
      "Shuffling training data\n",
      "Epoch 24 Iter 8300 loss=0.3375 elbo -0.2977 sf 0.6310 selected 0.5021 kl 0.0505 ll -0.9245\n",
      "Epoch 24 Iter 8400 loss=0.3348 elbo -0.3919 sf 0.7704 selected 0.5014 kl 0.0360 ll -1.1593\n",
      "Epoch 24 Iter 8500 loss=0.3296 elbo -0.3362 sf 0.6564 selected 0.4789 kl 0.0427 ll -0.9890\n",
      "\n",
      "# epoch 24 iter 8525: dev loss 0.3925 elbo -0.3925 sf 0.4168 selected 0.1586 kl 0.0478 ll -0.7615 acc 0.3942\n",
      " dev0 [gold=3,pred=1]: it 's a **lovely** **film** with **lovely** **performances** by buy and accorsi .\n",
      " dev1 [gold=2,pred=1]: **no** one goes **unindicted** here , which is probably for the best .\n",
      " dev2 [gold=3,pred=1]: and if you 're not nearly moved to **tears** by a couple of **scenes** , you 've got ice water in your **veins** .\n",
      "\n",
      "Shuffling training data\n",
      "Epoch 25 Iter 8600 loss=0.3386 elbo -0.3168 sf 0.6222 selected 0.4927 kl 0.0435 ll -0.9352\n",
      "Epoch 25 Iter 8700 loss=0.3344 elbo -0.3506 sf 0.7196 selected 0.4833 kl 0.0423 ll -1.0665\n",
      "Epoch 25 Iter 8800 loss=0.3351 elbo -0.3674 sf 0.7884 selected 0.4787 kl 0.0537 ll -1.1510\n",
      "\n",
      "# epoch 25 iter 8866: dev loss 0.3943 elbo -0.3943 sf 0.4139 selected 0.1477 kl 0.0460 ll -0.7622 acc 0.3887\n",
      " dev0 [gold=3,pred=1]: it 's a **lovely** film with **lovely** **performances** by buy and accorsi .\n",
      " dev1 [gold=2,pred=1]: **no** one goes **unindicted** here , which is probably for the best .\n",
      " dev2 [gold=3,pred=1]: and if you 're not nearly moved to **tears** by a couple of **scenes** , you 've got ice water in your **veins** .\n",
      "\n",
      "Shuffling training data\n",
      "Epoch 26 Iter 8900 loss=0.3403 elbo -0.3952 sf 0.8101 selected 0.5053 kl 0.0493 ll -1.2010\n",
      "Epoch 26 Iter 9000 loss=0.3440 elbo -0.3121 sf 0.6642 selected 0.4878 kl 0.0403 ll -0.9726\n",
      "Epoch 26 Iter 9100 loss=0.3247 elbo -0.3371 sf 0.7116 selected 0.5167 kl 0.0470 ll -1.0444\n",
      "Epoch 26 Iter 9200 loss=0.3577 elbo -0.4376 sf 0.8507 selected 0.4944 kl 0.0469 ll -1.2839\n",
      "\n",
      "# epoch 26 iter 9207: dev loss 0.3982 elbo -0.3982 sf 0.4088 selected 0.1440 kl 0.0449 ll -0.7622 acc 0.3915\n",
      " dev0 [gold=3,pred=1]: it 's a **lovely** **film** with **lovely** **performances** by buy and accorsi .\n",
      " dev1 [gold=2,pred=1]: no one goes **unindicted** here , which is probably for the best .\n",
      " dev2 [gold=3,pred=1]: and if you 're not nearly moved to **tears** by a couple of **scenes** , you 've got ice water in your **veins** .\n",
      "\n",
      "Shuffling training data\n",
      "Epoch 27 Iter 9300 loss=0.3529 elbo -0.4070 sf 0.7697 selected 0.4917 kl 0.0422 ll -1.1728\n",
      "Epoch 27 Iter 9400 loss=0.3461 elbo -0.4056 sf 0.8420 selected 0.4943 kl 0.0434 ll -1.2435\n",
      "Epoch 27 Iter 9500 loss=0.3456 elbo -0.3781 sf 0.7716 selected 0.4604 kl 0.0359 ll -1.1462\n",
      "\n",
      "# epoch 27 iter 9548: dev loss 0.4019 elbo -0.4019 sf 0.4039 selected 0.1417 kl 0.0435 ll -0.7624 acc 0.3869\n",
      " dev0 [gold=3,pred=1]: it 's a **lovely** **film** with **lovely** **performances** by buy and accorsi .\n",
      " dev1 [gold=2,pred=1]: no one goes **unindicted** here , which is probably for the best .\n",
      " dev2 [gold=3,pred=1]: and if you 're not nearly moved to **tears** by a couple of **scenes** , you 've got ice water in your **veins** .\n",
      "\n",
      "Shuffling training data\n",
      "Epoch 28 Iter 9600 loss=0.3392 elbo -0.4325 sf 0.8893 selected 0.4810 kl 0.0350 ll -1.3185\n",
      "Epoch 28 Iter 9700 loss=0.3427 elbo -0.2684 sf 0.5649 selected 0.4933 kl 0.0347 ll -0.8299\n",
      "Epoch 28 Iter 9800 loss=0.3324 elbo -0.4155 sf 0.8849 selected 0.4840 kl 0.0433 ll -1.2962\n",
      "\n",
      "# epoch 28 iter 9889: dev loss 0.4041 elbo -0.4041 sf 0.4012 selected 0.1344 kl 0.0425 ll -0.7628 acc 0.3860\n",
      " dev0 [gold=3,pred=1]: it 's a **lovely** film with **lovely** **performances** by buy and accorsi .\n",
      " dev1 [gold=2,pred=1]: no one goes **unindicted** here , which is probably for the best .\n",
      " dev2 [gold=3,pred=1]: and if you 're not nearly moved to **tears** by a couple of **scenes** , you 've got ice water in your **veins** .\n",
      "\n",
      "Epoch 29 Iter 9900 loss=0.3421 elbo -0.3282 sf 0.6769 selected 0.4768 kl 0.0435 ll -1.0009\n",
      "Shuffling training data\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29 Iter 10000 loss=0.3337 elbo -0.3733 sf 0.7877 selected 0.4584 kl 0.0416 ll -1.1568\n",
      "Epoch 29 Iter 10100 loss=0.3314 elbo -0.2817 sf 0.5800 selected 0.4868 kl 0.0323 ll -0.8584\n",
      "Epoch 29 Iter 10200 loss=0.3400 elbo -0.3457 sf 0.6788 selected 0.4849 kl 0.0417 ll -1.0202\n",
      "\n",
      "# epoch 29 iter 10230: dev loss 0.4081 elbo -0.4081 sf 0.3958 selected 0.1287 kl 0.0408 ll -0.7631 acc 0.3833\n",
      " dev0 [gold=3,pred=1]: it 's a **lovely** film with **lovely** **performances** by buy and accorsi .\n",
      " dev1 [gold=2,pred=1]: no one goes **unindicted** here , which is probably for the best .\n",
      " dev2 [gold=3,pred=1]: and if you 're not nearly moved to **tears** by a couple of **scenes** , you 've got ice water in your **veins** .\n",
      "\n",
      "Shuffling training data\n",
      "Epoch 30 Iter 10300 loss=0.3483 elbo -0.4952 sf 0.9765 selected 0.4457 kl 0.0394 ll -1.4677\n",
      "Epoch 30 Iter 10400 loss=0.3470 elbo -0.2416 sf 0.4686 selected 0.4722 kl 0.0422 ll -0.7058\n",
      "Epoch 30 Iter 10500 loss=0.3580 elbo -0.3463 sf 0.7272 selected 0.5148 kl 0.0404 ll -1.0693\n",
      "\n",
      "# epoch 30 iter 10571: dev loss 0.4073 elbo -0.4073 sf 0.3962 selected 0.1289 kl 0.0412 ll -0.7622 acc 0.3869\n",
      " dev0 [gold=3,pred=1]: it 's a **lovely** film with **lovely** **performances** by buy and accorsi .\n",
      " dev1 [gold=2,pred=1]: no one goes **unindicted** here , which is probably for the best .\n",
      " dev2 [gold=3,pred=1]: and if you 're not nearly moved to **tears** by a couple of **scenes** , you 've got ice water in your **veins** .\n",
      "\n",
      "Epoch 31 Iter 10600 loss=0.3494 elbo -0.4247 sf 0.8360 selected 0.4550 kl 0.0396 ll -1.2565\n",
      "Shuffling training data\n",
      "Epoch 31 Iter 10700 loss=0.3457 elbo -0.3403 sf 0.6620 selected 0.5012 kl 0.0466 ll -0.9973\n",
      "Epoch 31 Iter 10800 loss=0.3433 elbo -0.3118 sf 0.5983 selected 0.4424 kl 0.0441 ll -0.9053\n",
      "Epoch 31 Iter 10900 loss=0.3480 elbo -0.3621 sf 0.6996 selected 0.4724 kl 0.0361 ll -1.0577\n",
      "\n",
      "# epoch 31 iter 10912: dev loss 0.4079 elbo -0.4079 sf 0.3942 selected 0.1278 kl 0.0402 ll -0.7619 acc 0.3860\n",
      " dev0 [gold=3,pred=1]: it 's a **lovely** film with **lovely** **performances** by buy and accorsi .\n",
      " dev1 [gold=2,pred=1]: no one goes **unindicted** here , which is probably for the best .\n",
      " dev2 [gold=3,pred=1]: and if you 're not nearly moved to **tears** by a couple of **scenes** , you 've got ice water in your **veins** .\n",
      "\n",
      "Epoch    31: reducing learning rate of group 0 to 2.5000e-05.\n",
      "Shuffling training data\n",
      "Epoch 32 Iter 11000 loss=0.3499 elbo -0.2956 sf 0.5777 selected 0.4526 kl 0.0348 ll -0.8695\n",
      "Epoch 32 Iter 11100 loss=0.3427 elbo -0.3139 sf 0.5999 selected 0.4530 kl 0.0399 ll -0.9094\n",
      "Epoch 32 Iter 11200 loss=0.3302 elbo -0.3090 sf 0.6454 selected 0.4702 kl 0.0359 ll -0.9504\n",
      "\n",
      "# epoch 32 iter 11253: dev loss 0.4088 elbo -0.4088 sf 0.3926 selected 0.1267 kl 0.0396 ll -0.7618 acc 0.3842\n",
      " dev0 [gold=3,pred=1]: it 's a **lovely** film with **lovely** **performances** by buy and accorsi .\n",
      " dev1 [gold=2,pred=1]: no one goes **unindicted** here , which is probably for the best .\n",
      " dev2 [gold=3,pred=1]: and if you 're not nearly moved to **tears** by a couple of **scenes** , you 've got ice water in your **veins** .\n",
      "\n",
      "Shuffling training data\n",
      "Epoch 33 Iter 11300 loss=0.3474 elbo -0.3763 sf 0.7464 selected 0.4812 kl 0.0460 ll -1.1175\n",
      "Epoch 33 Iter 11400 loss=0.3344 elbo -0.2884 sf 0.6076 selected 0.4914 kl 0.0378 ll -0.8917\n",
      "Epoch 33 Iter 11500 loss=0.3472 elbo -0.3222 sf 0.6640 selected 0.4811 kl 0.0411 ll -0.9814\n",
      "\n",
      "# epoch 33 iter 11594: dev loss 0.4108 elbo -0.4108 sf 0.3900 selected 0.1255 kl 0.0389 ll -0.7620 acc 0.3833\n",
      " dev0 [gold=3,pred=1]: it 's a **lovely** film with **lovely** **performances** by buy and accorsi .\n",
      " dev1 [gold=2,pred=1]: no one goes **unindicted** here , which is probably for the best .\n",
      " dev2 [gold=3,pred=1]: and if you 're not nearly moved to **tears** by a couple of **scenes** , you 've got ice water in your **veins** .\n",
      "\n",
      "Epoch 34 Iter 11600 loss=0.3556 elbo -0.3778 sf 0.7108 selected 0.4958 kl 0.0431 ll -1.0837\n",
      "Shuffling training data\n",
      "Epoch 34 Iter 11700 loss=0.3700 elbo -0.2980 sf 0.5820 selected 0.4764 kl 0.0395 ll -0.8754\n",
      "Epoch 34 Iter 11800 loss=0.3299 elbo -0.4234 sf 0.9340 selected 0.4983 kl 0.0374 ll -1.3529\n",
      "Epoch 34 Iter 11900 loss=0.3503 elbo -0.3618 sf 0.6819 selected 0.4843 kl 0.0362 ll -1.0394\n",
      "\n",
      "# epoch 34 iter 11935: dev loss 0.4127 elbo -0.4127 sf 0.3871 selected 0.1240 kl 0.0381 ll -0.7618 acc 0.3842\n",
      " dev0 [gold=3,pred=1]: it 's a **lovely** film with **lovely** **performances** by buy and accorsi .\n",
      " dev1 [gold=2,pred=1]: no one goes **unindicted** here , which is probably for the best .\n",
      " dev2 [gold=3,pred=1]: and if you 're not nearly moved to **tears** by a couple of **scenes** , you 've got ice water in your **veins** .\n",
      "\n",
      "Shuffling training data\n",
      "Epoch 35 Iter 12000 loss=0.3593 elbo -0.4635 sf 0.8665 selected 0.4354 kl 0.0359 ll -1.3257\n",
      "Epoch 35 Iter 12100 loss=0.3415 elbo -0.3798 sf 0.7136 selected 0.4693 kl 0.0328 ll -1.0894\n",
      "Epoch 35 Iter 12200 loss=0.3440 elbo -0.3564 sf 0.7342 selected 0.4829 kl 0.0329 ll -1.0866\n",
      "\n",
      "# epoch 35 iter 12276: dev loss 0.4137 elbo -0.4137 sf 0.3858 selected 0.1235 kl 0.0378 ll -0.7616 acc 0.3842\n",
      " dev0 [gold=3,pred=1]: it 's a **lovely** film with **lovely** **performances** by buy and accorsi .\n",
      " dev1 [gold=2,pred=1]: no one goes **unindicted** here , which is probably for the best .\n",
      " dev2 [gold=3,pred=1]: and if you 're not nearly moved to **tears** by a couple of **scenes** , you 've got ice water in your **veins** .\n",
      "\n",
      "Epoch 36 Iter 12300 loss=0.3539 elbo -0.3850 sf 0.6627 selected 0.4447 kl 0.0349 ll -1.0434\n",
      "Shuffling training data\n",
      "Epoch 36 Iter 12400 loss=0.3414 elbo -0.3965 sf 0.8216 selected 0.4618 kl 0.0396 ll -1.2132\n",
      "Epoch 36 Iter 12500 loss=0.3423 elbo -0.3051 sf 0.7152 selected 0.4922 kl 0.0400 ll -1.0152\n",
      "Epoch 36 Iter 12600 loss=0.3585 elbo -0.2335 sf 0.4326 selected 0.4564 kl 0.0349 ll -0.6617\n",
      "\n",
      "# epoch 36 iter 12617: dev loss 0.4141 elbo -0.4141 sf 0.3848 selected 0.1230 kl 0.0374 ll -0.7615 acc 0.3842\n",
      " dev0 [gold=3,pred=1]: it 's a **lovely** film with **lovely** **performances** by buy and accorsi .\n",
      " dev1 [gold=2,pred=1]: no one goes **unindicted** here , which is probably for the best .\n",
      " dev2 [gold=3,pred=1]: and if you 're not nearly moved to **tears** by a couple of **scenes** , you 've got ice water in your **veins** .\n",
      "\n",
      "Shuffling training data\n",
      "Epoch 37 Iter 12700 loss=0.3645 elbo -0.3590 sf 0.7488 selected 0.4686 kl 0.0269 ll -1.1044\n",
      "Epoch 37 Iter 12800 loss=0.3499 elbo -0.3776 sf 0.7567 selected 0.4508 kl 0.0273 ll -1.1308\n",
      "Epoch 37 Iter 12900 loss=0.3425 elbo -0.3869 sf 0.7371 selected 0.4377 kl 0.0364 ll -1.1193\n",
      "\n",
      "# epoch 37 iter 12958: dev loss 0.4148 elbo -0.4148 sf 0.3837 selected 0.1217 kl 0.0366 ll -0.7618 acc 0.3824\n",
      " dev0 [gold=3,pred=1]: it 's a **lovely** film with **lovely** **performances** by buy and accorsi .\n",
      " dev1 [gold=2,pred=1]: no one goes **unindicted** here , which is probably for the best .\n",
      " dev2 [gold=3,pred=1]: and if you 're not nearly moved to **tears** by a couple of **scenes** , you 've got ice water in your **veins** .\n",
      "\n",
      "Shuffling training data\n",
      "Epoch 38 Iter 13000 loss=0.3655 elbo -0.2826 sf 0.5657 selected 0.4759 kl 0.0275 ll -0.8448\n",
      "Epoch 38 Iter 13100 loss=0.3634 elbo -0.3391 sf 0.6273 selected 0.4958 kl 0.0349 ll -0.9617\n",
      "Epoch 38 Iter 13200 loss=0.3527 elbo -0.3224 sf 0.5927 selected 0.4409 kl 0.0314 ll -0.9109\n",
      "\n",
      "# epoch 38 iter 13299: dev loss 0.4177 elbo -0.4177 sf 0.3807 selected 0.1174 kl 0.0352 ll -0.7631 acc 0.3778\n",
      " dev0 [gold=3,pred=1]: it 's a **lovely** film with **lovely** **performances** by buy and accorsi .\n",
      " dev1 [gold=2,pred=1]: no one goes **unindicted** here , which is probably for the best .\n",
      " dev2 [gold=3,pred=1]: and if you 're not nearly moved to **tears** by a couple of **scenes** , you 've got ice water in your **veins** .\n",
      "\n",
      "Epoch 39 Iter 13300 loss=0.3426 elbo -0.3989 sf 0.8037 selected 0.4595 kl 0.0295 ll -1.1987\n",
      "Shuffling training data\n",
      "Epoch 39 Iter 13400 loss=0.3627 elbo -0.2490 sf 0.5954 selected 0.4991 kl 0.0277 ll -0.8407\n",
      "Epoch 39 Iter 13500 loss=0.3369 elbo -0.2642 sf 0.5273 selected 0.4876 kl 0.0291 ll -0.7875\n",
      "Epoch 39 Iter 13600 loss=0.3534 elbo -0.3645 sf 0.6439 selected 0.4659 kl 0.0370 ll -1.0034\n",
      "\n",
      "# epoch 39 iter 13640: dev loss 0.4187 elbo -0.4187 sf 0.3794 selected 0.1157 kl 0.0347 ll -0.7634 acc 0.3751\n",
      " dev0 [gold=3,pred=1]: it 's a **lovely** film with **lovely** **performances** by buy and accorsi .\n",
      " dev1 [gold=2,pred=1]: no one goes **unindicted** here , which is probably for the best .\n",
      " dev2 [gold=3,pred=1]: and if you 're not nearly moved to **tears** by a couple of **scenes** , you 've got ice water in your **veins** .\n",
      "\n",
      "Shuffling training data\n",
      "Epoch 40 Iter 13700 loss=0.3430 elbo -0.3113 sf 0.5971 selected 0.4400 kl 0.0322 ll -0.9040\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 40 Iter 13800 loss=0.3455 elbo -0.4332 sf 0.7593 selected 0.4206 kl 0.0359 ll -1.1875\n",
      "Epoch 40 Iter 13900 loss=0.3536 elbo -0.3994 sf 0.7491 selected 0.4590 kl 0.0335 ll -1.1439\n",
      "\n",
      "# epoch 40 iter 13981: dev loss 0.4203 elbo -0.4203 sf 0.3772 selected 0.1113 kl 0.0339 ll -0.7636 acc 0.3778\n",
      " dev0 [gold=3,pred=1]: it 's a **lovely** film with **lovely** **performances** by buy and accorsi .\n",
      " dev1 [gold=2,pred=1]: no one goes **unindicted** here , which is probably for the best .\n",
      " dev2 [gold=3,pred=1]: and if you 're not nearly moved to **tears** by a couple of **scenes** , you 've got ice water in your **veins** .\n",
      "\n",
      "Epoch 41 Iter 14000 loss=0.3581 elbo -0.4278 sf 0.8502 selected 0.4338 kl 0.0321 ll -1.2735\n",
      "Shuffling training data\n",
      "Epoch 41 Iter 14100 loss=0.3573 elbo -0.2683 sf 0.5254 selected 0.4924 kl 0.0306 ll -0.7894\n",
      "Epoch 41 Iter 14200 loss=0.3707 elbo -0.3743 sf 0.6708 selected 0.4638 kl 0.0324 ll -1.0405\n",
      "Epoch 41 Iter 14300 loss=0.3558 elbo -0.3594 sf 0.6615 selected 0.4525 kl 0.0366 ll -1.0157\n",
      "\n",
      "# epoch 41 iter 14322: dev loss 0.4212 elbo -0.4212 sf 0.3760 selected 0.1093 kl 0.0333 ll -0.7639 acc 0.3778\n",
      " dev0 [gold=3,pred=1]: it 's a **lovely** film with **lovely** **performances** by buy and accorsi .\n",
      " dev1 [gold=2,pred=1]: no one goes **unindicted** here , which is probably for the best .\n",
      " dev2 [gold=3,pred=1]: and if you 're not nearly moved to tears by a couple of **scenes** , you 've got ice water in your **veins** .\n",
      "\n",
      "Shuffling training data\n",
      "Epoch 42 Iter 14400 loss=0.3559 elbo -0.2851 sf 0.5604 selected 0.4578 kl 0.0331 ll -0.8408\n",
      "Epoch 42 Iter 14500 loss=0.3659 elbo -0.5008 sf 0.9134 selected 0.4477 kl 0.0413 ll -1.4082\n",
      "Epoch 42 Iter 14600 loss=0.3659 elbo -0.3555 sf 0.6221 selected 0.4707 kl 0.0327 ll -0.9729\n",
      "\n",
      "# epoch 42 iter 14663: dev loss 0.4227 elbo -0.4227 sf 0.3736 selected 0.1081 kl 0.0326 ll -0.7637 acc 0.3797\n",
      " dev0 [gold=3,pred=3]: it 's a **lovely** film with **lovely** **performances** by buy and accorsi .\n",
      " dev1 [gold=2,pred=1]: no one goes **unindicted** here , which is probably for the best .\n",
      " dev2 [gold=3,pred=1]: and if you 're not nearly moved to tears by a couple of **scenes** , you 've got ice water in your **veins** .\n",
      "\n",
      "Epoch    42: reducing learning rate of group 0 to 1.2500e-05.\n",
      "Epoch 43 Iter 14700 loss=0.3543 elbo -0.2850 sf 0.6183 selected 0.4728 kl 0.0287 ll -0.8990\n",
      "Shuffling training data\n",
      "Epoch 43 Iter 14800 loss=0.3519 elbo -0.3927 sf 0.7160 selected 0.4335 kl 0.0264 ll -1.1048\n",
      "Epoch 43 Iter 14900 loss=0.3535 elbo -0.2455 sf 0.5361 selected 0.4589 kl 0.0270 ll -0.7775\n",
      "Epoch 43 Iter 15000 loss=0.3611 elbo -0.4331 sf 0.7562 selected 0.4600 kl 0.0398 ll -1.1834\n",
      "\n",
      "# epoch 43 iter 15004: dev loss 0.4230 elbo -0.4230 sf 0.3729 selected 0.1075 kl 0.0323 ll -0.7637 acc 0.3787\n",
      " dev0 [gold=3,pred=3]: it 's a **lovely** film with **lovely** **performances** by buy and accorsi .\n",
      " dev1 [gold=2,pred=1]: no one goes **unindicted** here , which is probably for the best .\n",
      " dev2 [gold=3,pred=1]: and if you 're not nearly moved to tears by a couple of **scenes** , you 've got ice water in your **veins** .\n",
      "\n",
      "Shuffling training data\n",
      "Epoch 44 Iter 15100 loss=0.3483 elbo -0.2391 sf 0.4388 selected 0.4356 kl 0.0290 ll -0.6734\n",
      "Epoch 44 Iter 15200 loss=0.3596 elbo -0.3987 sf 0.6605 selected 0.4400 kl 0.0308 ll -1.0545\n",
      "Epoch 44 Iter 15300 loss=0.3671 elbo -0.5129 sf 0.9381 selected 0.4509 kl 0.0353 ll -1.4456\n",
      "\n",
      "# epoch 44 iter 15345: dev loss 0.4234 elbo -0.4234 sf 0.3727 selected 0.1069 kl 0.0319 ll -0.7642 acc 0.3760\n",
      " dev0 [gold=3,pred=3]: it 's a **lovely** film with **lovely** **performances** by buy and accorsi .\n",
      " dev1 [gold=2,pred=1]: no one goes **unindicted** here , which is probably for the best .\n",
      " dev2 [gold=3,pred=1]: and if you 're not nearly moved to tears by a couple of **scenes** , you 've got ice water in your veins .\n",
      "\n",
      "Shuffling training data\n",
      "Epoch 45 Iter 15400 loss=0.3542 elbo -0.3431 sf 0.6026 selected 0.4724 kl 0.0304 ll -0.9411\n",
      "Epoch 45 Iter 15500 loss=0.3536 elbo -0.2703 sf 0.5061 selected 0.4558 kl 0.0214 ll -0.7731\n",
      "Epoch 45 Iter 15600 loss=0.3532 elbo -0.2745 sf 0.5557 selected 0.4620 kl 0.0280 ll -0.8258\n",
      "\n",
      "# epoch 45 iter 15686: dev loss 0.4234 elbo -0.4234 sf 0.3721 selected 0.1064 kl 0.0317 ll -0.7639 acc 0.3769\n",
      " dev0 [gold=3,pred=3]: it 's a **lovely** film with **lovely** **performances** by buy and accorsi .\n",
      " dev1 [gold=2,pred=1]: no one goes **unindicted** here , which is probably for the best .\n",
      " dev2 [gold=3,pred=1]: and if you 're not nearly moved to tears by a couple of **scenes** , you 've got ice water in your veins .\n",
      "\n",
      "Epoch 46 Iter 15700 loss=0.3623 elbo -0.2784 sf 0.5202 selected 0.5038 kl 0.0307 ll -0.7938\n",
      "Shuffling training data\n",
      "Epoch 46 Iter 15800 loss=0.3598 elbo -0.3563 sf 0.7395 selected 0.4737 kl 0.0313 ll -1.0908\n",
      "Epoch 46 Iter 15900 loss=0.3465 elbo -0.3917 sf 0.7156 selected 0.4544 kl 0.0309 ll -1.1023\n",
      "Epoch 46 Iter 16000 loss=0.3669 elbo -0.2830 sf 0.5408 selected 0.4635 kl 0.0297 ll -0.8191\n",
      "\n",
      "# epoch 46 iter 16027: dev loss 0.4238 elbo -0.4238 sf 0.3715 selected 0.1058 kl 0.0313 ll -0.7641 acc 0.3778\n",
      " dev0 [gold=3,pred=3]: it 's a **lovely** film with **lovely** **performances** by buy and accorsi .\n",
      " dev1 [gold=2,pred=1]: no one goes **unindicted** here , which is probably for the best .\n",
      " dev2 [gold=3,pred=1]: and if you 're not nearly moved to tears by a couple of **scenes** , you 've got ice water in your veins .\n",
      "\n",
      "Shuffling training data\n",
      "Epoch 47 Iter 16100 loss=0.3608 elbo -0.3900 sf 0.7205 selected 0.4553 kl 0.0338 ll -1.1051\n",
      "Epoch 47 Iter 16200 loss=0.3484 elbo -0.4373 sf 0.8900 selected 0.4581 kl 0.0348 ll -1.3217\n",
      "Epoch 47 Iter 16300 loss=0.3525 elbo -0.3221 sf 0.6248 selected 0.4710 kl 0.0271 ll -0.9425\n",
      "\n",
      "# epoch 47 iter 16368: dev loss 0.4243 elbo -0.4243 sf 0.3707 selected 0.1052 kl 0.0309 ll -0.7641 acc 0.3751\n",
      " dev0 [gold=3,pred=3]: it 's a **lovely** film with **lovely** **performances** by buy and accorsi .\n",
      " dev1 [gold=2,pred=1]: no one goes **unindicted** here , which is probably for the best .\n",
      " dev2 [gold=3,pred=1]: and if you 're not nearly moved to tears by a couple of **scenes** , you 've got ice water in your veins .\n",
      "\n",
      "Epoch 48 Iter 16400 loss=0.3603 elbo -0.2967 sf 0.5700 selected 0.4341 kl 0.0327 ll -0.8613\n",
      "Shuffling training data\n",
      "Epoch 48 Iter 16500 loss=0.3668 elbo -0.3372 sf 0.7042 selected 0.4364 kl 0.0334 ll -1.0359\n",
      "Epoch 48 Iter 16600 loss=0.3514 elbo -0.4176 sf 0.7666 selected 0.4470 kl 0.0240 ll -1.1802\n",
      "Epoch 48 Iter 16700 loss=0.3604 elbo -0.2519 sf 0.4357 selected 0.4733 kl 0.0292 ll -0.6827\n",
      "\n",
      "# epoch 48 iter 16709: dev loss 0.4242 elbo -0.4242 sf 0.3706 selected 0.1051 kl 0.0309 ll -0.7640 acc 0.3760\n",
      " dev0 [gold=3,pred=3]: it 's a **lovely** film with **lovely** **performances** by buy and accorsi .\n",
      " dev1 [gold=2,pred=1]: no one goes **unindicted** here , which is probably for the best .\n",
      " dev2 [gold=3,pred=1]: and if you 're not nearly moved to tears by a couple of **scenes** , you 've got ice water in your veins .\n",
      "\n",
      "Shuffling training data\n",
      "Epoch 49 Iter 16800 loss=0.3700 elbo -0.3987 sf 0.7814 selected 0.4800 kl 0.0232 ll -1.1762\n",
      "Epoch 49 Iter 16900 loss=0.3499 elbo -0.3526 sf 0.6590 selected 0.4558 kl 0.0306 ll -1.0064\n",
      "Epoch 49 Iter 17000 loss=0.3387 elbo -0.3388 sf 0.5938 selected 0.4578 kl 0.0309 ll -0.9273\n",
      "\n",
      "# epoch 49 iter 17050: dev loss 0.4253 elbo -0.4253 sf 0.3696 selected 0.1044 kl 0.0306 ll -0.7643 acc 0.3742\n",
      " dev0 [gold=3,pred=3]: it 's a **lovely** film with **lovely** **performances** by buy and accorsi .\n",
      " dev1 [gold=2,pred=1]: no one goes **unindicted** here , which is probably for the best .\n",
      " dev2 [gold=3,pred=1]: and if you 're not nearly moved to tears by a couple of **scenes** , you 've got ice water in your veins .\n",
      "\n",
      "Epoch 50 Iter 17100 loss=0.3637 elbo -0.3579 sf 0.7151 selected 0.4346 kl 0.0334 ll -1.0673\n",
      "Shuffling training data\n",
      "Epoch 50 Iter 17200 loss=0.3588 elbo -0.3890 sf 0.6436 selected 0.4222 kl 0.0280 ll -1.0278\n",
      "Epoch 50 Iter 17300 loss=0.3431 elbo -0.3248 sf 0.5941 selected 0.4638 kl 0.0297 ll -0.9137\n",
      "\n",
      "# epoch 50 iter 17391: dev loss 0.4269 elbo -0.4269 sf 0.3691 selected 0.1009 kl 0.0301 ll -0.7659 acc 0.3778\n",
      " dev0 [gold=3,pred=3]: it 's a **lovely** film with **lovely** **performances** by buy and accorsi .\n",
      " dev1 [gold=2,pred=1]: no one goes **unindicted** here , which is probably for the best .\n",
      " dev2 [gold=3,pred=1]: and if you 're not nearly moved to tears by a couple of **scenes** , you 've got ice water in your veins .\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7w_Ko657vRGo"
   },
   "source": [
    "# Variance reduction\n",
    "\n",
    "**This is an extra**\n",
    "\n",
    "We can use a *control variate* to reduce the variance of our gradient estimates.\n",
    "\n",
    "Let's recap the idea in general terms. We are looking to solve some expectation\n",
    "\\begin{align}\n",
    "\\mu_f = \\mathbb E[f(Z)]\n",
    "\\end{align}\n",
    "but unfortunatelly, realising the full sum (or integral for continuous variables) is intractable. Thus we employ MC estimation\n",
    "\\begin{align}\n",
    "\\hat \\mu_f &\\overset{\\text{MC}}{\\approx} \\frac{1}{S} \\sum_{s=1}^S f(z_s) & \\text{where }z_s \\sim Q(z|x)\n",
    "\\end{align}\n",
    "Note that the variance of this estimate is\n",
    "\\begin{align}\n",
    "\\text{Var}(\\hat \\mu_f) &=  \\frac{1}{S}\\text{Var}(f(Z)) \\\\\n",
    "&= \\frac{1}{S} \\mathbb E[( f(Z) - \\mathbb E[f(Z)])^2]\n",
    "\\end{align}\n",
    "Note that this variance is such that it goes down as we sample more, in a rate $\\mathcal O(S^{-1})$.\n",
    "See that if we sample $10$ times more, we will only obtain an decrease in variance in the order of $10^{-1}$. This means that sampling more is generally not the most convenient way to decrease variance.\n",
    "\n",
    "*Digression* we can estimate the variance itself via MC, an unbiased estimate looks like\n",
    "\\begin{align}\n",
    "\\hat \\sigma^2_f = \\frac{1}{S(S-1)} \\sum_{s=1}^S (f(z_s) - \\hat \\mu_f)^2\n",
    "\\end{align}\n",
    "but not that this estimate is even hard to improve since it decreases with $\\mathcal O(S^{-2})$.\n",
    "\n",
    "Back to out main problem: let's try and improve the variance of our estimator to $\\mu_f$.\n",
    "\n",
    "It's a fact, and it can be shown trivially, that\n",
    "\\begin{align}\n",
    "\\mu_f &=  \\mathbb E[f(Z) - \\psi(Z)] + \\underbrace{\\mathbb E[\\psi(Z)]}_{\\mu_\\psi} \\\\\n",
    " &\\overset{\\text{MC}}{\\approx} \\underbrace{\\left(\\frac{1}{S} \\sum_{s=1}^S f(z_s) - \\psi(z_s) \\right) + \\mu_\\psi}_{\\hat c}\n",
    "\\end{align}\n",
    "where we assume the existence of some function $\\psi(z)$ for which the expected value $\\mu_\\psi$ is known and we estimate the expected difference $\\mathbb E[f(Z) - \\psi(Z)]$ via MC. We used this axuxiliary function, also known as a *control variate*, to derive a new estimator, which we will denote by $\\hat c$.\n",
    "\n",
    "The variance of this new estimator is show below:\n",
    "\n",
    "\\begin{align}\n",
    "\\text{Var}( \\hat c ) &= \\text{Var}(\\hat \\mu_{f-\\psi}) + 2\\underbrace{\\text{Cov}(\\hat \\mu_{f-\\psi}, \\mu_\\psi)}_{\\mathbb E[\\hat \\mu_{f-\\psi}  \\mu_\\psi] - \\mathbb E[\\hat \\mu_{f-\\psi}] \\mathbb E[\\mu_\\psi]} + \\underbrace{\\text{Var}(\\mu_\\psi)}_{\\color{blue}{0} } \\\\\n",
    "&= \\frac{1}{S}\\text{Var}(f- \\psi)  + 2 \\underbrace{\\left( \\mu_\\psi \\mu_{f-\\psi} - \\mu_{f-\\psi} \\mu_\\psi \\right)}_{\\color{blue}{0}} \n",
    "\\end{align}\n",
    "where the variance of $\\mu_\\psi$ is 0 because we know it in closed form (no need for MC estimation), and the covariance is $0$ as shown in the second row.\n",
    "\n",
    "That is, the variance of $\\hat c$ is essentially the variance of estimating $\\mathbb E[f(Z) - \\psi(Z)]$, which in turn depends on the variance \n",
    "\n",
    "\\begin{align}\n",
    "\\text{Var}(f-\\psi) &= \\text{Var}(f) - 2\\text{Cov}(f, \\psi) + \\text{Var}(\\psi)\n",
    "\\end{align}\n",
    "where we can see that if $\\text{Cov}(f, \\psi) > \\frac{\\text{Var}(\\psi)}{2}$ we achieve variance reduction as then $\\text{Var}(f-\\psi)$ would be smaller than $\\text{Var(f)}$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ovKcRnqH_PGp"
   },
   "source": [
    "\n",
    "## Baselines\n",
    "\n",
    "Baslines are control variates of a very simple form:\n",
    "\\begin{align}\n",
    "\\mathbb E[f(Z)] = \\mathbb E[f(Z) - C] + \\mathbb E[C]\n",
    "\\end{align}\n",
    "where $C$ is a constant with respect to $z$.\n",
    "\n",
    "In the context of the score function estimator, a baseline looks like a quantity $C(x; \\omega)$, this may be\n",
    "* just a constant;\n",
    "* or a function of the input (but not of the latent variable), which could be itself implemented as a neural network;\n",
    "* a combination of the two.\n",
    " \n",
    "\n",
    "Let's focus on the first term of the ELBO (so I'm omitting the KL term here). The gradient with respect to parameters of the inference model becomes:\n",
    "\n",
    "\\begin{align}\n",
    "&\\mathbb E_{Q(z|x, \\lambda)}\\left[ \\log P(x|z, \\theta) \\nabla_\\lambda \\log Q(z|x, \\lambda)\\right]\\\\\n",
    "&=\\mathbb E_{Q(z|x, \\lambda)}\\left[\\log P(x|z, \\theta) \\nabla_\\lambda \\log Q(z|x, \\lambda) - \\color{red}{C(x; \\omega)\\nabla_\\lambda \\log Q(z|x, \\lambda) }  \\right] + \\underbrace{\\mathbb E_{Q(z|x, \\lambda)}\\left[\\color{red}{C(x; \\omega)\\nabla_\\lambda \\log Q(z|x, \\lambda) }  \\right] }_{=0} \\\\\n",
    "&= \\mathbb E_{Q(z|x, \\lambda)}\\left[ \\color{blue}{\\left(\\log P(x|z, \\theta) - C(x; \\omega) \\right)}\\nabla_\\lambda \\log Q(z|x, \\lambda)\\right] \\\\\n",
    "&\n",
    "\\end{align}\n",
    "We can show that the last term is $0$\n",
    "\n",
    "\\begin{align}\n",
    "&\\mathbb E_{Q(z|x, \\lambda)}\\left[C(x; \\omega)\\nabla_\\lambda \\log Q(z|x, \\lambda)   \\right]  \\\\&= C(x; \\omega) \\mathbb E_{Q(z|x, \\lambda)}\\left[\\nabla_\\lambda \\log Q(z|x, \\lambda)   \\right]\\\\\n",
    "&= C(x; \\omega) \\mathbb E_{Q(z|x, \\lambda)}\\left[\\frac{1}{Q(z|x, \\lambda)} \\nabla_\\lambda Q(z|x, \\lambda)   \\right] \\\\\n",
    "&= C(x; \\omega) \\sum_z Q(z|x, \\lambda) \\frac{1}{Q(z|x, \\lambda)} \\nabla_\\lambda Q(z|x, \\lambda)   \\\\\n",
    "&= C(x; \\omega) \\sum_z\\nabla_\\lambda Q(z|x, \\lambda)  \\\\\n",
    "&= C(x; \\omega) \\nabla_\\lambda \\underbrace{\\sum_z Q(z|x, \\lambda)  }_{=1}\\\\\n",
    "&=0\n",
    "\\end{align}\n",
    "\n",
    "Examples of useful baselines:\n",
    "\n",
    "* a running average of the learning signal: at some iteration $t$ we can use a running average of $\\log P(x|z, \\theta)$ using parameter estimates $\\theta$ from iterations $i < t$, this is a baseline that likely leads to high correlation between control variate and learning signal and can lead to variance reduction;\n",
    "* another technique is to have an MLP with parameters $\\omega$ predict a scalar and train this MLP to approximate the learning signal $\\log P(x|z, \\theta)$ via regression:\n",
    "\\begin{align}\n",
    "\\arg\\max_\\omega \\left( C(x; \\omega) - \\log P(x|z, \\theta) \\right)^2\n",
    "\\end{align}\n",
    "its left as an extra to implement these ideas.\n",
    "\n",
    "One more note: we can also use something called a *multiplicative baseline* in the literature of reinforcement learning, whereby we incorporate a running estimate of the standard deviation of the learning signal computed based on the values attained on previous iterations:\n",
    "\\begin{align}\n",
    "\\mathbb E_{Q(z|x, \\lambda)}\\left[ \\frac{1}{\\hat\\sigma_{\\text{past}}}\\left(\\log P(x|z, \\theta) - \\hat \\mu_{\\text{past}}\\right)\\nabla_\\lambda \\log Q(z|x, \\lambda)\\right]\n",
    "\\end{align}\n",
    "this form of contorl variate aim at promoting the learning signal (or reward in reinforcement learning literature) to be distributed by $\\mathcal N(0, 1)$. Note that multiplying the reward by a constant does not bias the estimator, and in this case, may lead to variance reduction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_counters(model, data, batch_size, device, cfg, iter_i):\n",
    "    _, rationales = evaluate(\n",
    "        model,\n",
    "        data, \n",
    "        batch_size=batch_size, \n",
    "        device=device,\n",
    "        cfg=cfg,\n",
    "        iter_i=0\n",
    "    )\n",
    "    important_counter = Counter()\n",
    "    all_counter = Counter()\n",
    "    for example in rationales:\n",
    "        tokens = [\n",
    "            token if not token.startswith(\"**\") else token[2:-2]\n",
    "            for token in example[0]\n",
    "        ]\n",
    "        tags = nltk.pos_tag(tokens)\n",
    "        for tag, token in zip(tags, example[0]):\n",
    "            all_counter[tag[1]] += 1\n",
    "            if token.startswith(\"**\"):\n",
    "                important_counter[tag[1]] += 1\n",
    "    return important_counter, all_counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "important_counter, all_counter = make_counters(model, dev_data, len(dev_data), device, cfg, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Сколько разные части речи считаются важными:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('JJ', 1284),\n",
       " ('NN', 1257),\n",
       " ('NNS', 515),\n",
       " ('RB', 373),\n",
       " ('VBG', 191),\n",
       " ('VBZ', 123),\n",
       " ('VBN', 112),\n",
       " ('VB', 112),\n",
       " ('VBD', 35),\n",
       " ('VBP', 32),\n",
       " ('IN', 28),\n",
       " (\"''\", 24),\n",
       " (':', 18),\n",
       " ('JJS', 16),\n",
       " ('JJR', 11),\n",
       " ('FW', 6),\n",
       " ('CD', 5),\n",
       " ('CC', 1),\n",
       " ('UH', 1),\n",
       " ('POS', 1),\n",
       " ('NNP', 1)]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Сколько разные части речи считаются важными:\")\n",
    "important_counter.most_common()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Частота важности частей речи:\n",
      "JJ 0.5634050021939447\n",
      "NNS 0.5124378109452736\n",
      "'' 0.47058823529411764\n",
      "VBG 0.4569377990430622\n",
      "FW 0.4\n",
      "VBN 0.3404255319148936\n",
      "NN 0.3106004447739066\n",
      "RB 0.2948616600790514\n",
      "UH 0.25\n",
      "JJS 0.2318840579710145\n",
      "VBD 0.22012578616352202\n",
      "VB 0.18855218855218855\n",
      "JJR 0.1746031746031746\n",
      "VBZ 0.119533527696793\n",
      "VBP 0.11510791366906475\n",
      ": 0.09326424870466321\n",
      "CD 0.03787878787878788\n",
      "NNP 0.02564102564102564\n",
      "IN 0.013005109150023224\n",
      "POS 0.005747126436781609\n",
      "CC 0.0012804097311139564\n",
      "PRP 0.0\n",
      "DT 0.0\n",
      ". 0.0\n",
      ", 0.0\n",
      "WDT 0.0\n",
      "TO 0.0\n",
      "PRP$ 0.0\n",
      "MD 0.0\n",
      "RBR 0.0\n",
      "WRB 0.0\n",
      "WP 0.0\n",
      "EX 0.0\n",
      "RP 0.0\n",
      "`` 0.0\n",
      "RBS 0.0\n",
      "WP$ 0.0\n",
      "PDT 0.0\n",
      "$ 0.0\n"
     ]
    }
   ],
   "source": [
    "fractions = []\n",
    "for token in all_counter:\n",
    "    fractions.append((token, important_counter[token] / all_counter[token]))\n",
    "print(\"Частота важности частей речи:\")\n",
    "for (token, frac) in sorted(fractions, key=lambda pair: -pair[1]):\n",
    "    print(token, frac)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "С большой частотой важными становятся слова, обозначающие действие или свойство. Как раз то, что нужно для понятия смысла. Бесполезные с точки зрения смысла слова типа IN (предлоги) выбразываются. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "# Constructing model\n",
      "Classifier #params: 1505\n",
      "ProductOfBernoullis #params: 542701\n",
      "\n",
      "# Loading embeddings\n",
      "fixed word embeddings\n",
      "Model(\n",
      "  (embed): Embedding(20727, 300, padding_idx=1)\n",
      "  (cls_net): Classifier(\n",
      "    (embed_layer): Sequential(\n",
      "      (0): Embedding(20727, 300, padding_idx=1)\n",
      "    )\n",
      "    (enc_layer): BagOfWordsEncoder()\n",
      "    (output_layer): Sequential(\n",
      "      (0): Dropout(p=0.5)\n",
      "      (1): Linear(in_features=300, out_features=5, bias=True)\n",
      "      (2): LogSoftmax()\n",
      "    )\n",
      "  )\n",
      "  (inference_net): ProductOfBernoullis(\n",
      "    (embed_layer): Sequential(\n",
      "      (0): Embedding(20727, 300, padding_idx=1)\n",
      "    )\n",
      "    (enc_layer): LSTMEncoder(\n",
      "      (lstm): LSTM(300, 150, batch_first=True, bidirectional=True)\n",
      "    )\n",
      "    (logit_layer): Linear(in_features=300, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "embed.weight             [20727, 300] requires_grad=False\n",
      "cls_net.output_layer.1.weight [5, 300]     requires_grad=True\n",
      "cls_net.output_layer.1.bias [5]          requires_grad=True\n",
      "inference_net.enc_layer.lstm.weight_ih_l0 [600, 300]   requires_grad=True\n",
      "inference_net.enc_layer.lstm.weight_hh_l0 [600, 150]   requires_grad=True\n",
      "inference_net.enc_layer.lstm.bias_ih_l0 [600]        requires_grad=True\n",
      "inference_net.enc_layer.lstm.bias_hh_l0 [600]        requires_grad=True\n",
      "inference_net.enc_layer.lstm.weight_ih_l0_reverse [600, 300]   requires_grad=True\n",
      "inference_net.enc_layer.lstm.weight_hh_l0_reverse [600, 150]   requires_grad=True\n",
      "inference_net.enc_layer.lstm.bias_ih_l0_reverse [600]        requires_grad=True\n",
      "inference_net.enc_layer.lstm.bias_hh_l0_reverse [600]        requires_grad=True\n",
      "inference_net.logit_layer.weight [1, 300]     requires_grad=True\n",
      "inference_net.logit_layer.bias [1]          requires_grad=True\n",
      "\n",
      "Total parameters: 6762306\n",
      "\n",
      "Shuffling training data\n",
      "Epoch 0 Iter 100 loss=0.2579 elbo -0.3450 sf 0.7383 selected 0.4480 kl 0.0275 ll -1.0833\n",
      "Epoch 0 Iter 200 loss=0.3258 elbo -0.3666 sf 0.7322 selected 0.4354 kl 0.0153 ll -1.0988\n",
      "Epoch 0 Iter 300 loss=0.3471 elbo -0.3817 sf 0.7646 selected 0.4194 kl 0.0189 ll -1.1462\n",
      "\n",
      "# epoch 0 iter 341: dev loss 0.4481 elbo -0.4481 sf 0.3909 selected 0.0001 kl 0.0081 ll -0.8309 acc 0.2534\n",
      " dev0 [gold=3,pred=3]: it 's a lovely film with lovely performances by buy and accorsi .\n",
      " dev1 [gold=2,pred=3]: no one goes unindicted here , which is probably for the best .\n",
      " dev2 [gold=3,pred=3]: and if you 're not nearly moved to tears by a couple of scenes , you 've got ice water in your veins .\n",
      "\n",
      "Shuffling training data\n",
      "Epoch 1 Iter 400 loss=0.3670 elbo -0.3909 sf 0.7540 selected 0.4213 kl 0.0080 ll -1.1449\n",
      "Epoch 1 Iter 500 loss=0.3856 elbo -0.4549 sf 0.7809 selected 0.4071 kl 0.0039 ll -1.2358\n",
      "Epoch 1 Iter 600 loss=0.3893 elbo -0.3154 sf 0.6600 selected 0.4567 kl 0.0173 ll -0.9753\n",
      "\n",
      "# epoch 1 iter 682: dev loss 0.3403 elbo -0.3403 sf 0.5147 selected 0.4970 kl 0.0337 ll -0.8213 acc 0.2816\n",
      " dev0 [gold=3,pred=3]: it 's a lovely film with lovely performances by buy and accorsi .\n",
      " dev1 [gold=2,pred=3]: **no** one goes **unindicted** here , which is probably for the best .\n",
      " dev2 [gold=3,pred=3]: and if you 're not nearly moved to tears by a couple **of** scenes , you 've got ice water in your veins .\n",
      "\n",
      "Shuffling training data\n",
      "Epoch 2 Iter 700 loss=0.3421 elbo -0.5359 sf 1.2357 selected 0.4941 kl 0.0329 ll -1.7714\n",
      "Epoch 2 Iter 800 loss=0.3324 elbo -0.4624 sf 1.0284 selected 0.4662 kl 0.0375 ll -1.4905\n",
      "Epoch 2 Iter 900 loss=0.3359 elbo -0.2671 sf 0.6014 selected 0.5109 kl 0.0295 ll -0.8682\n",
      "Epoch 2 Iter 1000 loss=0.3514 elbo -0.3552 sf 0.7505 selected 0.5570 kl 0.1006 ll -1.1047\n",
      "\n",
      "# epoch 2 iter 1023: dev loss 0.4350 elbo -0.4350 sf 0.4466 selected 0.9620 kl 0.0838 ll -0.7978 acc 0.3524\n",
      " dev0 [gold=3,pred=1]: **it** **'s** **a** **lovely** **film** **with** **lovely** **performances** by buy **and** **accorsi** **.**\n",
      " dev1 [gold=2,pred=3]: **no** **one** **goes** **unindicted** **here** **,** **which** **is** **probably** **for** **the** best **.**\n",
      " dev2 [gold=3,pred=3]: **and** **if** you 're **not** **nearly** moved to **tears** **by** **a** **couple** **of** **scenes** **,** you 've got ice **water** **in** your **veins** **.**\n",
      "\n",
      "Shuffling training data\n",
      "Epoch 3 Iter 1100 loss=0.3850 elbo -0.3804 sf 0.7810 selected 0.5405 kl 0.0489 ll -1.1608\n",
      "Epoch 3 Iter 1200 loss=0.3401 elbo -0.3285 sf 0.7364 selected 0.5006 kl 0.0568 ll -1.0642\n",
      "Epoch 3 Iter 1300 loss=0.3523 elbo -0.3334 sf 0.7255 selected 0.5238 kl 0.0887 ll -1.0577\n",
      "\n",
      "# epoch 3 iter 1364: dev loss 0.3772 elbo -0.3772 sf 0.4740 selected 0.8763 kl 0.0659 ll -0.7853 acc 0.3615\n",
      " dev0 [gold=3,pred=1]: **it** **'s** **a** **lovely** **film** **with** **lovely** performances by buy and accorsi .\n",
      " dev1 [gold=2,pred=3]: **no** **one** **goes** **unindicted** **here** **,** **which** **is** **probably** **for** **the** best .\n",
      " dev2 [gold=3,pred=1]: **and** **if** you **'re** **not** **nearly** moved to **tears** by **a** **couple** **of** **scenes** **,** you **'ve** got ice **water** **in** your **veins** .\n",
      "\n",
      "Shuffling training data\n",
      "Epoch 4 Iter 1400 loss=0.3307 elbo -0.3179 sf 0.6742 selected 0.5300 kl 0.0568 ll -0.9912\n",
      "Epoch 4 Iter 1500 loss=0.3166 elbo -0.2806 sf 0.6315 selected 0.4764 kl 0.0324 ll -0.9117\n",
      "Epoch 4 Iter 1600 loss=0.3219 elbo -0.3676 sf 0.8094 selected 0.4856 kl 0.0469 ll -1.1763\n",
      "Epoch 4 Iter 1700 loss=0.3148 elbo -0.3407 sf 0.7381 selected 0.5312 kl 0.0555 ll -1.0779\n",
      "\n",
      "# epoch 4 iter 1705: dev loss 0.3703 elbo -0.3703 sf 0.4722 selected 0.8538 kl 0.0631 ll -0.7794 acc 0.3588\n",
      " dev0 [gold=3,pred=1]: **it** **'s** **a** **lovely** **film** **with** **lovely** performances by buy and accorsi .\n",
      " dev1 [gold=2,pred=3]: **no** **one** **goes** **unindicted** **here** , **which** **is** **probably** **for** the best .\n",
      " dev2 [gold=3,pred=1]: **and** **if** you **'re** **not** **nearly** moved to tears by **a** **couple** **of** **scenes** , you **'ve** got ice water **in** your **veins** .\n",
      "\n",
      "Shuffling training data\n",
      "Epoch 5 Iter 1800 loss=0.3261 elbo -0.3845 sf 0.8074 selected 0.5424 kl 0.0795 ll -1.1904\n",
      "Epoch 5 Iter 1900 loss=0.3258 elbo -0.4016 sf 0.8868 selected 0.5290 kl 0.0603 ll -1.2873\n",
      "Epoch 5 Iter 2000 loss=0.3427 elbo -0.3965 sf 0.8556 selected 0.5382 kl 0.0657 ll -1.2508\n",
      "\n",
      "# epoch 5 iter 2046: dev loss 0.3414 elbo -0.3414 sf 0.4892 selected 0.6951 kl 0.0459 ll -0.7847 acc 0.3488\n",
      " dev0 [gold=3,pred=3]: it 's a **lovely** **film** with **lovely** performances by buy and accorsi .\n",
      " dev1 [gold=2,pred=3]: no one goes **unindicted** here , which is probably for the best .\n",
      " dev2 [gold=3,pred=1]: and if you 're not nearly moved to tears by a **couple** **of** **scenes** , you 've got ice water **in** your veins .\n",
      "\n",
      "Shuffling training data\n",
      "Epoch 6 Iter 2100 loss=0.3476 elbo -0.2831 sf 0.6182 selected 0.4903 kl 0.0404 ll -0.9004\n",
      "Epoch 6 Iter 2200 loss=0.3528 elbo -0.2609 sf 0.5704 selected 0.5219 kl 0.0376 ll -0.8304\n",
      "Epoch 6 Iter 2300 loss=0.3462 elbo -0.3449 sf 0.7337 selected 0.4924 kl 0.0260 ll -1.0781\n",
      "\n",
      "# epoch 6 iter 2387: dev loss 0.3830 elbo -0.3830 sf 0.4439 selected 0.0876 kl 0.0282 ll -0.7987 acc 0.3351\n",
      " dev0 [gold=3,pred=3]: it 's a **lovely** film with **lovely** performances by buy and accorsi .\n",
      " dev1 [gold=2,pred=3]: no one goes **unindicted** here , which is probably for the best .\n",
      " dev2 [gold=3,pred=1]: and if you 're not nearly moved to tears by a couple of **scenes** , you 've got ice water in your veins .\n",
      "\n",
      "Shuffling training data\n",
      "Epoch 7 Iter 2400 loss=0.3551 elbo -0.3732 sf 0.7990 selected 0.4695 kl 0.0291 ll -1.1715\n",
      "Epoch 7 Iter 2500 loss=0.3510 elbo -0.3812 sf 0.7509 selected 0.4553 kl 0.0151 ll -1.1317\n",
      "Epoch 7 Iter 2600 loss=0.3502 elbo -0.3090 sf 0.6356 selected 0.4410 kl 0.0147 ll -0.9443\n",
      "Epoch 7 Iter 2700 loss=0.3677 elbo -0.3529 sf 0.6670 selected 0.4503 kl 0.0317 ll -1.0190\n",
      "\n",
      "# epoch 7 iter 2728: dev loss 0.4459 elbo -0.4459 sf 0.3728 selected 0.0808 kl 0.0223 ll -0.7964 acc 0.3297\n",
      " dev0 [gold=3,pred=3]: it 's a **lovely** film with **lovely** performances by buy and accorsi .\n",
      " dev1 [gold=2,pred=3]: no one goes unindicted here , which is probably for the best .\n",
      " dev2 [gold=3,pred=1]: and if you 're not nearly moved to tears by a couple of scenes , you 've got ice water in your veins .\n",
      "\n",
      "Epoch     7: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Shuffling training data\n",
      "Epoch 8 Iter 2800 loss=0.3623 elbo -0.4112 sf 0.8809 selected 0.4573 kl 0.0273 ll -1.2913\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8 Iter 2900 loss=0.3698 elbo -0.4563 sf 0.6713 selected 0.4069 kl 0.0177 ll -1.1271\n",
      "Epoch 8 Iter 3000 loss=0.4207 elbo -0.5887 sf 0.9855 selected 0.4262 kl 0.0299 ll -1.5734\n",
      "\n",
      "# epoch 8 iter 3069: dev loss 0.4592 elbo -0.4592 sf 0.3588 selected 0.0966 kl 0.0272 ll -0.7908 acc 0.3261\n",
      " dev0 [gold=3,pred=3]: it 's a **lovely** film with **lovely** performances by buy and accorsi .\n",
      " dev1 [gold=2,pred=1]: no one goes unindicted here , which is probably for the best .\n",
      " dev2 [gold=3,pred=1]: and if you 're not nearly moved to tears by a couple of **scenes** , you 've got ice water in your veins .\n",
      "\n",
      "Shuffling training data\n",
      "Epoch 9 Iter 3100 loss=0.3945 elbo -0.3492 sf 0.5809 selected 0.4225 kl 0.0229 ll -0.9293\n",
      "Epoch 9 Iter 3200 loss=0.3839 elbo -0.3378 sf 0.7126 selected 0.4670 kl 0.0363 ll -1.0493\n",
      "Epoch 9 Iter 3300 loss=0.3723 elbo -0.4514 sf 0.8600 selected 0.4566 kl 0.0280 ll -1.3104\n",
      "Epoch 9 Iter 3400 loss=0.4097 elbo -0.3276 sf 0.5684 selected 0.4545 kl 0.0212 ll -0.8952\n",
      "\n",
      "# epoch 9 iter 3410: dev loss 0.4735 elbo -0.4735 sf 0.3422 selected 0.0887 kl 0.0258 ll -0.7899 acc 0.3306\n",
      " dev0 [gold=3,pred=3]: it 's a **lovely** film with **lovely** performances by buy and accorsi .\n",
      " dev1 [gold=2,pred=1]: no one goes unindicted here , which is probably for the best .\n",
      " dev2 [gold=3,pred=1]: and if you 're not nearly moved to tears by a couple of scenes , you 've got ice water in your veins .\n",
      "\n",
      "Shuffling training data\n",
      "Epoch 10 Iter 3500 loss=0.4161 elbo -0.3513 sf 0.6072 selected 0.4291 kl 0.0232 ll -0.9577\n",
      "Epoch 10 Iter 3600 loss=0.3967 elbo -0.3990 sf 0.6303 selected 0.4322 kl 0.0254 ll -1.0284\n",
      "Epoch 10 Iter 3700 loss=0.3960 elbo -0.3052 sf 0.5843 selected 0.4722 kl 0.0376 ll -0.8881\n",
      "\n",
      "# epoch 10 iter 3751: dev loss 0.4525 elbo -0.4525 sf 0.3667 selected 0.2837 kl 0.0386 ll -0.7806 acc 0.3470\n",
      " dev0 [gold=3,pred=3]: it 's a **lovely** film with **lovely** **performances** by buy and accorsi .\n",
      " dev1 [gold=2,pred=3]: no one goes **unindicted** here , which is probably for the best .\n",
      " dev2 [gold=3,pred=1]: and if you 're not nearly moved to tears by a couple of **scenes** , you 've got ice water in your **veins** .\n",
      "\n",
      "Shuffling training data\n",
      "Epoch 11 Iter 3800 loss=0.3697 elbo -0.2999 sf 0.5272 selected 0.4627 kl 0.0321 ll -0.8259\n",
      "Epoch 11 Iter 3900 loss=0.3832 elbo -0.5251 sf 0.8898 selected 0.4486 kl 0.0394 ll -1.4134\n",
      "Epoch 11 Iter 4000 loss=0.3691 elbo -0.4349 sf 0.7989 selected 0.4818 kl 0.0477 ll -1.2319\n",
      "\n",
      "# epoch 11 iter 4092: dev loss 0.4348 elbo -0.4348 sf 0.3859 selected 0.6470 kl 0.0479 ll -0.7728 acc 0.3579\n",
      " dev0 [gold=3,pred=3]: it 's a **lovely** film with **lovely** **performances** by buy and **accorsi** .\n",
      " dev1 [gold=2,pred=3]: no one goes **unindicted** here , which is probably for the best .\n",
      " dev2 [gold=3,pred=1]: and if you 're not nearly moved to **tears** by a **couple** of **scenes** , you 've got ice water in your **veins** .\n",
      "\n",
      "Epoch 12 Iter 4100 loss=0.3409 elbo -0.2560 sf 0.5321 selected 0.4600 kl 0.0355 ll -0.7867\n",
      "Shuffling training data\n",
      "Epoch 12 Iter 4200 loss=0.3540 elbo -0.3667 sf 0.6037 selected 0.4829 kl 0.0453 ll -0.9685\n",
      "Epoch 12 Iter 4300 loss=0.3586 elbo -0.3441 sf 0.6394 selected 0.4761 kl 0.0387 ll -0.9819\n",
      "Epoch 12 Iter 4400 loss=0.3625 elbo -0.4293 sf 0.7587 selected 0.4965 kl 0.0457 ll -1.1860\n",
      "\n",
      "# epoch 12 iter 4433: dev loss 0.4540 elbo -0.4540 sf 0.3658 selected 0.6223 kl 0.0471 ll -0.7726 acc 0.3579\n",
      " dev0 [gold=3,pred=3]: it 's a **lovely** film with **lovely** **performances** by buy and **accorsi** .\n",
      " dev1 [gold=2,pred=3]: no one goes unindicted here , which is probably for the best .\n",
      " dev2 [gold=3,pred=1]: and if you 're not nearly moved to tears by a couple of **scenes** , you 've got ice water in your **veins** .\n",
      "\n",
      "Shuffling training data\n",
      "Epoch 13 Iter 4500 loss=0.3651 elbo -0.3676 sf 0.7617 selected 0.4863 kl 0.0460 ll -1.1272\n",
      "Epoch 13 Iter 4600 loss=0.3819 elbo -0.3593 sf 0.6209 selected 0.4453 kl 0.0361 ll -0.9785\n",
      "Epoch 13 Iter 4700 loss=0.3783 elbo -0.2790 sf 0.4757 selected 0.4774 kl 0.0394 ll -0.7528\n",
      "\n",
      "# epoch 13 iter 4774: dev loss 0.4514 elbo -0.4514 sf 0.3674 selected 0.6343 kl 0.0508 ll -0.7681 acc 0.3688\n",
      " dev0 [gold=3,pred=3]: it 's a **lovely** film with **lovely** **performances** by buy and **accorsi** .\n",
      " dev1 [gold=2,pred=3]: no one goes unindicted here , which is probably for the best .\n",
      " dev2 [gold=3,pred=1]: and if you 're not nearly moved to tears by a couple of **scenes** , you 've got ice water in your **veins** .\n",
      "\n",
      "Shuffling training data\n",
      "Epoch 14 Iter 4800 loss=0.3842 elbo -0.4217 sf 0.7805 selected 0.4617 kl 0.0609 ll -1.1992\n",
      "Epoch 14 Iter 4900 loss=0.3653 elbo -0.5253 sf 0.9285 selected 0.4800 kl 0.0621 ll -1.4508\n",
      "Epoch 14 Iter 5000 loss=0.3647 elbo -0.2879 sf 0.5028 selected 0.4818 kl 0.0434 ll -0.7886\n",
      "Epoch 14 Iter 5100 loss=0.3813 elbo -0.2893 sf 0.5268 selected 0.4627 kl 0.0402 ll -0.8140\n",
      "\n",
      "# epoch 14 iter 5115: dev loss 0.4586 elbo -0.4586 sf 0.3554 selected 0.1579 kl 0.0459 ll -0.7681 acc 0.3669\n",
      " dev0 [gold=3,pred=3]: it 's a **lovely** film with **lovely** **performances** by buy and **accorsi** .\n",
      " dev1 [gold=2,pred=1]: no one goes unindicted here , which is probably for the best .\n",
      " dev2 [gold=3,pred=1]: and if you 're not nearly moved to **tears** by a couple of **scenes** , you 've got ice water in your **veins** .\n",
      "\n",
      "Shuffling training data\n",
      "Epoch 15 Iter 5200 loss=0.3744 elbo -0.4126 sf 0.7222 selected 0.4974 kl 0.0637 ll -1.1315\n",
      "Epoch 15 Iter 5300 loss=0.3637 elbo -0.4991 sf 0.8774 selected 0.4850 kl 0.0756 ll -1.3725\n",
      "Epoch 15 Iter 5400 loss=0.3615 elbo -0.4106 sf 0.7568 selected 0.5168 kl 0.0617 ll -1.1640\n",
      "\n",
      "# epoch 15 iter 5456: dev loss 0.4537 elbo -0.4537 sf 0.3630 selected 0.5762 kl 0.0558 ll -0.7608 acc 0.3724\n",
      " dev0 [gold=3,pred=1]: it 's a **lovely** film with **lovely** **performances** by buy and **accorsi** .\n",
      " dev1 [gold=2,pred=1]: no one goes unindicted here , which is probably for the best .\n",
      " dev2 [gold=3,pred=1]: and if you 're not nearly moved to **tears** by a **couple** of **scenes** , you 've got ice **water** in your **veins** .\n",
      "\n",
      "Shuffling training data\n",
      "Epoch 16 Iter 5500 loss=0.3595 elbo -0.3010 sf 0.5515 selected 0.5143 kl 0.0635 ll -0.8491\n",
      "Epoch 16 Iter 5600 loss=0.3581 elbo -0.3635 sf 0.6124 selected 0.5040 kl 0.0721 ll -0.9718\n",
      "Epoch 16 Iter 5700 loss=0.3664 elbo -0.3658 sf 0.6720 selected 0.5200 kl 0.0605 ll -1.0344\n",
      "\n",
      "# epoch 16 iter 5797: dev loss 0.4507 elbo -0.4507 sf 0.3655 selected 0.2259 kl 0.0596 ll -0.7566 acc 0.3669\n",
      " dev0 [gold=3,pred=1]: it 's a **lovely** film with **lovely** **performances** by buy **and** **accorsi** .\n",
      " dev1 [gold=2,pred=1]: **no** one goes unindicted here , which is probably for the best .\n",
      " dev2 [gold=3,pred=1]: and if you 're not nearly moved to **tears** by a **couple** of **scenes** , you 've got ice **water** in your **veins** .\n",
      "\n",
      "Epoch 17 Iter 5800 loss=0.3650 elbo -0.4143 sf 0.6399 selected 0.4764 kl 0.0513 ll -1.0512\n",
      "Shuffling training data\n",
      "Epoch 17 Iter 5900 loss=0.3561 elbo -0.3170 sf 0.6037 selected 0.5040 kl 0.0525 ll -0.9176\n",
      "Epoch 17 Iter 6000 loss=0.3553 elbo -0.3294 sf 0.6149 selected 0.4933 kl 0.0549 ll -0.9411\n",
      "Epoch 17 Iter 6100 loss=0.3669 elbo -0.3541 sf 0.7097 selected 0.4947 kl 0.0662 ll -1.0598\n",
      "\n",
      "# epoch 17 iter 6138: dev loss 0.4541 elbo -0.4541 sf 0.3591 selected 0.1953 kl 0.0572 ll -0.7559 acc 0.3806\n",
      " dev0 [gold=3,pred=1]: it 's a **lovely** film with **lovely** **performances** by buy and **accorsi** .\n",
      " dev1 [gold=2,pred=1]: **no** one goes unindicted here , which is probably for the best .\n",
      " dev2 [gold=3,pred=1]: and if you 're not nearly moved to **tears** by a **couple** of **scenes** , you 've got ice water in your **veins** .\n",
      "\n",
      "Shuffling training data\n",
      "Epoch 18 Iter 6200 loss=0.3751 elbo -0.2993 sf 0.4991 selected 0.4629 kl 0.0422 ll -0.7957\n",
      "Epoch 18 Iter 6300 loss=0.3793 elbo -0.3081 sf 0.6032 selected 0.4702 kl 0.0433 ll -0.9086\n",
      "Epoch 18 Iter 6400 loss=0.3768 elbo -0.3259 sf 0.5211 selected 0.4710 kl 0.0488 ll -0.8439\n",
      "\n",
      "# epoch 18 iter 6479: dev loss 0.4567 elbo -0.4567 sf 0.3510 selected 0.1596 kl 0.0493 ll -0.7585 acc 0.3869\n",
      " dev0 [gold=3,pred=1]: it 's a **lovely** film with **lovely** **performances** by buy and **accorsi** .\n",
      " dev1 [gold=2,pred=1]: **no** one goes unindicted here , which is probably for the best .\n",
      " dev2 [gold=3,pred=1]: and if you 're not nearly moved to tears by a **couple** of **scenes** , you 've got ice water in your **veins** .\n",
      "\n",
      "Epoch    18: reducing learning rate of group 0 to 5.0000e-05.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shuffling training data\n",
      "Epoch 19 Iter 6500 loss=0.3695 elbo -0.3978 sf 0.7573 selected 0.4646 kl 0.0548 ll -1.1516\n",
      "Epoch 19 Iter 6600 loss=0.3682 elbo -0.2790 sf 0.5279 selected 0.4530 kl 0.0432 ll -0.8041\n",
      "Epoch 19 Iter 6700 loss=0.3639 elbo -0.4839 sf 0.8070 selected 0.4389 kl 0.0535 ll -1.2873\n",
      "Epoch 19 Iter 6800 loss=0.3717 elbo -0.3147 sf 0.5740 selected 0.4329 kl 0.0333 ll -0.8864\n",
      "\n",
      "# epoch 19 iter 6820: dev loss 0.4587 elbo -0.4587 sf 0.3493 selected 0.1586 kl 0.0500 ll -0.7580 acc 0.3824\n",
      " dev0 [gold=3,pred=1]: it 's a **lovely** film with **lovely** **performances** by buy and **accorsi** .\n",
      " dev1 [gold=2,pred=1]: **no** one goes unindicted here , which is probably for the best .\n",
      " dev2 [gold=3,pred=1]: and if you 're not nearly moved to tears by a **couple** of **scenes** , you 've got ice water in your **veins** .\n",
      "\n",
      "Shuffling training data\n",
      "Epoch 20 Iter 6900 loss=0.3733 elbo -0.2926 sf 0.5230 selected 0.4549 kl 0.0508 ll -0.8121\n",
      "Epoch 20 Iter 7000 loss=0.3694 elbo -0.3076 sf 0.5464 selected 0.4650 kl 0.0443 ll -0.8510\n",
      "Epoch 20 Iter 7100 loss=0.3709 elbo -0.2823 sf 0.4971 selected 0.4337 kl 0.0428 ll -0.7764\n",
      "\n",
      "# epoch 20 iter 7161: dev loss 0.4712 elbo -0.4712 sf 0.3338 selected 0.1322 kl 0.0438 ll -0.7612 acc 0.3851\n",
      " dev0 [gold=3,pred=1]: it 's a **lovely** film with **lovely** **performances** by buy and accorsi .\n",
      " dev1 [gold=2,pred=1]: **no** one goes unindicted here , which is probably for the best .\n",
      " dev2 [gold=3,pred=1]: and if you 're not nearly moved to tears by a couple of **scenes** , you 've got ice water in your veins .\n",
      "\n",
      "Shuffling training data\n",
      "Epoch 21 Iter 7200 loss=0.3843 elbo -0.3238 sf 0.4393 selected 0.4568 kl 0.0388 ll -0.7603\n",
      "Epoch 21 Iter 7300 loss=0.4047 elbo -0.4469 sf 0.7516 selected 0.4389 kl 0.0404 ll -1.1955\n",
      "Epoch 21 Iter 7400 loss=0.3762 elbo -0.2235 sf 0.3579 selected 0.4408 kl 0.0248 ll -0.5796\n",
      "Epoch 21 Iter 7500 loss=0.4021 elbo -0.3550 sf 0.5511 selected 0.3928 kl 0.0286 ll -0.9039\n",
      "\n",
      "# epoch 21 iter 7502: dev loss 0.4901 elbo -0.4901 sf 0.3139 selected 0.1112 kl 0.0394 ll -0.7646 acc 0.3769\n",
      " dev0 [gold=3,pred=1]: it 's a **lovely** film with **lovely** **performances** by buy and accorsi .\n",
      " dev1 [gold=2,pred=1]: no one goes unindicted here , which is probably for the best .\n",
      " dev2 [gold=3,pred=1]: and if you 're not nearly moved to tears by a couple of **scenes** , you 've got ice water in your veins .\n",
      "\n",
      "Shuffling training data\n",
      "Epoch 22 Iter 7600 loss=0.4149 elbo -0.3832 sf 0.6770 selected 0.4181 kl 0.0464 ll -1.0567\n",
      "Epoch 22 Iter 7700 loss=0.3884 elbo -0.4490 sf 0.6884 selected 0.4455 kl 0.0382 ll -1.1345\n",
      "Epoch 22 Iter 7800 loss=0.3869 elbo -0.2629 sf 0.4455 selected 0.4451 kl 0.0402 ll -0.7052\n",
      "\n",
      "# epoch 22 iter 7843: dev loss 0.4685 elbo -0.4685 sf 0.3335 selected 0.1301 kl 0.0430 ll -0.7590 acc 0.3806\n",
      " dev0 [gold=3,pred=1]: it 's a **lovely** film with **lovely** **performances** by buy and **accorsi** .\n",
      " dev1 [gold=2,pred=1]: **no** one goes unindicted here , which is probably for the best .\n",
      " dev2 [gold=3,pred=1]: and if you 're not nearly moved to tears by a couple of **scenes** , you 've got ice water in your veins .\n",
      "\n",
      "Shuffling training data\n",
      "Epoch 23 Iter 7900 loss=0.3884 elbo -0.2894 sf 0.4703 selected 0.4173 kl 0.0301 ll -0.7574\n",
      "Epoch 23 Iter 8000 loss=0.3925 elbo -0.5000 sf 0.7708 selected 0.4276 kl 0.0502 ll -1.2668\n",
      "Epoch 23 Iter 8100 loss=0.3886 elbo -0.3595 sf 0.6057 selected 0.4530 kl 0.0379 ll -0.9621\n",
      "\n",
      "# epoch 23 iter 8184: dev loss 0.4772 elbo -0.4772 sf 0.3251 selected 0.1303 kl 0.0444 ll -0.7579 acc 0.3851\n",
      " dev0 [gold=3,pred=1]: it 's a **lovely** film with **lovely** **performances** by buy and **accorsi** .\n",
      " dev1 [gold=2,pred=1]: **no** one goes unindicted here , which is probably for the best .\n",
      " dev2 [gold=3,pred=1]: and if you 're not nearly moved to tears by a couple of **scenes** , you 've got ice water in your veins .\n",
      "\n",
      "Epoch 24 Iter 8200 loss=0.3821 elbo -0.3339 sf 0.6345 selected 0.4183 kl 0.0307 ll -0.9659\n",
      "Shuffling training data\n",
      "Epoch 24 Iter 8300 loss=0.3996 elbo -0.2872 sf 0.4282 selected 0.4165 kl 0.0381 ll -0.7123\n",
      "Epoch 24 Iter 8400 loss=0.4004 elbo -0.4141 sf 0.5500 selected 0.4221 kl 0.0417 ll -0.9606\n",
      "Epoch 24 Iter 8500 loss=0.4052 elbo -0.3489 sf 0.5527 selected 0.4190 kl 0.0382 ll -0.8983\n",
      "\n",
      "# epoch 24 iter 8525: dev loss 0.4971 elbo -0.4971 sf 0.3033 selected 0.0970 kl 0.0352 ll -0.7652 acc 0.3778\n",
      " dev0 [gold=3,pred=1]: it 's a **lovely** film with **lovely** **performances** by buy and accorsi .\n",
      " dev1 [gold=2,pred=1]: **no** one goes unindicted here , which is probably for the best .\n",
      " dev2 [gold=3,pred=1]: and if you 're not nearly moved to tears by a couple of scenes , you 've got ice water in your veins .\n",
      "\n",
      "Shuffling training data\n",
      "Epoch 25 Iter 8600 loss=0.4287 elbo -0.5312 sf 0.6592 selected 0.3700 kl 0.0377 ll -1.1872\n",
      "Epoch 25 Iter 8700 loss=0.4402 elbo -0.3859 sf 0.5159 selected 0.3676 kl 0.0301 ll -0.8992\n",
      "Epoch 25 Iter 8800 loss=0.4161 elbo -0.3776 sf 0.5664 selected 0.3678 kl 0.0273 ll -0.9416\n",
      "\n",
      "# epoch 25 iter 8866: dev loss 0.4884 elbo -0.4884 sf 0.3094 selected 0.0935 kl 0.0333 ll -0.7646 acc 0.3806\n",
      " dev0 [gold=3,pred=1]: it 's a **lovely** film with **lovely** **performances** by buy and accorsi .\n",
      " dev1 [gold=2,pred=1]: no one goes unindicted here , which is probably for the best .\n",
      " dev2 [gold=3,pred=1]: and if you 're not nearly moved to tears by a couple of scenes , you 've got ice water in your veins .\n",
      "\n",
      "Shuffling training data\n",
      "Epoch 26 Iter 8900 loss=0.4098 elbo -0.4207 sf 0.7887 selected 0.4376 kl 0.0330 ll -1.2064\n",
      "Epoch 26 Iter 9000 loss=0.3915 elbo -0.4814 sf 0.7736 selected 0.4259 kl 0.0441 ll -1.2510\n",
      "Epoch 26 Iter 9100 loss=0.3923 elbo -0.4699 sf 0.6733 selected 0.3915 kl 0.0438 ll -1.1392\n",
      "Epoch 26 Iter 9200 loss=0.3702 elbo -0.2222 sf 0.3741 selected 0.4455 kl 0.0393 ll -0.5927\n",
      "\n",
      "# epoch 26 iter 9207: dev loss 0.4570 elbo -0.4570 sf 0.3404 selected 0.1261 kl 0.0416 ll -0.7559 acc 0.3851\n",
      " dev0 [gold=3,pred=1]: it 's a **lovely** film with **lovely** **performances** by buy and **accorsi** .\n",
      " dev1 [gold=2,pred=1]: **no** one goes unindicted here , which is probably for the best .\n",
      " dev2 [gold=3,pred=1]: and if you 're not nearly moved to tears by a couple of **scenes** , you 've got ice water in your veins .\n",
      "\n",
      "Shuffling training data\n",
      "Epoch 27 Iter 9300 loss=0.3723 elbo -0.4203 sf 0.6988 selected 0.4337 kl 0.0370 ll -1.1156\n",
      "Epoch 27 Iter 9400 loss=0.3835 elbo -0.3985 sf 0.7685 selected 0.4480 kl 0.0356 ll -1.1637\n",
      "Epoch 27 Iter 9500 loss=0.3755 elbo -0.5739 sf 1.1122 selected 0.4237 kl 0.0428 ll -1.6821\n",
      "\n",
      "# epoch 27 iter 9548: dev loss 0.4715 elbo -0.4715 sf 0.3251 selected 0.1042 kl 0.0358 ll -0.7608 acc 0.3833\n",
      " dev0 [gold=3,pred=1]: it 's a **lovely** film with **lovely** **performances** by buy and accorsi .\n",
      " dev1 [gold=2,pred=1]: **no** one goes unindicted here , which is probably for the best .\n",
      " dev2 [gold=3,pred=1]: and if you 're not nearly moved to tears by a couple of scenes , you 've got ice water in your veins .\n",
      "\n",
      "Shuffling training data\n",
      "Epoch 28 Iter 9600 loss=0.3707 elbo -0.3469 sf 0.5705 selected 0.4352 kl 0.0252 ll -0.9150\n",
      "Epoch 28 Iter 9700 loss=0.3905 elbo -0.2981 sf 0.4518 selected 0.4133 kl 0.0221 ll -0.7478\n",
      "Epoch 28 Iter 9800 loss=0.4000 elbo -0.3673 sf 0.6098 selected 0.4351 kl 0.0321 ll -0.9740\n",
      "\n",
      "# epoch 28 iter 9889: dev loss 0.4909 elbo -0.4909 sf 0.3061 selected 0.0892 kl 0.0326 ll -0.7644 acc 0.3742\n",
      " dev0 [gold=3,pred=1]: it 's a **lovely** film with **lovely** **performances** by buy and accorsi .\n",
      " dev1 [gold=2,pred=1]: no one goes unindicted here , which is probably for the best .\n",
      " dev2 [gold=3,pred=1]: and if you 're not nearly moved to tears by a couple of scenes , you 've got ice water in your veins .\n",
      "\n",
      "Epoch 29 Iter 9900 loss=0.4160 elbo -0.2843 sf 0.4357 selected 0.4238 kl 0.0302 ll -0.7171\n",
      "Shuffling training data\n",
      "Epoch 29 Iter 10000 loss=0.4061 elbo -0.4148 sf 0.7050 selected 0.3933 kl 0.0417 ll -1.1156\n",
      "Epoch 29 Iter 10100 loss=0.3863 elbo -0.4045 sf 0.6808 selected 0.4274 kl 0.0520 ll -1.0800\n",
      "Epoch 29 Iter 10200 loss=0.3770 elbo -0.4036 sf 0.7087 selected 0.4141 kl 0.0338 ll -1.1088\n",
      "\n",
      "# epoch 29 iter 10230: dev loss 0.4786 elbo -0.4786 sf 0.3184 selected 0.1073 kl 0.0385 ll -0.7585 acc 0.3860\n",
      " dev0 [gold=3,pred=1]: it 's a **lovely** film with **lovely** **performances** by buy and **accorsi** .\n",
      " dev1 [gold=2,pred=1]: no one goes unindicted here , which is probably for the best .\n",
      " dev2 [gold=3,pred=1]: and if you 're not nearly moved to tears by a couple of **scenes** , you 've got ice water in your veins .\n",
      "\n",
      "Epoch    29: reducing learning rate of group 0 to 2.5000e-05.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shuffling training data\n",
      "Epoch 30 Iter 10300 loss=0.3916 elbo -0.5485 sf 0.8169 selected 0.4073 kl 0.0407 ll -1.3612\n",
      "Epoch 30 Iter 10400 loss=0.4000 elbo -0.4451 sf 0.6831 selected 0.4314 kl 0.0348 ll -1.1246\n",
      "Epoch 30 Iter 10500 loss=0.3864 elbo -0.4020 sf 0.5708 selected 0.4126 kl 0.0435 ll -0.9682\n",
      "\n",
      "# epoch 30 iter 10571: dev loss 0.4867 elbo -0.4867 sf 0.3091 selected 0.0938 kl 0.0343 ll -0.7615 acc 0.3887\n",
      " dev0 [gold=3,pred=1]: it 's a **lovely** film with **lovely** **performances** by buy and accorsi .\n",
      " dev1 [gold=2,pred=1]: no one goes unindicted here , which is probably for the best .\n",
      " dev2 [gold=3,pred=1]: and if you 're not nearly moved to tears by a couple of scenes , you 've got ice water in your veins .\n",
      "\n",
      "Epoch 31 Iter 10600 loss=0.4045 elbo -0.4280 sf 0.6397 selected 0.3984 kl 0.0247 ll -1.0652\n",
      "Shuffling training data\n",
      "Epoch 31 Iter 10700 loss=0.3901 elbo -0.3686 sf 0.5598 selected 0.4182 kl 0.0453 ll -0.9235\n",
      "Epoch 31 Iter 10800 loss=0.4039 elbo -0.4526 sf 0.7644 selected 0.4158 kl 0.0277 ll -1.2141\n",
      "Epoch 31 Iter 10900 loss=0.3885 elbo -0.4696 sf 0.6919 selected 0.3695 kl 0.0304 ll -1.1582\n",
      "\n",
      "# epoch 31 iter 10912: dev loss 0.4793 elbo -0.4793 sf 0.3155 selected 0.0961 kl 0.0346 ll -0.7602 acc 0.3906\n",
      " dev0 [gold=3,pred=1]: it 's a **lovely** film with **lovely** **performances** by buy and accorsi .\n",
      " dev1 [gold=2,pred=1]: no one goes unindicted here , which is probably for the best .\n",
      " dev2 [gold=3,pred=1]: and if you 're not nearly moved to tears by a couple of scenes , you 've got ice water in your veins .\n",
      "\n",
      "Shuffling training data\n",
      "Epoch 32 Iter 11000 loss=0.3833 elbo -0.3093 sf 0.6108 selected 0.4737 kl 0.0337 ll -0.9164\n",
      "Epoch 32 Iter 11100 loss=0.4009 elbo -0.5193 sf 0.8575 selected 0.4033 kl 0.0357 ll -1.3728\n",
      "Epoch 32 Iter 11200 loss=0.4108 elbo -0.4709 sf 0.8193 selected 0.4281 kl 0.0352 ll -1.2862\n",
      "\n",
      "# epoch 32 iter 11253: dev loss 0.4900 elbo -0.4900 sf 0.3042 selected 0.0849 kl 0.0313 ll -0.7629 acc 0.3806\n",
      " dev0 [gold=3,pred=1]: it 's a **lovely** film with **lovely** **performances** by buy and accorsi .\n",
      " dev1 [gold=2,pred=1]: no one goes unindicted here , which is probably for the best .\n",
      " dev2 [gold=3,pred=1]: and if you 're not nearly moved to tears by a couple of scenes , you 've got ice water in your veins .\n",
      "\n",
      "Shuffling training data\n",
      "Epoch 33 Iter 11300 loss=0.3877 elbo -0.5498 sf 0.8340 selected 0.3753 kl 0.0301 ll -1.3803\n",
      "Epoch 33 Iter 11400 loss=0.4061 elbo -0.4016 sf 0.6155 selected 0.4078 kl 0.0267 ll -1.0140\n",
      "Epoch 33 Iter 11500 loss=0.4130 elbo -0.4240 sf 0.6153 selected 0.3746 kl 0.0279 ll -1.0361\n",
      "\n",
      "# epoch 33 iter 11594: dev loss 0.4799 elbo -0.4799 sf 0.3136 selected 0.0893 kl 0.0319 ll -0.7615 acc 0.3851\n",
      " dev0 [gold=3,pred=1]: it 's a **lovely** film with **lovely** **performances** by buy and accorsi .\n",
      " dev1 [gold=2,pred=1]: no one goes unindicted here , which is probably for the best .\n",
      " dev2 [gold=3,pred=1]: and if you 're not nearly moved to tears by a couple of scenes , you 've got ice water in your veins .\n",
      "\n",
      "Epoch 34 Iter 11600 loss=0.3909 elbo -0.2147 sf 0.3441 selected 0.3937 kl 0.0340 ll -0.5548\n",
      "Shuffling training data\n",
      "Epoch 34 Iter 11700 loss=0.3905 elbo -0.3904 sf 0.5498 selected 0.3751 kl 0.0286 ll -0.9369\n",
      "Epoch 34 Iter 11800 loss=0.3951 elbo -0.4916 sf 0.6184 selected 0.3870 kl 0.0255 ll -1.1070\n",
      "Epoch 34 Iter 11900 loss=0.3873 elbo -0.3107 sf 0.5189 selected 0.4142 kl 0.0252 ll -0.8267\n",
      "\n",
      "# epoch 34 iter 11935: dev loss 0.4783 elbo -0.4783 sf 0.3148 selected 0.0895 kl 0.0319 ll -0.7612 acc 0.3860\n",
      " dev0 [gold=3,pred=1]: it 's a **lovely** film with **lovely** **performances** by buy and accorsi .\n",
      " dev1 [gold=2,pred=1]: no one goes unindicted here , which is probably for the best .\n",
      " dev2 [gold=3,pred=1]: and if you 're not nearly moved to tears by a couple of scenes , you 've got ice water in your veins .\n",
      "\n",
      "Shuffling training data\n",
      "Epoch 35 Iter 12000 loss=0.3899 elbo -0.3747 sf 0.6181 selected 0.4040 kl 0.0257 ll -0.9897\n",
      "Epoch 35 Iter 12100 loss=0.4100 elbo -0.4166 sf 0.6179 selected 0.4062 kl 0.0257 ll -1.0314\n",
      "Epoch 35 Iter 12200 loss=0.4155 elbo -0.3759 sf 0.5210 selected 0.3824 kl 0.0313 ll -0.8930\n",
      "\n",
      "# epoch 35 iter 12276: dev loss 0.4893 elbo -0.4893 sf 0.3044 selected 0.0773 kl 0.0283 ll -0.7654 acc 0.3706\n",
      " dev0 [gold=3,pred=1]: it 's a **lovely** film with **lovely** **performances** by buy and accorsi .\n",
      " dev1 [gold=2,pred=1]: no one goes unindicted here , which is probably for the best .\n",
      " dev2 [gold=3,pred=1]: and if you 're not nearly moved to tears by a couple of scenes , you 've got ice water in your veins .\n",
      "\n",
      "Epoch 36 Iter 12300 loss=0.3935 elbo -0.3520 sf 0.5504 selected 0.3849 kl 0.0194 ll -0.8999\n",
      "Shuffling training data\n",
      "Epoch 36 Iter 12400 loss=0.3954 elbo -0.3448 sf 0.5059 selected 0.4205 kl 0.0281 ll -0.8473\n",
      "Epoch 36 Iter 12500 loss=0.4079 elbo -0.5277 sf 0.8708 selected 0.4217 kl 0.0379 ll -1.3938\n",
      "Epoch 36 Iter 12600 loss=0.3862 elbo -0.4727 sf 0.8255 selected 0.4432 kl 0.0320 ll -1.2942\n",
      "\n",
      "# epoch 36 iter 12617: dev loss 0.4665 elbo -0.4665 sf 0.3253 selected 0.0934 kl 0.0322 ll -0.7597 acc 0.3833\n",
      " dev0 [gold=3,pred=1]: it 's a **lovely** film with **lovely** **performances** by buy and **accorsi** .\n",
      " dev1 [gold=2,pred=1]: no one goes unindicted here , which is probably for the best .\n",
      " dev2 [gold=3,pred=1]: and if you 're not nearly moved to tears by a couple of scenes , you 've got ice water in your veins .\n",
      "\n",
      "Shuffling training data\n",
      "Epoch 37 Iter 12700 loss=0.3863 elbo -0.2626 sf 0.4981 selected 0.4188 kl 0.0379 ll -0.7559\n",
      "Epoch 37 Iter 12800 loss=0.4072 elbo -0.4469 sf 0.6780 selected 0.3879 kl 0.0310 ll -1.1209\n",
      "Epoch 37 Iter 12900 loss=0.4007 elbo -0.3141 sf 0.6303 selected 0.4082 kl 0.0269 ll -0.9409\n",
      "\n",
      "# epoch 37 iter 12958: dev loss 0.4764 elbo -0.4764 sf 0.3148 selected 0.0813 kl 0.0286 ll -0.7626 acc 0.3769\n",
      " dev0 [gold=3,pred=1]: it 's a **lovely** film with **lovely** **performances** by buy and accorsi .\n",
      " dev1 [gold=2,pred=1]: no one goes unindicted here , which is probably for the best .\n",
      " dev2 [gold=3,pred=1]: and if you 're not nearly moved to tears by a couple of scenes , you 've got ice water in your veins .\n",
      "\n",
      "Shuffling training data\n",
      "Epoch 38 Iter 13000 loss=0.3890 elbo -0.3897 sf 0.5933 selected 0.4121 kl 0.0332 ll -0.9788\n",
      "Epoch 38 Iter 13100 loss=0.3866 elbo -0.3828 sf 0.6099 selected 0.4064 kl 0.0203 ll -0.9900\n",
      "Epoch 38 Iter 13200 loss=0.3890 elbo -0.3878 sf 0.5803 selected 0.4129 kl 0.0402 ll -0.9628\n",
      "\n",
      "# epoch 38 iter 13299: dev loss 0.4744 elbo -0.4744 sf 0.3169 selected 0.0792 kl 0.0278 ll -0.7635 acc 0.3715\n",
      " dev0 [gold=3,pred=1]: it 's a **lovely** film with **lovely** **performances** by buy and **accorsi** .\n",
      " dev1 [gold=2,pred=1]: no one goes unindicted here , which is probably for the best .\n",
      " dev2 [gold=3,pred=1]: and if you 're not nearly moved to tears by a couple of scenes , you 've got ice water in your veins .\n",
      "\n",
      "Epoch 39 Iter 13300 loss=0.3847 elbo -0.3721 sf 0.5679 selected 0.3871 kl 0.0228 ll -0.9370\n",
      "Shuffling training data\n",
      "Epoch 39 Iter 13400 loss=0.4012 elbo -0.3194 sf 0.5108 selected 0.4114 kl 0.0343 ll -0.8256\n",
      "Epoch 39 Iter 13500 loss=0.3757 elbo -0.3515 sf 0.6277 selected 0.3834 kl 0.0233 ll -0.9761\n",
      "Epoch 39 Iter 13600 loss=0.4033 elbo -0.4854 sf 0.7895 selected 0.3812 kl 0.0259 ll -1.2714\n",
      "\n",
      "# epoch 39 iter 13640: dev loss 0.4764 elbo -0.4764 sf 0.3150 selected 0.0766 kl 0.0271 ll -0.7643 acc 0.3715\n",
      " dev0 [gold=3,pred=1]: it 's a **lovely** film with **lovely** **performances** by buy and accorsi .\n",
      " dev1 [gold=2,pred=1]: no one goes unindicted here , which is probably for the best .\n",
      " dev2 [gold=3,pred=1]: and if you 're not nearly moved to tears by a couple of scenes , you 've got ice water in your veins .\n",
      "\n",
      "Shuffling training data\n",
      "Epoch 40 Iter 13700 loss=0.3904 elbo -0.3446 sf 0.6401 selected 0.4133 kl 0.0284 ll -0.9808\n",
      "Epoch 40 Iter 13800 loss=0.3720 elbo -0.4063 sf 0.6979 selected 0.4044 kl 0.0293 ll -1.1001\n",
      "Epoch 40 Iter 13900 loss=0.4070 elbo -0.4148 sf 0.6686 selected 0.3883 kl 0.0213 ll -1.0805\n",
      "\n",
      "# epoch 40 iter 13981: dev loss 0.4888 elbo -0.4888 sf 0.3028 selected 0.0639 kl 0.0235 ll -0.7681 acc 0.3678\n",
      " dev0 [gold=3,pred=1]: it 's a **lovely** film with **lovely** **performances** by buy and accorsi .\n",
      " dev1 [gold=2,pred=1]: no one goes unindicted here , which is probably for the best .\n",
      " dev2 [gold=3,pred=1]: and if you 're not nearly moved to tears by a couple of scenes , you 've got ice water in your veins .\n",
      "\n",
      "Epoch    40: reducing learning rate of group 0 to 1.2500e-05.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 41 Iter 14000 loss=0.4155 elbo -0.3351 sf 0.5259 selected 0.3947 kl 0.0172 ll -0.8585\n",
      "Shuffling training data\n",
      "Epoch 41 Iter 14100 loss=0.3991 elbo -0.5263 sf 0.7283 selected 0.3497 kl 0.0234 ll -1.2513\n",
      "Epoch 41 Iter 14200 loss=0.3854 elbo -0.4347 sf 0.6336 selected 0.3817 kl 0.0228 ll -1.0651\n",
      "Epoch 41 Iter 14300 loss=0.3966 elbo -0.4118 sf 0.6165 selected 0.3800 kl 0.0251 ll -1.0248\n",
      "\n",
      "# epoch 41 iter 14322: dev loss 0.4802 elbo -0.4802 sf 0.3109 selected 0.0698 kl 0.0251 ll -0.7661 acc 0.3751\n",
      " dev0 [gold=3,pred=1]: it 's a **lovely** film with **lovely** **performances** by buy and accorsi .\n",
      " dev1 [gold=2,pred=1]: no one goes unindicted here , which is probably for the best .\n",
      " dev2 [gold=3,pred=1]: and if you 're not nearly moved to tears by a couple of scenes , you 've got ice water in your veins .\n",
      "\n",
      "Shuffling training data\n",
      "Epoch 42 Iter 14400 loss=0.3998 elbo -0.3771 sf 0.5781 selected 0.3463 kl 0.0196 ll -0.9524\n",
      "Epoch 42 Iter 14500 loss=0.3837 elbo -0.4226 sf 0.6152 selected 0.3744 kl 0.0229 ll -1.0344\n",
      "Epoch 42 Iter 14600 loss=0.4082 elbo -0.4013 sf 0.5788 selected 0.3744 kl 0.0276 ll -0.9762\n",
      "\n",
      "# epoch 42 iter 14663: dev loss 0.4814 elbo -0.4814 sf 0.3096 selected 0.0699 kl 0.0251 ll -0.7659 acc 0.3733\n",
      " dev0 [gold=3,pred=1]: it 's a **lovely** film with **lovely** **performances** by buy and accorsi .\n",
      " dev1 [gold=2,pred=1]: no one goes unindicted here , which is probably for the best .\n",
      " dev2 [gold=3,pred=1]: and if you 're not nearly moved to tears by a couple of scenes , you 've got ice water in your veins .\n",
      "\n",
      "Epoch 43 Iter 14700 loss=0.3881 elbo -0.3792 sf 0.5476 selected 0.3589 kl 0.0218 ll -0.9235\n",
      "Shuffling training data\n",
      "Epoch 43 Iter 14800 loss=0.3886 elbo -0.2221 sf 0.4415 selected 0.4107 kl 0.0202 ll -0.6606\n",
      "Epoch 43 Iter 14900 loss=0.3902 elbo -0.3943 sf 0.6181 selected 0.3827 kl 0.0242 ll -1.0088\n",
      "Epoch 43 Iter 15000 loss=0.4053 elbo -0.3212 sf 0.5317 selected 0.4070 kl 0.0247 ll -0.8491\n",
      "\n",
      "# epoch 43 iter 15004: dev loss 0.4851 elbo -0.4851 sf 0.3061 selected 0.0662 kl 0.0242 ll -0.7670 acc 0.3669\n",
      " dev0 [gold=3,pred=1]: it 's a **lovely** film with **lovely** **performances** by buy and accorsi .\n",
      " dev1 [gold=2,pred=1]: no one goes unindicted here , which is probably for the best .\n",
      " dev2 [gold=3,pred=1]: and if you 're not nearly moved to tears by a couple of scenes , you 've got ice water in your veins .\n",
      "\n",
      "Shuffling training data\n",
      "Epoch 44 Iter 15100 loss=0.3845 elbo -0.3745 sf 0.6450 selected 0.3982 kl 0.0181 ll -1.0168\n",
      "Epoch 44 Iter 15200 loss=0.4057 elbo -0.4531 sf 0.7824 selected 0.3657 kl 0.0182 ll -1.2328\n",
      "Epoch 44 Iter 15300 loss=0.3976 elbo -0.4500 sf 0.7032 selected 0.3726 kl 0.0235 ll -1.1496\n",
      "\n",
      "# epoch 44 iter 15345: dev loss 0.4774 elbo -0.4774 sf 0.3128 selected 0.0709 kl 0.0252 ll -0.7650 acc 0.3706\n",
      " dev0 [gold=3,pred=1]: it 's a **lovely** film with **lovely** **performances** by buy and accorsi .\n",
      " dev1 [gold=2,pred=1]: no one goes unindicted here , which is probably for the best .\n",
      " dev2 [gold=3,pred=1]: and if you 're not nearly moved to tears by a couple of scenes , you 've got ice water in your veins .\n",
      "\n",
      "Shuffling training data\n",
      "Epoch 45 Iter 15400 loss=0.4059 elbo -0.2877 sf 0.4398 selected 0.3789 kl 0.0209 ll -0.7242\n",
      "Epoch 45 Iter 15500 loss=0.3883 elbo -0.2283 sf 0.3820 selected 0.3966 kl 0.0221 ll -0.6069\n",
      "Epoch 45 Iter 15600 loss=0.3778 elbo -0.2511 sf 0.3219 selected 0.4076 kl 0.0238 ll -0.5692\n",
      "\n",
      "# epoch 45 iter 15686: dev loss 0.4702 elbo -0.4702 sf 0.3191 selected 0.0744 kl 0.0260 ll -0.7633 acc 0.3715\n",
      " dev0 [gold=3,pred=1]: it 's a **lovely** film with **lovely** **performances** by buy and accorsi .\n",
      " dev1 [gold=2,pred=1]: no one goes unindicted here , which is probably for the best .\n",
      " dev2 [gold=3,pred=1]: and if you 're not nearly moved to tears by a couple of scenes , you 've got ice water in your veins .\n",
      "\n",
      "Epoch 46 Iter 15700 loss=0.3937 elbo -0.3088 sf 0.5224 selected 0.4019 kl 0.0239 ll -0.8275\n",
      "Shuffling training data\n",
      "Epoch 46 Iter 15800 loss=0.4088 elbo -0.4008 sf 0.6673 selected 0.3846 kl 0.0254 ll -1.0641\n",
      "Epoch 46 Iter 15900 loss=0.3911 elbo -0.3519 sf 0.5824 selected 0.4121 kl 0.0274 ll -0.9299\n",
      "Epoch 46 Iter 16000 loss=0.4035 elbo -0.3416 sf 0.5866 selected 0.3946 kl 0.0222 ll -0.9247\n",
      "\n",
      "# epoch 46 iter 16027: dev loss 0.4859 elbo -0.4859 sf 0.3043 selected 0.0597 kl 0.0220 ll -0.7683 acc 0.3678\n",
      " dev0 [gold=3,pred=3]: it 's a **lovely** film with **lovely** **performances** by buy and accorsi .\n",
      " dev1 [gold=2,pred=1]: no one goes unindicted here , which is probably for the best .\n",
      " dev2 [gold=3,pred=3]: and if you 're not nearly moved to tears by a couple of scenes , you 've got ice water in your veins .\n",
      "\n",
      "Shuffling training data\n",
      "Epoch 47 Iter 16100 loss=0.4154 elbo -0.4234 sf 0.6724 selected 0.3991 kl 0.0217 ll -1.0923\n",
      "Epoch 47 Iter 16200 loss=0.4026 elbo -0.3745 sf 0.5645 selected 0.3767 kl 0.0266 ll -0.9347\n",
      "Epoch 47 Iter 16300 loss=0.3984 elbo -0.2778 sf 0.4428 selected 0.3520 kl 0.0261 ll -0.7164\n",
      "\n",
      "# epoch 47 iter 16368: dev loss 0.4861 elbo -0.4861 sf 0.3043 selected 0.0584 kl 0.0215 ll -0.7689 acc 0.3660\n",
      " dev0 [gold=3,pred=3]: it 's a **lovely** film with **lovely** **performances** by buy and accorsi .\n",
      " dev1 [gold=2,pred=1]: no one goes unindicted here , which is probably for the best .\n",
      " dev2 [gold=3,pred=3]: and if you 're not nearly moved to tears by a couple of scenes , you 've got ice water in your veins .\n",
      "\n",
      "Epoch 48 Iter 16400 loss=0.4012 elbo -0.3719 sf 0.5195 selected 0.3805 kl 0.0215 ll -0.8879\n",
      "Shuffling training data\n",
      "Epoch 48 Iter 16500 loss=0.4031 elbo -0.3548 sf 0.5345 selected 0.3668 kl 0.0198 ll -0.8860\n",
      "Epoch 48 Iter 16600 loss=0.3813 elbo -0.2832 sf 0.4609 selected 0.3962 kl 0.0162 ll -0.7414\n",
      "Epoch 48 Iter 16700 loss=0.3857 elbo -0.3922 sf 0.6974 selected 0.3878 kl 0.0252 ll -1.0854\n",
      "\n",
      "# epoch 48 iter 16709: dev loss 0.4665 elbo -0.4665 sf 0.3215 selected 0.0693 kl 0.0240 ll -0.7640 acc 0.3760\n",
      " dev0 [gold=3,pred=1]: it 's a **lovely** film with **lovely** **performances** by buy and accorsi .\n",
      " dev1 [gold=2,pred=1]: no one goes unindicted here , which is probably for the best .\n",
      " dev2 [gold=3,pred=1]: and if you 're not nearly moved to tears by a couple of scenes , you 've got ice water in your veins .\n",
      "\n",
      "Shuffling training data\n",
      "Epoch 49 Iter 16800 loss=0.3749 elbo -0.3979 sf 0.8162 selected 0.4041 kl 0.0191 ll -1.2109\n",
      "Epoch 49 Iter 16900 loss=0.3953 elbo -0.2891 sf 0.4796 selected 0.4365 kl 0.0201 ll -0.7653\n",
      "Epoch 49 Iter 17000 loss=0.3765 elbo -0.3680 sf 0.5856 selected 0.4206 kl 0.0406 ll -0.9467\n",
      "\n",
      "# epoch 49 iter 17050: dev loss 0.4568 elbo -0.4568 sf 0.3301 selected 0.0770 kl 0.0260 ll -0.7608 acc 0.3797\n",
      " dev0 [gold=3,pred=1]: it 's a **lovely** film with **lovely** **performances** by buy and **accorsi** .\n",
      " dev1 [gold=2,pred=1]: no one goes unindicted here , which is probably for the best .\n",
      " dev2 [gold=3,pred=1]: and if you 're not nearly moved to tears by a couple of scenes , you 've got ice water in your veins .\n",
      "\n",
      "Epoch 50 Iter 17100 loss=0.3885 elbo -0.4039 sf 0.7555 selected 0.4027 kl 0.0214 ll -1.1558\n",
      "Shuffling training data\n",
      "Epoch 50 Iter 17200 loss=0.3745 elbo -0.4209 sf 0.6583 selected 0.3533 kl 0.0221 ll -1.0754\n",
      "Epoch 50 Iter 17300 loss=0.3920 elbo -0.3796 sf 0.5789 selected 0.4039 kl 0.0226 ll -0.9545\n",
      "\n",
      "# epoch 50 iter 17391: dev loss 0.4653 elbo -0.4653 sf 0.3229 selected 0.0686 kl 0.0238 ll -0.7644 acc 0.3733\n",
      " dev0 [gold=3,pred=1]: it 's a **lovely** film with **lovely** **performances** by buy and accorsi .\n",
      " dev1 [gold=2,pred=1]: no one goes unindicted here , which is probably for the best .\n",
      " dev2 [gold=3,pred=1]: and if you 're not nearly moved to tears by a couple of scenes , you 've got ice water in your veins .\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cfg['layer_inf'] = 'lstm'\n",
    "lstm_model = train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь посмотрим то же самое для lstm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_important_counter, lstm_all_counter = make_counters(lstm_model, dev_data, len(dev_data), device, cfg, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Сколько разные части речи считаются важными:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('JJ', 1105),\n",
       " ('NN', 710),\n",
       " ('RB', 362),\n",
       " ('NNS', 191),\n",
       " ('VBG', 120),\n",
       " ('VBZ', 72),\n",
       " ('VBN', 61),\n",
       " ('VB', 50),\n",
       " ('CC', 27),\n",
       " ('VBP', 16),\n",
       " ('VBD', 16),\n",
       " ('IN', 15),\n",
       " ('MD', 13),\n",
       " ('JJR', 6),\n",
       " (',', 5),\n",
       " ('DT', 5),\n",
       " ('JJS', 5),\n",
       " ('UH', 4),\n",
       " ('RBR', 4),\n",
       " ('.', 3),\n",
       " (':', 1)]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Сколько разные части речи считаются важными:\")\n",
    "lstm_important_counter.most_common()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Частота важности частей речи:\n",
      "UH 1.0\n",
      "JJ 0.4848617814831066\n",
      "VBG 0.28708133971291866\n",
      "RB 0.28616600790513835\n",
      "NNS 0.1900497512437811\n",
      "VBN 0.18541033434650456\n",
      "NN 0.17543859649122806\n",
      "VBD 0.10062893081761007\n",
      "JJR 0.09523809523809523\n",
      "VB 0.08417508417508418\n",
      "JJS 0.07246376811594203\n",
      "VBZ 0.06997084548104957\n",
      "RBR 0.06896551724137931\n",
      "MD 0.06666666666666667\n",
      "VBP 0.05755395683453238\n",
      "CC 0.034571062740076826\n",
      "IN 0.006967022758941012\n",
      ", 0.0053475935828877\n",
      ": 0.0051813471502590676\n",
      ". 0.0028011204481792717\n",
      "DT 0.002364066193853428\n",
      "PRP 0.0\n",
      "WDT 0.0\n",
      "TO 0.0\n",
      "PRP$ 0.0\n",
      "CD 0.0\n",
      "NNP 0.0\n",
      "POS 0.0\n",
      "WRB 0.0\n",
      "WP 0.0\n",
      "EX 0.0\n",
      "RP 0.0\n",
      "`` 0.0\n",
      "'' 0.0\n",
      "RBS 0.0\n",
      "WP$ 0.0\n",
      "PDT 0.0\n",
      "FW 0.0\n",
      "$ 0.0\n"
     ]
    }
   ],
   "source": [
    "fractions = []\n",
    "for token in all_counter:\n",
    "    fractions.append((token, lstm_important_counter[token] / lstm_all_counter[token]))\n",
    "print(\"Частота важности частей речи:\")\n",
    "for (token, frac) in sorted(fractions, key=lambda pair: -pair[1]):\n",
    "    print(token, frac)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Получается примерно то же самое. Похоже в этой задаче, если обучать ее до той точности, до которой обучаю я, хватает bag of words модели и lstm особо не нужен. Почему-то со 100% частотой становятся важными междометия. Казалось бы, это не самые важные для смысла слова. Но смысл - понятие довольно сложное. В любом случае, междометий в датасете мало, поэтому делать выводы по этой статистике с хорошей точностью нельзя."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "SST.ipynb",
   "provenance": [],
   "toc_visible": true,
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
